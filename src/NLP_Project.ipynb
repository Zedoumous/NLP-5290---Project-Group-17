{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#README.txt"
      ],
      "metadata": {
        "id": "VqvVC9cM4PPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and drop a ZIPPED version of the folder `/data` from GitHub repo into Colab.\n",
        "\n",
        "< - - - - -"
      ],
      "metadata": {
        "id": "SkYHORUB4Rig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# then run this and refresh directory...\n",
        "\n",
        "# unzip datasets\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxJqWMSD5rS3",
        "outputId": "7c2e7539-703f-469a-bd47-83d26de283e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/job-postings/\n",
            "  inflating: data/job-postings/Data_Job_NY.csv  \n",
            "  inflating: data/job-postings/Data_Job_SF.csv  \n",
            "  inflating: data/job-postings/Data_Job_TX.csv  \n",
            "  inflating: data/job-postings/Data_Job_WA.csv  \n",
            "  inflating: data/README.txt         \n",
            "   creating: data/resumes/\n",
            "  inflating: data/resumes/kaggleResumes.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# API"
      ],
      "metadata": {
        "id": "anBUXima7kjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v4wViL775dD",
        "outputId": "36f05df3-1eef-4df5-9450-11a6f928997f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting config\n",
            "  Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: config\n",
            "Successfully installed config-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import config\n",
        "# API Docs found here: https://developer.usajobs.gov/Tutorials/Search-Jobs"
      ],
      "metadata": {
        "id": "uNcEHO9t7rKr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.US_JOBS_API_KEY = \"xVc0TZiLfhcr17ci7Ngk6bLAetdRVFgntm2pZgWNtww=\"\n",
        "config.EMAIL_ADDRESS = \"gjacobthomas@gmail.com\""
      ],
      "metadata": {
        "id": "sobm0urbDUff"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I'm assuming this API has some limiter on it so we don't want to lose access. -tyler"
      ],
      "metadata": {
        "id": "89Ei-XMSDhIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "host = 'data.usajobs.gov' \n",
        "# add these values in the config.py file\n",
        "userAgent = config.EMAIL_ADDRESS\n",
        "authKey = config.US_JOBS_API_KEY\n",
        "\n",
        "base_url = \"https://data.usajobs.gov/api/search\"\n",
        "\n",
        "parameters = {\n",
        "    \"JobCategoryCode\": 2210,\n",
        "    \"Keyword\": \"Software Development\",\n",
        "    \"LocationName\": \"Washington, DC\"\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Host\": host,          \n",
        "    \"User-Agent\": userAgent,          \n",
        "    \"Authorization-Key\": authKey  \n",
        "}\n",
        "\n",
        "resp = requests.request(\"GET\", base_url,headers=headers, params=parameters)\n",
        "result = resp.json()['SearchResult']['SearchResultItems']\n",
        "\n",
        "# get Job Title \n",
        "print(result[1]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "# get Job Summary\n",
        "print(result[1]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "# more parameters are found here: https://developer.usajobs.gov/API-Reference/GET-api-Search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVoxo2K47j_l",
        "outputId": "36d5789e-7d7f-4a35-be73-010507de5e77"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervisory IT Program Manager (APPSW)/Assistant Director, Development Services\n",
            "This position is located in Criminal Division's Office of Administration, Information Technology Management (ITM) unit and serves as the Assistant Director, Development Services. The Assistant Director has responsibility for the management of internal and external software services, including custom application development, intranet support, SharePoint, software service providers, and all related cyber security functions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# KeyBERT Extraction Function"
      ],
      "metadata": {
        "id": "GVU9pbVd3-hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "-RUKzBOv5-_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HIogy6WO3hvX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from keybert import KeyBERT # pip install keybert (give it a minute...)\n",
        "\n",
        "'''\n",
        "/*---------------------------------------------------------------------\n",
        " |  Method: extractKeywordsBERT\n",
        " |\n",
        " |  Purpose: Uses the KeyBert Keyword Extraction Tool to extract\n",
        " |           and return keywords from a given corpus. \n",
        " |      \n",
        " |  Author: Tyler Parks\n",
        " |  Created On: 10/30/22\n",
        " |\n",
        " |  Parameters:\n",
        " |      normalized_corpus -- A single string containing all text of the\n",
        " |                           normalized corpus.\n",
        " |\n",
        " |  Returns: \n",
        " |      keywords -- List of collected keywords\n",
        " |      scores -- List of those keyword's scores\n",
        " |\n",
        " |  References: https://maartengr.github.io/KeyBERT/#usage\n",
        " |\n",
        " *-------------------------------------------------------------------*/\n",
        "''' \n",
        "def extractKeywordsBERT(normalized_corpus):   \n",
        "    print('---KeyBert Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    # init. language model \n",
        "    language_model = KeyBERT(model = 'all-mpnet-base-v2')\n",
        "\n",
        "    # extract those keywords!\n",
        "    data = language_model.extract_keywords( normalized_corpus, \n",
        "                                            keyphrase_ngram_range=(1, 3), \n",
        "                                            stop_words='english',\n",
        "                                            use_maxsum=False, \n",
        "                                            use_mmr=True,\n",
        "                                            diversity=0.7,\n",
        "                                            nr_candidates=20, \n",
        "                                            top_n=15\n",
        "                                        )\n",
        "\n",
        "    # zip the lists\n",
        "    zipped = list(map(list, zip(*data)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "    print('-Skill-'.ljust(40), '-Score-')\n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "    print()\n",
        "\n",
        "    return keywords, scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# CSO-Classifier Extraction Function"
      ],
      "metadata": {
        "id": "vmHHZLv57ex9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install cso-classifier"
      ],
      "metadata": {
        "id": "CXlOaYQj7jDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't restart runtime if the terminal says so! Keep going.**"
      ],
      "metadata": {
        "id": "A0Ga47IZ8cH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import spacy\n",
        "from cso_classifier import CSOClassifier      # import classifier tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip21zn4U8Rxj",
        "outputId": "7cdf35f5-3f57-4175-aba6-0f4ebb5d84f4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the most recent model\n",
        "CSOClassifier.update() \n",
        "\n",
        "# define the model object\n",
        "CSO_Extractor = CSOClassifier(modules = \"both\", enhancement = \"first\", explanation = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLaIVJli8qOe",
        "outputId": "32bd44e6-678c-499d-a573-5213b05b5b26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ======================================================\n",
            "#     ONTOLOGY\n",
            "# ======================================================\n",
            "The ontology is already up to date.\n",
            "\n",
            "# ======================================================\n",
            "#     MODELS: CACHED & WORD2VEC\n",
            "# ======================================================\n",
            "Updating the models: cached and word2vec\n",
            "[██████████████████████████████████████████████████] 63M/63M\n",
            "[*] Done!\n",
            "[██████████████████████████████████████████████████] 349M/349M\n",
            "[*] Done!\n",
            "Models downloaded successfully.\n",
            "Update completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsCSO(normalized_corpus):\n",
        "\n",
        "  # run the extraction\n",
        "  result = CSO_Extractor.run(normalized_corpus)\n",
        "\n",
        "  print('-----CSO Extraction-----')\n",
        "  print('------------------------\\n')\n",
        "  \n",
        "  for keyword in result['union']:\n",
        "    print(keyword)\n",
        "\n",
        "  return result['union']"
      ],
      "metadata": {
        "id": "Cctwpsvv86iZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Driver Code"
      ],
      "metadata": {
        "id": "FIi95ge84CGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd     # pip install pandas. usage: loading data from csv files into dataframes"
      ],
      "metadata": {
        "id": "YpAjFMcT6H3S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Helper Functions\n",
        "\n",
        "# Function to retrieve text data.\n",
        "# (either 1 or more job postings or resumes)?\n",
        "def getFileData(filename, dir):\n",
        "    return pd.read_csv('data/' + dir + '/' + filename)\n",
        "\n",
        "# Function to normalize text data. \n",
        "# (some skill extraction tools will normalize text for us; however, if not, this function is here)\n",
        "# includes removing stopwords, punctuation, dates, links, etc...\n",
        "def normalizeCorpus(corpus):\n",
        "    pass\n",
        "\n",
        "    # for now\n",
        "    return corpus\n",
        "\n",
        "# Function to extract skill words from a given corpus.\n",
        "# ideally, this function will output a set of skills extracted from the corpus\n",
        "def extractSkills(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO              = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    # extraction method n...\n",
        "    # keep going!\n",
        "\n",
        "### Driver Code\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # fetch the data\n",
        "    job_posting_dataframe = getFileData('Data_Job_TX.csv'  , 'job-postings')\n",
        "    resume_dataframe      = getFileData('kaggleResumes.csv', 'resumes'     )\n",
        "    #---------------\n",
        "\n",
        "    '''\n",
        "    # print the dataframes\n",
        "    print('DataFrame of Job Postings:')\n",
        "    print(job_posting_dataframe)    \n",
        "    print()\n",
        "\n",
        "    print('DataFrame of Resumes:')\n",
        "    print(resume_dataframe)\n",
        "    print()\n",
        "    #----------------------\n",
        "    '''\n",
        "\n",
        "    # fetch the job descriptions and resumes by themselves\n",
        "    jpCorpus = list(job_posting_dataframe['Job_Desc'])\n",
        "    rCorpus  = list(resume_dataframe['Resume'])\n",
        "    #----------------------\n",
        "\n",
        "    # Number of both job posting and resume samples to view\n",
        "    NUM_SAMPLES = 0\n",
        "\n",
        "    # for each JOB POSTING from the corpus\n",
        "    i = 0\n",
        "    for posting in jpCorpus:\n",
        "        print('Job Posting #', i+1)\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(posting)\n",
        "        extractSkills(text)\n",
        "\n",
        "        # print lines, we are done with this posting\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X postings\n",
        "        i += 1\n",
        "        if i > NUM_SAMPLES:\n",
        "            break\n",
        "    #---------------------- \n",
        "\n",
        "    # for each RESUME from the corpus\n",
        "    i = 0\n",
        "    for resume in rCorpus:\n",
        "        print('Resume #', i+1)\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(resume)\n",
        "        extractSkills(text)\n",
        "\n",
        "        # print lines, we are done with this resume\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X resumes\n",
        "        i += 1\n",
        "        if i > NUM_SAMPLES:\n",
        "            break\n",
        "    #---------------------- \n",
        "\n",
        "    \n",
        "# keep going!\n",
        "\n",
        "# end of driver code\n",
        "#---------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6HTo9qA4Fxs",
        "outputId": "7061150b-fd6b-44e6-db95-0cdc8694397d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job Posting # 1\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "senior data scientist                    0.6052\n",
            "products alarm monitoring                0.3292\n",
            "summary brinks home                      0.281\n",
            "forest regression designing              0.2549\n",
            "join team trusted                        0.2381\n",
            "azure aws google                         0.2262\n",
            "acquisition customer retention           0.2061\n",
            "sentiment analysis gradient              0.177\n",
            "efforts requirements                     0.1714\n",
            "clearly communicate model                0.151\n",
            "true                                     0.1165\n",
            "libraries thorough understanding         0.114\n",
            "excellent                                0.1049\n",
            "america                                  0.0347\n",
            "action true responsive                   0.0209\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "computer science\n",
            "sales\n",
            "business intelligence\n",
            "smart homes\n",
            "machine learning\n",
            "communication\n",
            "engineers\n",
            "engineering\n",
            "random forests\n",
            "text mining\n",
            "cloud infrastructures\n",
            "customer retention\n",
            "visualization\n",
            "optimization\n",
            "correlation analysis\n",
            "sentiment analysis\n",
            "mathematics\n",
            "applied mathematics\n",
            "boosting\n",
            "------------------------\n",
            "\n",
            "Resume # 1\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "developing automated review              0.5601\n",
            "associate data science                   0.4096\n",
            "lawyers                                  0.3539\n",
            "portfolio synthesizes intelligence       0.3337\n",
            "tar technology assisted                  0.2238\n",
            "relevant recommended                     0.2234\n",
            "bayes knn random                         0.213\n",
            "platform fraud                           0.2127\n",
            "tableau regular expression               0.196\n",
            "word embedding                           0.1828\n",
            "elasticsearch d3 js                      0.1816\n",
            "exprience 24 months                      0.1683\n",
            "kafka python flask                       0.0694\n",
            "pii personally                           0.0692\n",
            "jquery css bootstrap                     0.0626\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "database systems\n",
            "machine learning\n",
            "javascript\n",
            "erp system\n",
            "user information\n",
            "word embedding\n",
            "java\n",
            "html\n",
            "dimensionality reduction\n",
            "natural language processing\n",
            "visualization\n",
            "question answering\n",
            "sentiment analysis\n",
            "deep learning\n",
            "computer vision\n",
            "python\n",
            "textual data\n",
            "ajax\n",
            "boosting\n",
            "svm\n",
            "cyber-attacks\n",
            "recommendation\n",
            "natural languages\n",
            "text data\n",
            "classifiers\n",
            "visualization tools\n",
            "random forests\n",
            "classification models\n",
            "cluster analysis\n",
            "text document\n",
            "k-nearest neighbors\n",
            "vehicles\n",
            "css\n",
            "programming languages\n",
            "decision trees\n",
            "customer review\n",
            "------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}