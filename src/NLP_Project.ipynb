{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#README.txt"
      ],
      "metadata": {
        "id": "VqvVC9cM4PPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and drop a ZIPPED version of the folder `/data` from GitHub repo into Colab.\n",
        "\n",
        "< - - - - -"
      ],
      "metadata": {
        "id": "SkYHORUB4Rig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# then run this and refresh directory...\n",
        "\n",
        "# unzip datasets\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "kxJqWMSD5rS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# API"
      ],
      "metadata": {
        "id": "anBUXima7kjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v4wViL775dD",
        "outputId": "3dd099f9-fec7-4740-cfa2-44e171611eb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: config in /usr/local/lib/python3.8/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import config\n",
        "# API Docs found here: https://developer.usajobs.gov/Tutorials/Search-Jobs"
      ],
      "metadata": {
        "id": "uNcEHO9t7rKr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.US_JOBS_API_KEY = \"xVc0TZiLfhcr17ci7Ngk6bLAetdRVFgntm2pZgWNtww=\"\n",
        "config.EMAIL_ADDRESS = \"gjacobthomas@gmail.com\""
      ],
      "metadata": {
        "id": "sobm0urbDUff"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I'm assuming this API has some limiter on it so we don't want to lose access. -tyler"
      ],
      "metadata": {
        "id": "89Ei-XMSDhIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# more parameters are found here: https://developer.usajobs.gov/API-Reference/GET-api-Search\n",
        "def queryJobsAPI(keyword, location):\n",
        "  host = 'data.usajobs.gov' \n",
        "  # add these values in the config.py file\n",
        "  userAgent = config.EMAIL_ADDRESS\n",
        "  authKey = config.US_JOBS_API_KEY\n",
        "\n",
        "  base_url = \"https://data.usajobs.gov/api/search\"\n",
        "\n",
        "  parameters = {\n",
        "      \"Keyword\": keyword,\n",
        "      \"LocationName\": location\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "      \"Host\": host,          \n",
        "      \"User-Agent\": userAgent,          \n",
        "      \"Authorization-Key\": authKey  \n",
        "  }\n",
        "\n",
        "  resp = requests.request(\"GET\", base_url,headers=headers, params=parameters)\n",
        "  result = resp.json()['SearchResult']['SearchResultItems']\n",
        "\n",
        "  # get Job Title \n",
        "  print(result[1]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "  # get Job Summary\n",
        "  print(result[1]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "SVoxo2K47j_l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# KeyBERT Extraction Function"
      ],
      "metadata": {
        "id": "GVU9pbVd3-hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "-RUKzBOv5-_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HIogy6WO3hvX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from keybert import KeyBERT # pip install keybert (give it a minute...)\n",
        "from statistics import mean\n",
        "\n",
        "'''\n",
        "/*---------------------------------------------------------------------\n",
        " |  Method: extractKeywordsBERT\n",
        " |\n",
        " |  Purpose: Uses the KeyBert Keyword Extraction Tool to extract\n",
        " |           and return keywords from a given corpus. \n",
        " |      \n",
        " |  Author: Tyler Parks\n",
        " |  Created On: 10/30/22\n",
        " |\n",
        " |  Parameters:\n",
        " |      normalized_corpus -- A single string containing all text of the\n",
        " |                           normalized corpus.\n",
        " |\n",
        " |  Returns: \n",
        " |      keywords -- List of collected keywords\n",
        " |      scores -- List of those keyword's scores\n",
        " |\n",
        " |  References: https://maartengr.github.io/KeyBERT/#usage\n",
        " |\n",
        " *-------------------------------------------------------------------*/\n",
        "''' \n",
        "def extractKeywordsBERT(normalized_corpus):   \n",
        "    print('---KeyBert Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    # init. language model \n",
        "    language_model = KeyBERT(model = 'all-mpnet-base-v2')\n",
        "\n",
        "    # extract those keywords!\n",
        "    data = language_model.extract_keywords( normalized_corpus, \n",
        "                                            keyphrase_ngram_range=(1, 3), \n",
        "                                            stop_words='english',\n",
        "                                            use_maxsum=False, \n",
        "                                            use_mmr=True,\n",
        "                                            diversity=0.7,\n",
        "                                            nr_candidates=20, \n",
        "                                            top_n=15\n",
        "                                        )\n",
        "\n",
        "    # zip the lists\n",
        "    zipped = list(map(list, zip(*data)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "    print('-Skill-'.ljust(40), '-Score-')\n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "    print()\n",
        "\n",
        "    # print(\"type: \"+str(type(scores))+\"length: \"+ str(len(scores)))\n",
        "\n",
        "    avg = mean(scores[:14])\n",
        "    print(\"Score Average: \" + str(avg))\n",
        "\n",
        "    return keywords, scores, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# CSO-Classifier Extraction Function"
      ],
      "metadata": {
        "id": "vmHHZLv57ex9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install cso-classifier"
      ],
      "metadata": {
        "id": "CXlOaYQj7jDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51671007-a1b0-4851-8995-9159c80591b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-03 18:33:41.596750: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.7.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cso-classifier in /usr/local/lib/python3.8/dist-packages (3.0)\n",
            "Requirement already satisfied: nltk==3.6.2 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (3.6.2)\n",
            "Requirement already satisfied: strsimpy==0.2.0 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.2.0)\n",
            "Collecting spacy==3.0.5\n",
            "  Using cached spacy-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (12.9 MB)\n",
            "Requirement already satisfied: python-Levenshtein==0.12.2 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.12.2)\n",
            "Requirement already satisfied: kneed==0.3.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.3.1)\n",
            "Requirement already satisfied: update-checker==0.18.0 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.18.0)\n",
            "Requirement already satisfied: python-igraph==0.9.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (1.21.6)\n",
            "Requirement already satisfied: hurry.filesize==0.9 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.9)\n",
            "Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (2.25.1)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (7.1.2)\n",
            "Requirement already satisfied: gensim==3.8.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (3.8.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from hurry.filesize==0.9->cso-classifier) (57.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from kneed==0.3.1->cso-classifier) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from kneed==0.3.1->cso-classifier) (1.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (2022.6.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from python-igraph==0.9.1->cso-classifier) (1.6.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (2022.9.24)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (21.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (1.7.4)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "  Using cached thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.7.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (3.0.8)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy==3.0.5->cso-classifier) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy==3.0.5->cso-classifier) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->kneed==0.3.1->cso-classifier) (3.1.0)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.3\n",
            "    Uninstalling spacy-3.4.3:\n",
            "      Successfully uninstalled spacy-3.4.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.0.5 which is incompatible.\u001b[0m\n",
            "Successfully installed spacy-3.0.5 thinc-8.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't restart runtime if the terminal says so! Keep going.**"
      ],
      "metadata": {
        "id": "A0Ga47IZ8cH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# update spacy\n",
        "!pip install -U spacy\n",
        "import spacy\n",
        "from cso_classifier import CSOClassifier      # import classifier tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip21zn4U8Rxj",
        "outputId": "06f05661-0cdf-44ae-b8a4-081372109529"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.0.5)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Using cached thinc-8.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (819 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.7.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.0.5\n",
            "    Uninstalling spacy-3.0.5:\n",
            "      Successfully uninstalled spacy-3.0.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cso-classifier 3.0 requires spacy==3.0.5, but you have spacy 3.4.3 which is incompatible.\u001b[0m\n",
            "Successfully installed spacy-3.4.3 thinc-8.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the most recent model\n",
        "CSOClassifier.update() \n",
        "\n",
        "# define the model object\n",
        "CSO_Extractor = CSOClassifier(modules = \"both\", enhancement = \"first\", explanation = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLaIVJli8qOe",
        "outputId": "3dd6987b-7a93-43aa-8ffc-59c1441eeb07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ======================================================\n",
            "#     ONTOLOGY\n",
            "# ======================================================\n",
            "The ontology is already up to date.\n",
            "\n",
            "# ======================================================\n",
            "#     MODELS: CACHED & WORD2VEC\n",
            "# ======================================================\n",
            "Updating the models: cached and word2vec\n",
            "[██████████████████████████████████████████████████] 63M/63M\n",
            "[*] Done!\n",
            "[██████████████████████████████████████████████████] 349M/349M\n",
            "[*] Done!\n",
            "Models downloaded successfully.\n",
            "Update completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsCSO(normalized_corpus):\n",
        "\n",
        "  # run the extraction\n",
        "  result = CSO_Extractor.run(normalized_corpus)\n",
        "\n",
        "  print('\\n-----CSO Extraction-----')\n",
        "  print('------------------------\\n')\n",
        "  \n",
        "  for keyword in result['union']:\n",
        "    print(keyword)\n",
        "\n",
        "  return result['union']"
      ],
      "metadata": {
        "id": "Cctwpsvv86iZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YAKE Extractor**"
      ],
      "metadata": {
        "id": "-4h4ZTbXu9CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install yake\n",
        "import yake "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNiHvEtdvF0G",
        "outputId": "01e36326-d9c6-4c93-835b-f47faf463242"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from yake) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from yake) (1.21.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from yake) (0.8.10)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting segtok\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.8/dist-packages (from yake) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from segtok->yake) (2022.6.2)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp38-cp38-linux_x86_64.whl size=70607 sha256=f7060ab76a2d6a5febb3f6fc00478b0ddad5ea0b41afe253d5fda14da1aa0df9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/c7/3c/4c83132de76359e3a429fd09c08995945ca96c5290a41651d3\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.9.0 segtok-1.5.11 yake-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def extractKeywordsYAKE(normalized_corpus):\n",
        "    print('\\n---YAKE Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    language = \"en\"\n",
        "    max_ngram_size = 3\n",
        "    deduplication_threshold = 0.9\n",
        "    deduplication_algo = 'seqm'\n",
        "    windowSize = 1\n",
        "    numOfKeywords = 20\n",
        "\n",
        "    kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
        "    keywords = kw_extractor.extract_keywords(normalized_corpus)\n",
        "\n",
        "    zipped = list(map(list, zip(*keywords)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "   \n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "\n",
        "    avg = mean(scores[:19])\n",
        "    print(\"Score Average: \" + str(avg)+ \"\\n\")\n",
        "\n",
        "\n",
        "    return keywords, scores, avg"
      ],
      "metadata": {
        "id": "JGGvwVeYvaRv"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TextRank Extractor**"
      ],
      "metadata": {
        "id": "SbPfZZiZQGTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install summa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsnAZdNSQA8y",
        "outputId": "0dc7de85-f3cd-4d60-8be0-80f639ca43db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.8/dist-packages (from summa) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy>=0.19->summa) (1.21.6)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=a5519050f640a090b82ae91d495b9d670914a9d4a60610fa84d60be429999f74\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/6a/dd/209eb19d5f2266b9cfd06827539bf70435b0ad5fe8244e52d3\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summa import keywords\n",
        "from statistics import mean\n",
        "\n",
        "def extractKeywordsTextRank(normalized_corpus):\n",
        "  print('\\n')\n",
        "  print('---TextRank Extraction---')\n",
        "  print('------------------------\\n')\n",
        "\n",
        "  #extract\n",
        "  TR_keywords = keywords.keywords(normalized_corpus, scores=True)\n",
        "\n",
        "  #zip into list\n",
        "  zipped = list(map(list, zip(*TR_keywords)))\n",
        "  TR_keywords = zipped[0]\n",
        "  scores = zipped[1]\n",
        "  \n",
        "\n",
        "  #print(TR_keywords[0:20])\n",
        "  print('-Skill-'.ljust(40), '-Score-')\n",
        "  for i, value in enumerate(TR_keywords[0:19]):\n",
        "    print(value.ljust(40), scores[i])\n",
        "    \n",
        "  avg = mean(scores[:19])\n",
        "  print(\"\\nScore Average: \" + str(avg))\n",
        "  \n",
        "  \n",
        "\n",
        "  return TR_keywords, scores, avg\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "ybCL_9CJQCrl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Skill Matching**"
      ],
      "metadata": {
        "id": "Oao0RKq6aDyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install jieba\n",
        "\n",
        "from functools import reduce\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import jieba\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "class SkillsMatching:\n",
        "  def __init__(self, all_job_postings_keywords, all_jp_titles, all_resumes_keywords):\n",
        "      self.all_job_postings_keywords = all_job_postings_keywords\n",
        "      self.all_jp_titles = all_jp_titles\n",
        "      self.all_resumes_keywords = all_resumes_keywords\n",
        "      \n",
        "\n",
        "  # output JSON with matching job postings for each resume\n",
        "  def skill_match(self):\n",
        "      results = []\n",
        "      \n",
        "      for resume_json in self.all_resumes_keywords:\n",
        "          resume_arr = []\n",
        "          for key in resume_json:\n",
        "            # loop through the keys = every classifier \n",
        "            classifier_json = {key: {\"matching_job\": \"\",\n",
        "                      \"job_title\": \"\"}}\n",
        "            resume_keywords = resume_json[key]\n",
        "            scores = self.sim_score(self.all_job_postings_keywords, resume_keywords)\n",
        "            # grab the index of the largest score in the arr\n",
        "            max_idx = np.argmax(scores)\n",
        "            classifier_json[key]['matching_job'] = self.all_job_postings_keywords[max_idx]\n",
        "            classifier_json[key]['job_title'] = self.all_jp_titles[max_idx]\n",
        "            resume_arr.append(classifier_json)\n",
        "          results.append(resume_arr)\n",
        "      return results\n",
        "\n",
        "\n",
        "  def split_and_join_arr(self, arr):\n",
        "    new_arr = []\n",
        "    for w in arr:\n",
        "      word_arr = re.split('\\W+', w.lower())\n",
        "      new_arr = new_arr + word_arr\n",
        "    # print(new_arr)\n",
        "    return new_arr\n",
        "  \n",
        "  def sim_score(self, docs, keywords):\n",
        "    keywords = self.split_and_join_arr(keywords)\n",
        "    gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in docs]\n",
        "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "    tf_idf = gensim.models.TfidfModel(corpus)\n",
        "    sims = gensim.similarities.Similarity('/usr/workdir',tf_idf[corpus],\n",
        "                                      num_features=len(dictionary))\n",
        "\n",
        "    query_doc_bow = dictionary.doc2bow(keywords)\n",
        "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "    # print(sims[query_doc_tf_idf])\n",
        "    return sims[query_doc_tf_idf]\n",
        "\n",
        "  def print_results(self, match_job_postings):\n",
        "    for i, resume in enumerate(match_job_postings):\n",
        "      print('------------------------\\n')\n",
        "      print(\"Matching Job Posting for Resume %s\" % (i + 1))\n",
        "      for classifier_obj in resume:\n",
        "        classifier_type = list(classifier_obj.keys())[0]\n",
        "        print(classifier_type)\n",
        "        print(classifier_obj[classifier_type]['job_title'])\n",
        "        print(classifier_obj[classifier_type]['matching_job'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9IBrSshaKbM",
        "outputId": "fd20fccc-5a98-4755-b952-2b22e12e8e02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (0.42.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Driver Code"
      ],
      "metadata": {
        "id": "FIi95ge84CGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "9rzIUXh-7cLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd     # pip install pandas. usage: loading data from csv files into dataframes\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import contractions\n",
        "import spacy\n",
        "from numpy.lib.npyio import savez_compressed\n",
        "from array import *\n",
        "nltk.download(\"all\")\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "YpAjFMcT6H3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Helper Functions\n",
        "\n",
        "# Function to retrieve text data.\n",
        "# (either 1 or more job postings or resumes)?\n",
        "def getFileData(filename, dir):\n",
        "    return pd.read_csv('data/' + dir + '/' + filename)\n",
        "\n",
        "# Function to normalize text data. \n",
        "# (some skill extraction tools will normalize text for us; however, if not, this function is here)\n",
        "# includes removing stopwords, punctuation, dates, links, etc...\n",
        "def normalizeCorpus(corpus):\n",
        "    \n",
        "\n",
        "    return corpus\n",
        "    # need to fix this \n",
        "    # nltk_tokenList = word_tokenize(corpus)\n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # nltk_lemmaList = []\n",
        "    # for word in nltk_tokenList:\n",
        "    #     nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # normalized_corpus = []  \n",
        "    # nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "    # for w in nltk_lemmaList:  \n",
        "    #     if w not in nltk_stop_words:  \n",
        "    #         normalized_corpus.append(w)\n",
        "\n",
        "    # punctuation = \";:.,?!\"\n",
        "    # for word in normalized_corpus:\n",
        "    #     if word in punctuation:\n",
        "    #         normalized_corpus.remove(word)\n",
        "\n",
        "    #still need to add dates and links\n",
        "    # return normalized_corpus\n",
        "\n",
        "\n",
        "    \n",
        "def similiartychecker(tokenA, tokenB):\n",
        "\n",
        "    A = nlp(tokenA)\n",
        "    B = nlp(tokenB)\n",
        "  \n",
        "    score = A.similarity(B)\n",
        "  \n",
        "    return score\n",
        "\n",
        "# Function to extract skill words from a given corpus.\n",
        "# ideally, this function will output a set of skills extracted from the corpus\n",
        "def extractSkills(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT, avgBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    keywordsYAKE, scoresYAKE, avgYAKE = extractKeywordsYAKE(corpus)\n",
        "\n",
        "    # extraction method 3\n",
        "    keywordsTextRank, scoresTextRank, avgTextRank = extractKeywordsTextRank(corpus)\n",
        "\n",
        "    # return keywordsBERT, keywordsYAKE[0], keywordsTextRank[0]\n",
        "    # return keywordsBERT, keywordsCSO, keywordsYAKE, keywordsTextRank,\n",
        "    return {\"keywordsBERT\": keywordsBERT,\n",
        "            \"keywordsCSO\": keywordsCSO,\n",
        "            \"keywordsYAKE\": keywordsYAKE}\n",
        "            # \"keywordsTextRAnk\": keywordsTextRank}\n",
        "\n",
        "def extractSkillsforcomp(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT, avgBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    keywordsYAKE, scoresYAKE, avgYAKE = extractKeywordsYAKE(corpus)\n",
        "\n",
        "    # extraction method 3\n",
        "    keywordsTextRank, scoresTextRank, avgTextRank = extractKeywordsTextRank(corpus)\n",
        "\n",
        "    # return keywordsBERT, keywordsYAKE[0], keywordsTextRank[0]\n",
        "    return keywordsBERT, keywordsCSO, keywordsYAKE, keywordsTextRank\n",
        "\n",
        "   \n",
        "### Driver Code\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # fetch the data\n",
        "    job_posting_obj = queryJobsAPI(\"Data Scientist\", \"Washington, DC\")\n",
        "    resume_dataframe = getFileData('kaggleResumes.csv', 'resumes')\n",
        "    #---------------\n",
        "\n",
        "    '''\n",
        "    # print the dataframes\n",
        "    print('DataFrame of Job Postings:')\n",
        "    print(job_posting_dataframe)    \n",
        "    print()\n",
        "\n",
        "    print('DataFrame of Resumes:')\n",
        "    print(resume_dataframe)\n",
        "    print()\n",
        "    #----------------------\n",
        "    '''\n",
        "\n",
        "    # fetch the job descriptions and resumes by themselves\n",
        "    jpCorpus = []\n",
        "    jpTitles = []\n",
        "\n",
        "    for i in range(len(job_posting_obj)):\n",
        "      jpCorpus.append(job_posting_obj[i]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "      jpTitles.append(job_posting_obj[i]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "\n",
        "    rCorpus  = list(resume_dataframe['Resume'])\n",
        "    #----------------------\n",
        "\n",
        "    # # Number of resume samples to view\n",
        "    # NUM_SAMPLES = 10\n",
        "    NUM_SAMPLES = 1\n",
        "    # # number of job postings to view\n",
        "    # NUM_JPS = 10\n",
        "    NUM_JPS = 1\n",
        "\n",
        "    # for each JOB POSTING from the corpus\n",
        "    i = 0\n",
        "    all_job_postings = []\n",
        "    for posting in jpCorpus:\n",
        "        print('Job Posting #', i+1)\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(posting)\n",
        "        job_posting_json = extractSkills(text)\n",
        "        all_job_postings.append(text)\n",
        "\n",
        "        #for similiarity referencer:\n",
        "        jobKeyBERTsimComp, jobCSOsimComp, jobYAKEsimComp, jobTextRanksimComp = extractSkillsforcomp(text)\n",
        "       \n",
        "\n",
        "        # print lines, we are done with this posting\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X postings\n",
        "        i += 1\n",
        "        if i > NUM_JPS:\n",
        "            break\n",
        "    #---------------------- \n",
        "\n",
        "    # for each RESUME from the corpus\n",
        "    i = 0\n",
        "    all_resumes_keywords = []\n",
        "    for resume in rCorpus:\n",
        "        print('Resume #', i+1)\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(resume)\n",
        "        resume_json = extractSkills(text)\n",
        "        all_resumes_keywords.append(resume_json)\n",
        "\n",
        "        #for similiarity referencer:\n",
        "        resKeyBERTsimComp, resCSOsimComp, resYAKEsimComp, resTextRanksimComp = extractSkillsforcomp(text)\n",
        "\n",
        "        # print lines, we are done with this resume\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X resumes\n",
        "        i += 1\n",
        "        if i > NUM_SAMPLES:\n",
        "            break\n",
        "    #---------------------- \n",
        "    '''\n",
        "    ----------------------------------------------------------------------|\n",
        "    |coss reference similiary of resume and job posting extracted keywords|\n",
        "    |-addtion by Tyrell Richardson                                        |\n",
        "    |                                                                     |\n",
        "    |---------------------------------------------------------------------|\n",
        "    '''\n",
        "\n",
        "    keyBERTSimiliarity = np.zeros(14)\n",
        "\n",
        "    for k in range (0,14):\n",
        "        keyBERTSimiliarity[k] = similiartychecker(jobKeyBERTsimComp[k], resKeyBERTsimComp[k])\n",
        "\n",
        "    print('KeyBERT match Similarity Mean: '+str(np.mean(keyBERTSimiliarity)))\n",
        "    \n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    CSOSimilarity = np.zeros(len(min(jobCSOsimComp, resCSOsimComp)))\n",
        "\n",
        "    for h in range (0,3):\n",
        "      CSOSimilarity[h] = similiartychecker(jobCSOsimComp[h],resCSOsimComp[h])\n",
        "\n",
        "    print('CSO match Similiarity Mean: '+str(np.mean(CSOSimilarity)))\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    YakeSimilarity = np.zeros(len(min(jobYAKEsimComp, resYAKEsimComp)))\n",
        "    for i in range (0,19):\n",
        "      YakeSimilarity[i] = similiartychecker(jobYAKEsimComp[i],resYAKEsimComp[i])\n",
        "\n",
        "    print('Yake match Similiarity Mean: '+str(np.mean(YakeSimilarity)))\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    TextRankSimiliarity = np.zeros(5)\n",
        "    for j in range (0, 5):\n",
        "      TextRankSimiliarity[j] = similiartychecker(jobTextRanksimComp[j], resTextRanksimComp[j])\n",
        "\n",
        "    print('TextRank match Similarity Mean: '+str(np.mean(TextRankSimiliarity)))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# keep going!\n",
        "\n",
        "# end of driver code\n",
        "#---------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6HTo9qA4Fxs",
        "outputId": "07472e10-4749-4af1-cb96-3459a025d10c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Scientist\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "Job Posting # 1\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist provide                   0.6452\n",
            "electronic records management            0.436\n",
            "law enforcement programs                 0.4179\n",
            "systems                                  0.2913\n",
            "worn camera electronic                   0.2899\n",
            "park service law                         0.2727\n",
            "strengthen public trust                  0.2479\n",
            "transparency availability                0.2332\n",
            "remote position                          0.1724\n",
            "office created                           0.1639\n",
            "branch                                   0.1535\n",
            "accessibility                            0.141\n",
            "specific body                            0.1143\n",
            "national                                 0.094\n",
            "fully                                    0.0283\n",
            "\n",
            "Score Average: 0.2623714285714286\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "information management\n",
            "data management\n",
            "electronic records\n",
            "records management\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Technology Branch Office                 0.001481109174040813\n",
            "provide data management                  0.004659336811099895\n",
            "records management systems               0.005216835943208545\n",
            "Technology Branch                        0.0063933047560725985\n",
            "body worn camera                         0.006747317732994754\n",
            "electronic records management            0.0072077123711537915\n",
            "Data Scientist                           0.007651743677230578\n",
            "National Park Service                    0.01430530479782861\n",
            "Branch Office                            0.018178522502453803\n",
            "Public Trust                             0.018963124994821837\n",
            "strengthen public trust                  0.019324532985338896\n",
            "provide data                             0.022659745444925165\n",
            "data management                          0.024147626145501592\n",
            "management systems                       0.026987689958630504\n",
            "Park Service law                         0.029608428689612158\n",
            "analytical support                       0.034736319679920705\n",
            "body worn                                0.034736319679920705\n",
            "worn camera                              0.034736319679920705\n",
            "camera and electronic                    0.034736319679920705\n",
            "electronic records                       0.034736319679920705\n",
            "Score Average: 0.018551453405505073\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.301955340467862\n",
            "management                               0.29572417903919207\n",
            "park                                     0.26218228598341187\n",
            "enforcement                              0.26218228598341164\n",
            "remote                                   0.2621822859834115\n",
            "worn                                     0.2621822859834114\n",
            "Score Average: 0.2744014439067834\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist provide                   0.6452\n",
            "electronic records management            0.436\n",
            "law enforcement programs                 0.4179\n",
            "systems                                  0.2913\n",
            "worn camera electronic                   0.2899\n",
            "park service law                         0.2727\n",
            "strengthen public trust                  0.2479\n",
            "transparency availability                0.2332\n",
            "remote position                          0.1724\n",
            "office created                           0.1639\n",
            "branch                                   0.1535\n",
            "accessibility                            0.141\n",
            "specific body                            0.1143\n",
            "national                                 0.094\n",
            "fully                                    0.0283\n",
            "\n",
            "Score Average: 0.2623714285714286\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "information management\n",
            "data management\n",
            "electronic records\n",
            "records management\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Technology Branch Office                 0.001481109174040813\n",
            "provide data management                  0.004659336811099895\n",
            "records management systems               0.005216835943208545\n",
            "Technology Branch                        0.0063933047560725985\n",
            "body worn camera                         0.006747317732994754\n",
            "electronic records management            0.0072077123711537915\n",
            "Data Scientist                           0.007651743677230578\n",
            "National Park Service                    0.01430530479782861\n",
            "Branch Office                            0.018178522502453803\n",
            "Public Trust                             0.018963124994821837\n",
            "strengthen public trust                  0.019324532985338896\n",
            "provide data                             0.022659745444925165\n",
            "data management                          0.024147626145501592\n",
            "management systems                       0.026987689958630504\n",
            "Park Service law                         0.029608428689612158\n",
            "analytical support                       0.034736319679920705\n",
            "body worn                                0.034736319679920705\n",
            "worn camera                              0.034736319679920705\n",
            "camera and electronic                    0.034736319679920705\n",
            "electronic records                       0.034736319679920705\n",
            "Score Average: 0.018551453405505073\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.301955340467862\n",
            "management                               0.29572417903919207\n",
            "park                                     0.26218228598341187\n",
            "enforcement                              0.26218228598341164\n",
            "remote                                   0.2621822859834115\n",
            "worn                                     0.2621822859834114\n",
            "Score Average: 0.2744014439067834\n",
            "------------------------\n",
            "\n",
            "Job Posting # 2\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "health informatics obrhi                 0.6482\n",
            "centers medicare medicaid                0.4176\n",
            "group data                               0.3836\n",
            "analytical statistical programming       0.3435\n",
            "position located department              0.3154\n",
            "scientist gs                             0.3021\n",
            "reduction                                0.2062\n",
            "cms                                      0.1765\n",
            "emerging                                 0.1474\n",
            "sets                                     0.1415\n",
            "burden                                   0.1377\n",
            "interpret unique                         0.1183\n",
            "mechanisms necessary collect             0.0853\n",
            "13                                       0.0808\n",
            "highly                                   0.0576\n",
            "\n",
            "Score Average: 0.25029285714285715\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "integrated data\n",
            "university\n",
            "programming languages\n",
            "service delivery\n",
            "target position\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Emerging Innovations Group               0.00010839280424678759\n",
            "specialized data sets                    0.0005597800499497894\n",
            "highly specialized data                  0.0007454122597000672\n",
            "Human Services                           0.0018855728774306911\n",
            "Medicaid Services                        0.0018855728774306911\n",
            "Centers for Medicare                     0.0019447784389868556\n",
            "Office of Burden                         0.0024500117744031833\n",
            "Emerging Innovations                     0.0024500117744031833\n",
            "Innovations Group                        0.0024500117744031833\n",
            "Health Informatics                       0.0027427897865850287\n",
            "Burden Reduction                         0.003086903891076617\n",
            "Data Scientist                           0.0035038272852105624\n",
            "data sets                                0.005760446985463852\n",
            "implement the analytical                 0.006976324594720111\n",
            "specialized data                         0.007655593716194504\n",
            "position is located                      0.009275175508151086\n",
            "programming mechanisms                   0.009275175508151086\n",
            "interpret unique                         0.009275175508151086\n",
            "unique and highly                        0.009275175508151086\n",
            "highly specialized                       0.009275175508151086\n",
            "Score Average: 0.004279270153832076\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "services                                 0.32103624195484254\n",
            "data                                     0.27784070084032225\n",
            "mechanisms                               0.23957589467244314\n",
            "organize                                 0.23957589467244297\n",
            "scientist                                0.1945584756514095\n",
            "specialized                              0.19455847565140943\n",
            "Score Average: 0.24452428057381165\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "health informatics obrhi                 0.6482\n",
            "centers medicare medicaid                0.4176\n",
            "group data                               0.3836\n",
            "analytical statistical programming       0.3435\n",
            "position located department              0.3154\n",
            "scientist gs                             0.3021\n",
            "reduction                                0.2062\n",
            "cms                                      0.1765\n",
            "emerging                                 0.1474\n",
            "sets                                     0.1415\n",
            "burden                                   0.1377\n",
            "interpret unique                         0.1183\n",
            "mechanisms necessary collect             0.0853\n",
            "13                                       0.0808\n",
            "highly                                   0.0576\n",
            "\n",
            "Score Average: 0.25029285714285715\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "integrated data\n",
            "university\n",
            "programming languages\n",
            "service delivery\n",
            "target position\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Emerging Innovations Group               0.00010839280424678759\n",
            "specialized data sets                    0.0005597800499497894\n",
            "highly specialized data                  0.0007454122597000672\n",
            "Human Services                           0.0018855728774306911\n",
            "Medicaid Services                        0.0018855728774306911\n",
            "Centers for Medicare                     0.0019447784389868556\n",
            "Office of Burden                         0.0024500117744031833\n",
            "Emerging Innovations                     0.0024500117744031833\n",
            "Innovations Group                        0.0024500117744031833\n",
            "Health Informatics                       0.0027427897865850287\n",
            "Burden Reduction                         0.003086903891076617\n",
            "Data Scientist                           0.0035038272852105624\n",
            "data sets                                0.005760446985463852\n",
            "implement the analytical                 0.006976324594720111\n",
            "specialized data                         0.007655593716194504\n",
            "position is located                      0.009275175508151086\n",
            "programming mechanisms                   0.009275175508151086\n",
            "interpret unique                         0.009275175508151086\n",
            "unique and highly                        0.009275175508151086\n",
            "highly specialized                       0.009275175508151086\n",
            "Score Average: 0.004279270153832076\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "services                                 0.32103624195484254\n",
            "data                                     0.27784070084032225\n",
            "mechanisms                               0.23957589467244314\n",
            "organize                                 0.23957589467244297\n",
            "scientist                                0.1945584756514095\n",
            "specialized                              0.19455847565140943\n",
            "Score Average: 0.24452428057381165\n",
            "------------------------\n",
            "\n",
            "Resume # 1\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "developing automated review              0.5601\n",
            "associate data science                   0.4096\n",
            "lawyers                                  0.3539\n",
            "portfolio synthesizes intelligence       0.3337\n",
            "tar technology assisted                  0.2238\n",
            "relevant recommended                     0.2234\n",
            "bayes knn random                         0.213\n",
            "platform fraud                           0.2127\n",
            "tableau regular expression               0.196\n",
            "word embedding                           0.1828\n",
            "elasticsearch d3 js                      0.1816\n",
            "exprience 24 months                      0.1683\n",
            "kafka python flask                       0.0694\n",
            "pii personally                           0.0692\n",
            "jquery css bootstrap                     0.0626\n",
            "\n",
            "Score Average: 0.24267857142857144\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "random forests\n",
            "css\n",
            "decision trees\n",
            "cyber-attacks\n",
            "vehicles\n",
            "sentiment analysis\n",
            "classification models\n",
            "python\n",
            "text data\n",
            "visualization\n",
            "text document\n",
            "programming languages\n",
            "html\n",
            "natural languages\n",
            "ajax\n",
            "classifiers\n",
            "user information\n",
            "k-nearest neighbors\n",
            "cluster analysis\n",
            "computer vision\n",
            "natural language processing\n",
            "svm\n",
            "database systems\n",
            "customer review\n",
            "javascript\n",
            "boosting\n",
            "question answering\n",
            "dimensionality reduction\n",
            "word embedding\n",
            "deep learning\n",
            "erp system\n",
            "machine learning\n",
            "java\n",
            "visualization tools\n",
            "recommendation\n",
            "textual data\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Programming Languages                    0.011097799901366744\n",
            "Science Assurance Associate              0.013949751329009795\n",
            "Data Science Assurance                   0.022415562610591837\n",
            "Natural Language processing              0.024842409502224133\n",
            "Natural Language                         0.030869805965504205\n",
            "Assurance Associate Data                 0.03253356899175182\n",
            "Assurance Associate                      0.034404802981160124\n",
            "Data                                     0.036070315641983854\n",
            "Topic Modelling                          0.036510412807599535\n",
            "Python                                   0.04010119489686849\n",
            "Analysis                                 0.041907382019738176\n",
            "Random Forest                            0.04201651955321839\n",
            "Neural Nets                              0.04201651955321839\n",
            "Young LLP                                0.042070408278407044\n",
            "TECHNOLOGY ASSISTED REVIEW               0.04448734350858905\n",
            "Dispute Services Assurance               0.04467877095035721\n",
            "Science Assurance                        0.04616412764960541\n",
            "Details Data Science                     0.04916688874052016\n",
            "Sentiment Analysis                       0.04960901425112712\n",
            "Associate Data Science                   0.04969096207192514\n",
            "Score Average: 0.03604803153330745\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.24032115573231244\n",
            "information                              0.23792037683359116\n",
            "informed                                 0.23792037683359116\n",
            "review                                   0.16945488184144417\n",
            "reviews                                  0.16945488184144417\n",
            "analysis                                 0.1557761190261625\n",
            "tools                                    0.14990505274693028\n",
            "customer                                 0.1434033015248964\n",
            "customized                               0.1434033015248964\n",
            "python                                   0.14294280784628546\n",
            "word                                     0.1388655007731966\n",
            "words                                    0.1388655007731966\n",
            "modelling                                0.13778698447654159\n",
            "models                                   0.13778698447654159\n",
            "learning                                 0.13418402881458724\n",
            "analytics                                0.13259469151763387\n",
            "analytic                                 0.13259469151763387\n",
            "platform tool                            0.12447527268204243\n",
            "questions                                0.12160898323451332\n",
            "Score Average: 0.15732973126407584\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "developing automated review              0.5601\n",
            "associate data science                   0.4096\n",
            "lawyers                                  0.3539\n",
            "portfolio synthesizes intelligence       0.3337\n",
            "tar technology assisted                  0.2238\n",
            "relevant recommended                     0.2234\n",
            "bayes knn random                         0.213\n",
            "platform fraud                           0.2127\n",
            "tableau regular expression               0.196\n",
            "word embedding                           0.1828\n",
            "elasticsearch d3 js                      0.1816\n",
            "exprience 24 months                      0.1683\n",
            "kafka python flask                       0.0694\n",
            "pii personally                           0.0692\n",
            "jquery css bootstrap                     0.0626\n",
            "\n",
            "Score Average: 0.24267857142857144\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "random forests\n",
            "css\n",
            "decision trees\n",
            "cyber-attacks\n",
            "vehicles\n",
            "sentiment analysis\n",
            "classification models\n",
            "python\n",
            "text data\n",
            "visualization\n",
            "text document\n",
            "programming languages\n",
            "html\n",
            "natural languages\n",
            "ajax\n",
            "classifiers\n",
            "user information\n",
            "k-nearest neighbors\n",
            "cluster analysis\n",
            "computer vision\n",
            "natural language processing\n",
            "svm\n",
            "database systems\n",
            "customer review\n",
            "javascript\n",
            "boosting\n",
            "question answering\n",
            "dimensionality reduction\n",
            "word embedding\n",
            "deep learning\n",
            "erp system\n",
            "machine learning\n",
            "java\n",
            "visualization tools\n",
            "recommendation\n",
            "textual data\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Programming Languages                    0.011097799901366744\n",
            "Science Assurance Associate              0.013949751329009795\n",
            "Data Science Assurance                   0.022415562610591837\n",
            "Natural Language processing              0.024842409502224133\n",
            "Natural Language                         0.030869805965504205\n",
            "Assurance Associate Data                 0.03253356899175182\n",
            "Assurance Associate                      0.034404802981160124\n",
            "Data                                     0.036070315641983854\n",
            "Topic Modelling                          0.036510412807599535\n",
            "Python                                   0.04010119489686849\n",
            "Analysis                                 0.041907382019738176\n",
            "Random Forest                            0.04201651955321839\n",
            "Neural Nets                              0.04201651955321839\n",
            "Young LLP                                0.042070408278407044\n",
            "TECHNOLOGY ASSISTED REVIEW               0.04448734350858905\n",
            "Dispute Services Assurance               0.04467877095035721\n",
            "Science Assurance                        0.04616412764960541\n",
            "Details Data Science                     0.04916688874052016\n",
            "Sentiment Analysis                       0.04960901425112712\n",
            "Associate Data Science                   0.04969096207192514\n",
            "Score Average: 0.03604803153330745\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.24032115573231244\n",
            "information                              0.23792037683359116\n",
            "informed                                 0.23792037683359116\n",
            "review                                   0.16945488184144417\n",
            "reviews                                  0.16945488184144417\n",
            "analysis                                 0.1557761190261625\n",
            "tools                                    0.14990505274693028\n",
            "customer                                 0.1434033015248964\n",
            "customized                               0.1434033015248964\n",
            "python                                   0.14294280784628546\n",
            "word                                     0.1388655007731966\n",
            "words                                    0.1388655007731966\n",
            "modelling                                0.13778698447654159\n",
            "models                                   0.13778698447654159\n",
            "learning                                 0.13418402881458724\n",
            "analytics                                0.13259469151763387\n",
            "analytic                                 0.13259469151763387\n",
            "platform tool                            0.12447527268204243\n",
            "questions                                0.12160898323451332\n",
            "Score Average: 0.15732973126407584\n",
            "------------------------\n",
            "\n",
            "Resume # 2\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist matelabs                  0.7015\n",
            "mixed attributes company                 0.4055\n",
            "time series forecasting                  0.3483\n",
            "12 months aws                            0.3261\n",
            "keras exprience                          0.2863\n",
            "deployed auto preprocessing              0.244\n",
            "value treatment outlier                  0.2424\n",
            "sukh sagar                               0.2126\n",
            "feature selection dimensionality         0.2047\n",
            "india developed                          0.191\n",
            "winter prophet worked                    0.1604\n",
            "extracting problem github                0.1406\n",
            "block achievements tasks                 0.1246\n",
            "art research paper                       0.0609\n",
            "mainly                                   0.0545\n",
            "\n",
            "Score Average: 0.2606357142857143\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "education\n",
            "business process\n",
            "forecasting models\n",
            "genetic selection\n",
            "machine learning\n",
            "regression model\n",
            "dimensionality reduction\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Scientist Data Scientist                 0.013992378784693497\n",
            "Exprience                                0.016594253201207266\n",
            "year months                              0.017817825504942618\n",
            "Data Scientist Data                      0.019595468127241167\n",
            "Data Scientist                           0.030969289830867597\n",
            "months                                   0.04331271186058463\n",
            "year                                     0.05246182643419103\n",
            "Scientist Data                           0.061938579661735195\n",
            "Education Details                        0.06547309831103336\n",
            "Skill Details Python                     0.06836827484444442\n",
            "Details Python                           0.08824715786075003\n",
            "Details                                  0.09657487095585507\n",
            "Matelabs                                 0.09782011222698457\n",
            "Scientist                                0.10369988605522729\n",
            "Matelabs description                     0.10543028765049242\n",
            "year monthsCompany Details               0.11789468317651851\n",
            "Data                                     0.1406123844385533\n",
            "Skill Details                            0.20058132697401396\n",
            "Education                                0.21209723796184535\n",
            "UIT-RGPV                                 0.21209723796184535\n",
            "Score Average: 0.08176219230848322\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "months                                   0.25497307513650697\n",
            "exprience                                0.25443855603882914\n",
            "feature                                  0.20019113243878428\n",
            "details                                  0.1961765911917297\n",
            "model                                    0.19484293012294743\n",
            "matelabs                                 0.1680577208177487\n",
            "deployed                                 0.16322877744950048\n",
            "professionals                            0.15094412568526502\n",
            "research                                 0.1509441256852645\n",
            "company                                  0.12772640163758137\n",
            "india                                    0.12715820471591244\n",
            "sagar                                    0.12715820471591238\n",
            "achievements                             0.12715820471591213\n",
            "th                                       0.12715820471591208\n",
            "bengaluru                                0.12365061911560406\n",
            "Score Average: 0.16625379161222736\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist matelabs                  0.7015\n",
            "mixed attributes company                 0.4055\n",
            "time series forecasting                  0.3483\n",
            "12 months aws                            0.3261\n",
            "keras exprience                          0.2863\n",
            "deployed auto preprocessing              0.244\n",
            "value treatment outlier                  0.2424\n",
            "sukh sagar                               0.2126\n",
            "feature selection dimensionality         0.2047\n",
            "india developed                          0.191\n",
            "winter prophet worked                    0.1604\n",
            "extracting problem github                0.1406\n",
            "block achievements tasks                 0.1246\n",
            "art research paper                       0.0609\n",
            "mainly                                   0.0545\n",
            "\n",
            "Score Average: 0.2606357142857143\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "education\n",
            "business process\n",
            "forecasting models\n",
            "genetic selection\n",
            "machine learning\n",
            "regression model\n",
            "dimensionality reduction\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "Scientist Data Scientist                 0.013992378784693497\n",
            "Exprience                                0.016594253201207266\n",
            "year months                              0.017817825504942618\n",
            "Data Scientist Data                      0.019595468127241167\n",
            "Data Scientist                           0.030969289830867597\n",
            "months                                   0.04331271186058463\n",
            "year                                     0.05246182643419103\n",
            "Scientist Data                           0.061938579661735195\n",
            "Education Details                        0.06547309831103336\n",
            "Skill Details Python                     0.06836827484444442\n",
            "Details Python                           0.08824715786075003\n",
            "Details                                  0.09657487095585507\n",
            "Matelabs                                 0.09782011222698457\n",
            "Scientist                                0.10369988605522729\n",
            "Matelabs description                     0.10543028765049242\n",
            "year monthsCompany Details               0.11789468317651851\n",
            "Data                                     0.1406123844385533\n",
            "Skill Details                            0.20058132697401396\n",
            "Education                                0.21209723796184535\n",
            "UIT-RGPV                                 0.21209723796184535\n",
            "Score Average: 0.08176219230848322\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "months                                   0.25497307513650697\n",
            "exprience                                0.25443855603882914\n",
            "feature                                  0.20019113243878428\n",
            "details                                  0.1961765911917297\n",
            "model                                    0.19484293012294743\n",
            "matelabs                                 0.1680577208177487\n",
            "deployed                                 0.16322877744950048\n",
            "professionals                            0.15094412568526502\n",
            "research                                 0.1509441256852645\n",
            "company                                  0.12772640163758137\n",
            "india                                    0.12715820471591244\n",
            "sagar                                    0.12715820471591238\n",
            "achievements                             0.12715820471591213\n",
            "th                                       0.12715820471591208\n",
            "bengaluru                                0.12365061911560406\n",
            "Score Average: 0.16625379161222736\n",
            "------------------------\n",
            "\n",
            "KeyBERT match Similarity Mean: 0.32140456064241885\n",
            "CSO match Similiarity Mean: 0.22399092724487232\n",
            "Yake match Similiarity Mean: 0.3907436461314949\n",
            "TextRank match Similarity Mean: 0.4610943917020035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7U8PxgRxBrD",
        "outputId": "66886b77-aa70-4920-b269-d0e238b1f9a0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['health informatics obrhi', 'centers medicare medicaid', 'group data', 'analytical statistical programming', 'position located department', 'scientist gs', 'reduction', 'cms', 'emerging', 'sets', 'burden', 'interpret unique', 'mechanisms necessary collect', '13', 'highly'], ['integrated data', 'university', 'programming languages', 'service delivery', 'target position'], ['Emerging Innovations Group', 'specialized data sets', 'highly specialized data', 'Human Services', 'Medicaid Services', 'Centers for Medicare', 'Office of Burden', 'Emerging Innovations', 'Innovations Group', 'Health Informatics', 'Burden Reduction', 'Data Scientist', 'data sets', 'implement the analytical', 'specialized data', 'position is located', 'programming mechanisms', 'interpret unique', 'unique and highly', 'highly specialized'], ['services', 'data', 'mechanisms', 'organize', 'scientist', 'specialized'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume and Job Matching"
      ],
      "metadata": {
        "id": "MsExUakXPC_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume and Job Posting Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, all_resumes_keywords)\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)"
      ],
      "metadata": {
        "id": "LQHvfMS9l7Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd6f1cd-01f9-49c2-be77-2277cc336d7a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------\n",
            "\n",
            "Matching Job Posting for Resume 1\n",
            "keywordsBERT\n",
            "Data Scientist\n",
            "As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "keywordsCSO\n",
            "Data Scientist\n",
            "As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "keywordsYAKE\n",
            "Data Scientist\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "------------------------\n",
            "\n",
            "Matching Job Posting for Resume 2\n",
            "keywordsBERT\n",
            "Data Scientist\n",
            "As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "keywordsCSO\n",
            "Data Scientist\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "keywordsYAKE\n",
            "Data Scientist\n",
            "As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through the transparency, availability, and accessibility of information. This is a fully remote position.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Upload"
      ],
      "metadata": {
        "id": "xxp4OHVTXpSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and Drop a Data Scientist resum into Colab \n",
        "\n",
        "Make sure the file is named **resume.pdf**\n",
        "\n",
        "There is a dummy resume located in the `/data` directory.\n",
        "\n",
        "\n",
        "\n",
        "< - - - - - - -"
      ],
      "metadata": {
        "id": "1RCg41ohJkHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer3"
      ],
      "metadata": {
        "id": "PPWWB4B9YP5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9587cfd4-59c1-4999-b6a5-09586b149735"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer3\n",
            "  Downloading pdfminer3-2018.12.3.0.tar.gz (5.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (2.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (3.0.4)\n",
            "Building wheels for collected packages: pdfminer3\n",
            "  Building wheel for pdfminer3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer3: filename=pdfminer3-2018.12.3.0-py3-none-any.whl size=117823 sha256=f066bbfc81d96d284c624077bdbc6e3b56951403c5ccd077575ce3daaecda34b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/bc/f6/b518c318a55ab9e6d72092195cb9b49cac9ca60d6e000d0a1c\n",
            "Successfully built pdfminer3\n",
            "Installing collected packages: pycryptodome, pdfminer3\n",
            "Successfully installed pdfminer3-2018.12.3.0 pycryptodome-3.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer3.layout import LAParams, LTTextBox\n",
        "from pdfminer3.pdfpage import PDFPage\n",
        "from pdfminer3.pdfinterp import PDFResourceManager\n",
        "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer3.converter import PDFPageAggregator\n",
        "from pdfminer3.converter import TextConverter\n",
        "import io\n",
        "import sys\n",
        "from contextlib import redirect_stdout\n",
        "from io import StringIO \n",
        "\n",
        "resource_manager = PDFResourceManager()\n",
        "fake_file_handle = io.StringIO()\n",
        "converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
        "page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "\n",
        "with open('resume.pdf', 'rb') as fh:\n",
        "\n",
        "    for page in PDFPage.get_pages(fh,\n",
        "                                  caching=True,\n",
        "                                  check_extractable=True):\n",
        "        page_interpreter.process_page(page)\n",
        "\n",
        "    text = fake_file_handle.getvalue()\n",
        "\n",
        "# close open handles\n",
        "converter.close()\n",
        "fake_file_handle.close()\n",
        "\n",
        "# normalize and extract skills \n",
        "text = normalizeCorpus(text)\n",
        "resume_kw = extractSkills(text)\n",
        "\n",
        "# Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, [resume_kw])\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)"
      ],
      "metadata": {
        "id": "3vsrbN-UXsG8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "8788942f-eb85-4047-8d5f-3c239ab6cfad"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-3f2f897f1ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpage_interpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resume.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     for page in PDFPage.get_pages(fh,\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resume.pdf'"
          ]
        }
      ]
    }
  ]
}