{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#README.txt"
      ],
      "metadata": {
        "id": "VqvVC9cM4PPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and drop a ZIPPED version of the folder `/data` from GitHub repo into Colab.\n",
        "\n",
        "< - - - - -"
      ],
      "metadata": {
        "id": "SkYHORUB4Rig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# then run this and refresh directory...\n",
        "\n",
        "# unzip datasets\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxJqWMSD5rS3",
        "outputId": "de7bce4b-1034-4e1d-c33d-ce5c1a2624e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/job-postings/\n",
            "  inflating: data/job-postings/Data_Job_NY.csv  \n",
            "  inflating: data/job-postings/Data_Job_SF.csv  \n",
            "  inflating: data/job-postings/Data_Job_TX.csv  \n",
            "  inflating: data/job-postings/Data_Job_WA.csv  \n",
            "  inflating: data/README.txt         \n",
            "  inflating: data/resume.pdf         \n",
            "   creating: data/resumes/\n",
            "  inflating: data/resumes/kaggleResumes.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# API"
      ],
      "metadata": {
        "id": "anBUXima7kjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v4wViL775dD",
        "outputId": "c6ffe23e-73f1-4149-aef3-de576c91ccf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting config\n",
            "  Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: config\n",
            "Successfully installed config-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import config\n",
        "# API Docs found here: https://developer.usajobs.gov/Tutorials/Search-Jobs"
      ],
      "metadata": {
        "id": "uNcEHO9t7rKr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.US_JOBS_API_KEY = \"xVc0TZiLfhcr17ci7Ngk6bLAetdRVFgntm2pZgWNtww=\"\n",
        "config.EMAIL_ADDRESS = \"gjacobthomas@gmail.com\""
      ],
      "metadata": {
        "id": "sobm0urbDUff"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I'm assuming this API has some limiter on it so we don't want to lose access. -tyler"
      ],
      "metadata": {
        "id": "89Ei-XMSDhIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# more parameters are found here: https://developer.usajobs.gov/API-Reference/GET-api-Search\n",
        "def queryJobsAPI(keyword, location):\n",
        "  host = 'data.usajobs.gov' \n",
        "  # add these values in the config.py file\n",
        "  userAgent = config.EMAIL_ADDRESS\n",
        "  authKey = config.US_JOBS_API_KEY\n",
        "\n",
        "  base_url = \"https://data.usajobs.gov/api/search\"\n",
        "\n",
        "  parameters = {\n",
        "      \"Keyword\": keyword,\n",
        "      \"LocationName\": location\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "      \"Host\": host,          \n",
        "      \"User-Agent\": userAgent,          \n",
        "      \"Authorization-Key\": authKey  \n",
        "  }\n",
        "\n",
        "  resp = requests.request(\"GET\", base_url,headers=headers, params=parameters)\n",
        "  result = resp.json()['SearchResult']['SearchResultItems']\n",
        "\n",
        "  # get Job Title \n",
        "  print(result[1]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "  # get Job Summary\n",
        "  print(result[1]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "SVoxo2K47j_l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# KeyBERT Extraction Function"
      ],
      "metadata": {
        "id": "GVU9pbVd3-hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "-RUKzBOv5-_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5f595f-54bd-47a0-aaa6-9c4452932c08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.21.6)\n",
            "Collecting rich>=10.4.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (4.1.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.13.1+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 67.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.9.24)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Building wheels for collected packages: keybert, sentence-transformers\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23800 sha256=7d03e4ee3a19d7307c78831aa1b1ed4b15cd46b88f3f8772a7fa19ca7d1b2208\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/bc/8b/a51bee77aec33895e6c8c236144b4cc10875659c4d2c80f070\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=356242bd0129c9050436fb16cce045bac8fc828626bb85f0959fa9d37eae7279\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built keybert sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, commonmark, sentence-transformers, rich, keybert\n",
            "Successfully installed commonmark-0.9.1 huggingface-hub-0.11.1 keybert-0.7.0 rich-12.6.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HIogy6WO3hvX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from keybert import KeyBERT # pip install keybert (give it a minute...)\n",
        "\n",
        "'''\n",
        "/*---------------------------------------------------------------------\n",
        " |  Method: extractKeywordsBERT\n",
        " |\n",
        " |  Purpose: Uses the KeyBert Keyword Extraction Tool to extract\n",
        " |           and return keywords from a given corpus. \n",
        " |      \n",
        " |  Author: Tyler Parks\n",
        " |  Created On: 10/30/22\n",
        " |\n",
        " |  Parameters:\n",
        " |      normalized_corpus -- A single string containing all text of the\n",
        " |                           normalized corpus.\n",
        " |\n",
        " |  Returns: \n",
        " |      keywords -- List of collected keywords\n",
        " |      scores -- List of those keyword's scores\n",
        " |\n",
        " |  References: https://maartengr.github.io/KeyBERT/#usage\n",
        " |\n",
        " *-------------------------------------------------------------------*/\n",
        "''' \n",
        "def extractKeywordsBERT(normalized_corpus):   \n",
        "    print('---KeyBert Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    # init. language model \n",
        "    language_model = KeyBERT(model = 'all-mpnet-base-v2')\n",
        "\n",
        "    # extract those keywords!\n",
        "    data = language_model.extract_keywords( normalized_corpus, \n",
        "                                            keyphrase_ngram_range=(1, 3), \n",
        "                                            stop_words='english',\n",
        "                                            use_maxsum=False, \n",
        "                                            use_mmr=True,\n",
        "                                            diversity=0.7,\n",
        "                                            nr_candidates=20, \n",
        "                                            top_n=15\n",
        "                                        )\n",
        "\n",
        "    # zip the lists\n",
        "    zipped = list(map(list, zip(*data)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "    print('-Skill-'.ljust(40), '-Score-')\n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "    print()\n",
        "\n",
        "    return keywords, scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# CSO-Classifier Extraction Function"
      ],
      "metadata": {
        "id": "vmHHZLv57ex9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install cso-classifier"
      ],
      "metadata": {
        "id": "CXlOaYQj7jDa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d3dde4d-77e7-422b-d3a8-48ddef401da5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-03 19:28:54.681641: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cso-classifier\n",
            "  Downloading cso_classifier-3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting kneed==0.3.1\n",
            "  Downloading kneed-0.3.1.tar.gz (9.1 kB)\n",
            "Collecting python-Levenshtein==0.12.2\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting nltk==3.6.2\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 34.7 MB/s \n",
            "\u001b[?25hCollecting python-igraph==0.9.1\n",
            "  Downloading python_igraph-0.9.1-cp38-cp38-manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (1.21.6)\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.4 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting update-checker==0.18.0\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting hurry.filesize==0.9\n",
            "  Downloading hurry.filesize-0.9.tar.gz (2.8 kB)\n",
            "Collecting spacy==3.0.5\n",
            "  Downloading spacy-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 10.7 MB/s \n",
            "\u001b[?25hCollecting strsimpy==0.2.0\n",
            "  Downloading strsimpy-0.2.0-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (1.15.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (5.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from hurry.filesize==0.9->cso-classifier) (57.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from kneed==0.3.1->cso-classifier) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from kneed==0.3.1->cso-classifier) (1.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (2022.6.2)\n",
            "Collecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.7.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.0.7)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (3.0.10)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 59.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.9.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (3.0.8)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp38-cp38-manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3 MB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy==3.0.5->cso-classifier) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy==3.0.5->cso-classifier) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->kneed==0.3.1->cso-classifier) (3.1.0)\n",
            "Building wheels for collected packages: gensim, hurry.filesize, kneed, python-Levenshtein\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-3.8.1-cp38-cp38-linux_x86_64.whl size=25870889 sha256=b76899634f13361ac26a8bb8e825631d6e75af80533f0b13a98741a06e6fda48\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/de/03/7346ae70da7f980f78569668caf78fb2d678b176e549557c7d\n",
            "  Building wheel for hurry.filesize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hurry.filesize: filename=hurry.filesize-0.9-py3-none-any.whl size=4134 sha256=c75ac8758527c2f5b894fec85c293e2a7aba8640482b269d3c68de581b16c690\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/4b/2b/e1eaf7375b72542a9a3f3c3fa66b7098cc9e8048fe345deace\n",
            "  Building wheel for kneed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kneed: filename=kneed-0.3.1-py2.py3-none-any.whl size=7744 sha256=5bf5bd759e6d6edcc07739695eabcb4c2c8f1892e174ddeb93952ced76084271\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/67/e5/86665a048d2fba48bf33f9bdbbdc11494da2797f6329b30be3\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp38-cp38-linux_x86_64.whl size=149833 sha256=0984862ec15ff63d8bc15877943033c56be3ca156d6952f023e220062c0c4dd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/0c/76/042b46eb0df65c3ccd0338f791210c55ab79d209bcc269e2c7\n",
            "Successfully built gensim hurry.filesize kneed python-Levenshtein\n",
            "Installing collected packages: typer, pydantic, thinc, texttable, requests, update-checker, strsimpy, spacy, python-Levenshtein, python-igraph, nltk, kneed, hurry.filesize, gensim, cso-classifier\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.3\n",
            "    Uninstalling spacy-3.4.3:\n",
            "      Successfully uninstalled spacy-3.4.3\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.0.5 which is incompatible.\u001b[0m\n",
            "Successfully installed cso-classifier-3.0 gensim-3.8.1 hurry.filesize-0.9 kneed-0.3.1 nltk-3.6.2 pydantic-1.7.4 python-Levenshtein-0.12.2 python-igraph-0.9.1 requests-2.25.1 spacy-3.0.5 strsimpy-0.2.0 texttable-1.6.7 thinc-8.0.17 typer-0.3.2 update-checker-0.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't restart runtime if the terminal says so! Keep going.**"
      ],
      "metadata": {
        "id": "A0Ga47IZ8cH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# update spacy\n",
        "!pip install -U spacy\n",
        "import spacy\n",
        "from cso_classifier import CSOClassifier      # import classifier tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip21zn4U8Rxj",
        "outputId": "be00e23f-ea72-4a31-ead4-d30c6da23bf9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.0.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.7.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Downloading thinc-8.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (819 kB)\n",
            "\u001b[K     |████████████████████████████████| 819 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.0.5\n",
            "    Uninstalling spacy-3.0.5:\n",
            "      Successfully uninstalled spacy-3.0.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cso-classifier 3.0 requires spacy==3.0.5, but you have spacy 3.4.3 which is incompatible.\u001b[0m\n",
            "Successfully installed spacy-3.4.3 thinc-8.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the most recent model\n",
        "CSOClassifier.update() \n",
        "\n",
        "# define the model object\n",
        "CSO_Extractor = CSOClassifier(modules = \"both\", enhancement = \"first\", explanation = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLaIVJli8qOe",
        "outputId": "1dde7a9b-108f-42e4-c582-a98c9de974c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ======================================================\n",
            "#     ONTOLOGY\n",
            "# ======================================================\n",
            "Updating the ontology file\n",
            "Downloading the Computer Science Ontology from https://cso.kmi.open.ac.uk/download/version-3.3/cso_v3.3.csv\n",
            "[██████████████████████████████████████████████████] 7M/7M\n",
            "[*] Done!\n",
            "Extracting and converting ontology.\n",
            "Creating ontology pickle file from a copy of the CSO Ontology found in /usr/local/lib/python3.8/dist-packages/cso_classifier/assets/cso.csv\n",
            "Creating graph representation of the ontology.\n",
            "Saving the graph representation of the ontology (in a pickle object) in /usr/local/lib/python3.8/dist-packages/cso_classifier/assets/cso_graph.p\n",
            "\n",
            "# ======================================================\n",
            "#     MODELS: CACHED & WORD2VEC\n",
            "# ======================================================\n",
            "Couldn't delete file cached model: not found\n",
            "Couldn't delete file word2vec model: not found\n",
            "Updating the models: cached and word2vec\n",
            "[██████████████████████████████████████████████████] 63M/63M\n",
            "[*] Done!\n",
            "[██████████████████████████████████████████████████] 349M/349M\n",
            "[*] Done!\n",
            "Models downloaded successfully.\n",
            "Update completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsCSO(normalized_corpus):\n",
        "\n",
        "  # run the extraction\n",
        "  result = CSO_Extractor.run(normalized_corpus)\n",
        "\n",
        "  print('-----CSO Extraction-----')\n",
        "  print('------------------------\\n')\n",
        "  \n",
        "  for keyword in result['union']:\n",
        "    print(keyword)\n",
        "\n",
        "  return result['union']"
      ],
      "metadata": {
        "id": "Cctwpsvv86iZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YAKE Extractor**"
      ],
      "metadata": {
        "id": "-4h4ZTbXu9CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install yake\n",
        "import yake "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNiHvEtdvF0G",
        "outputId": "a77a7dc7-c72f-4f20-887b-16fd6683e305"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from yake) (2.6.3)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.8/dist-packages (from yake) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from yake) (1.21.6)\n",
            "Collecting segtok\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from yake) (0.8.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from segtok->yake) (2022.6.2)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp38-cp38-linux_x86_64.whl size=70602 sha256=f2bc9f063b61b6325e2c9a1f42c790012ae15702bc2617f8253d18a78f4e0baf\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/c7/3c/4c83132de76359e3a429fd09c08995945ca96c5290a41651d3\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.9.0 segtok-1.5.11 yake-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsYAKE(normalized_corpus):\n",
        "    print()\n",
        "    print('---YAKE Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    language = \"en\"\n",
        "    max_ngram_size = 3\n",
        "    deduplication_threshold = 0.9\n",
        "    deduplication_algo = 'seqm'\n",
        "    windowSize = 1\n",
        "    numOfKeywords = 20\n",
        "\n",
        "    kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
        "    keywords = kw_extractor.extract_keywords(normalized_corpus)\n",
        "\n",
        "    zipped = list(map(list, zip(*keywords)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "    print('-Skill-'.ljust(40), '-Score-')\n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "    \n",
        "    return keywords, scores"
      ],
      "metadata": {
        "id": "JGGvwVeYvaRv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Skill Matching**"
      ],
      "metadata": {
        "id": "Oao0RKq6aDyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install jieba\n",
        "\n",
        "from functools import reduce\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import jieba\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJF-rEVU3ZOy",
        "outputId": "9c0dd078-8ca8-47ff-c14f-b44f3b3b0d12"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.8.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (0.42.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=175)  # wrapper options"
      ],
      "metadata": {
        "id": "aXYDvHXw10kL"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkillsMatching:\n",
        "  def __init__(self, all_job_postings_keywords, all_jp_titles, all_resumes_keywords):\n",
        "      self.all_job_postings_keywords = all_job_postings_keywords\n",
        "      self.all_jp_titles = all_jp_titles\n",
        "      self.all_resumes_keywords = all_resumes_keywords\n",
        "      \n",
        "\n",
        "  # output JSON with matching job postings for each resume\n",
        "  def skill_match(self):\n",
        "      results = []\n",
        "      \n",
        "      for resume_json in self.all_resumes_keywords:\n",
        "          resume_arr = []\n",
        "          for key in resume_json:\n",
        "            # loop through the keys = every classifier \n",
        "            classifier_json = {key: {\"matching_job\": \"\",\n",
        "                      \"job_title\": \"\"}}\n",
        "            resume_keywords = resume_json[key]\n",
        "            scores = self.sim_score(self.all_job_postings_keywords, resume_keywords)\n",
        "\n",
        "            # grab largest score\n",
        "            largestScore = max(scores)\n",
        "\n",
        "            # grab the index of the largest score in the arr\n",
        "            max_idx = np.argmax(scores)\n",
        "\n",
        "            classifier_json[key]['matching_job'] = self.all_job_postings_keywords[max_idx]\n",
        "            classifier_json[key]['job_title'] = self.all_jp_titles[max_idx]\n",
        "            classifier_json[key]['score'] = largestScore\n",
        "\n",
        "            resume_arr.append(classifier_json)\n",
        "          results.append(resume_arr)\n",
        "      return results\n",
        "\n",
        "\n",
        "  def split_and_join_arr(self, arr):\n",
        "    new_arr = []\n",
        "    for w in arr:\n",
        "      word_arr = re.split('\\W+', w.lower())\n",
        "      new_arr = new_arr + word_arr\n",
        "    # print(new_arr)\n",
        "    return new_arr\n",
        "  \n",
        "  def sim_score(self, docs, keywords):\n",
        "    keywords = self.split_and_join_arr(keywords)\n",
        "    gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in docs]\n",
        "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "    tf_idf = gensim.models.TfidfModel(corpus)\n",
        "    sims = gensim.similarities.Similarity('/usr/workdir',tf_idf[corpus],\n",
        "                                      num_features=len(dictionary))\n",
        "\n",
        "    query_doc_bow = dictionary.doc2bow(keywords)\n",
        "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "    # print(sims[query_doc_tf_idf])\n",
        "    return sims[query_doc_tf_idf]\n",
        "\n",
        "  def print_results(self, match_job_postings):\n",
        "\n",
        "    # init average scores\n",
        "    avgScoreBERT = 0\n",
        "    avgScoreCSO = 0\n",
        "    avgScoreYAKE = 0\n",
        "\n",
        "    # for each resume\n",
        "    for i, resume in enumerate(match_job_postings):\n",
        "      print('\\n------------------------')\n",
        "      print(\"Matching Job Posting for Resume %s\" % (i + 1))\n",
        "      for classifier_obj in resume:\n",
        "        classifier_type = list(classifier_obj.keys())[0]\n",
        "\n",
        "        # if BERT\n",
        "        if classifier_type == 'keywordsBERT':\n",
        "          avgScoreBERT += round(classifier_obj[classifier_type]['score'] * 100, 2)\n",
        "\n",
        "        # else if CSO\n",
        "        elif classifier_type == 'keywordsCSO':\n",
        "          avgScoreCSO += round(classifier_obj[classifier_type]['score'] * 100, 2)\n",
        "\n",
        "        # else YAKE\n",
        "        else:\n",
        "          avgScoreYAKE += round(classifier_obj[classifier_type]['score'] * 100, 2)\n",
        "\n",
        "        print()\n",
        "        print('Skill Extracted Using:', classifier_type)\n",
        "        print('Matched JP Title:', classifier_obj[classifier_type]['job_title'])\n",
        "        print('With a score of:', round(classifier_obj[classifier_type]['score'] * 100, 2), '%')\n",
        "\n",
        "        # Wrap this text.\n",
        "        word_list = wrapper.wrap(text = \n",
        "              'Matched JP Description: ' + classifier_obj[classifier_type]['matching_job'])\n",
        "\n",
        "        # Print each line.\n",
        "        for element in word_list:\n",
        "            print(element)\n",
        "\n",
        "    avgScoreBERT /= len(match_job_postings)\n",
        "    avgScoreCSO /= len(match_job_postings)\n",
        "    avgScoreYAKE /= len(match_job_postings)\n",
        "\n",
        "    print()\n",
        "    print('---POST PROCESSING ANALYSIS----')\n",
        "    print('-------------------------------')\n",
        "    print()\n",
        "    print('Average Matching Scores for Skill Extraction Method')\n",
        "    print('BERT: ', round(avgScoreBERT, 2), '%')\n",
        "    print('CSO : ', round(avgScoreCSO, 2), '%')\n",
        "    print('YAKE: ', round(avgScoreYAKE, 2), '%')"
      ],
      "metadata": {
        "id": "F9IBrSshaKbM"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Driver Code"
      ],
      "metadata": {
        "id": "FIi95ge84CGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd     # pip install pandas. usage: loading data from csv files into dataframes\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.utils import shuffle\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "id": "YpAjFMcT6H3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30e1e84-e294-4cf2-ab6f-8236a8650f17"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Helper Functions\n",
        "\n",
        "# Function to retrieve text data.\n",
        "# (either 1 or more job postings or resumes)?\n",
        "def getFileData(filename, dir):\n",
        "    return pd.read_csv('data/' + dir + '/' + filename)\n",
        "\n",
        "# Function to normalize text data. \n",
        "# (some skill extraction tools will normalize text for us; however, if not, this function is here)\n",
        "# includes removing stopwords, punctuation, dates, links, etc...\n",
        "def normalizeCorpus(corpus):\n",
        "    return corpus\n",
        "    # need to fix this \n",
        "    # nltk_tokenList = word_tokenize(corpus)\n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # nltk_lemmaList = []\n",
        "    # for word in nltk_tokenList:\n",
        "    #     nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # normalized_corpus = []  \n",
        "    # nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "    # for w in nltk_lemmaList:  \n",
        "    #     if w not in nltk_stop_words:  \n",
        "    #         normalized_corpus.append(w)\n",
        "\n",
        "    # punctuation = \";:.,?!\"\n",
        "    # for word in normalized_corpus:\n",
        "    #     if word in punctuation:\n",
        "    #         normalized_corpus.remove(word)\n",
        "\n",
        "    #still need to add dates and links\n",
        "    # return normalized_corpus\n",
        "\n",
        "# Function to extract skill words from a given corpus.\n",
        "# ideally, this function will output a set of skills extracted from the corpus\n",
        "def extractSkills(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO              = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    keywordsYAKE, scoresYAKE = extractKeywordsYAKE(corpus)\n",
        "\n",
        "    # extraction method n...\n",
        "\n",
        "    # keep going!\n",
        "    return {\n",
        "              \"keywordsBERT\": keywordsBERT,\n",
        "              \"keywordsCSO\": keywordsCSO,\n",
        "              \"keywordsYAKE\": keywordsYAKE\n",
        "           }\n",
        "\n",
        "### Driver Code\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # fetch the data\n",
        "    jpCorpus = []\n",
        "    jpTitles = []\n",
        "\n",
        "    # --- FETCH MORE DATA VIA API ---\n",
        "    job_posting_obj = queryJobsAPI(\"Data Scientist\", \"Washington, DC\")\n",
        "    for i in range(len(job_posting_obj)):\n",
        "      jpCorpus.append(job_posting_obj[i]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "      jpTitles.append(job_posting_obj[i]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "\n",
        "    # --- FETCH DATA FROM /CONTENT/DATA/..\n",
        "\n",
        "\n",
        "    # collect all Job Postings\n",
        "    dfNY = getFileData('Data_Job_NY.csv', 'job-postings')\n",
        "    dfSF = getFileData('Data_Job_SF.csv', 'job-postings')\n",
        "    dfTX = getFileData('Data_Job_TX.csv', 'job-postings')\n",
        "    dfWA = getFileData('Data_Job_WA.csv', 'job-postings')\n",
        "\n",
        "    frames = [dfNY, dfSF, dfTX, dfWA] # build frame\n",
        "    job_posting_dataframe = pd.concat(frames) # concat frames\n",
        "    job_posting_dataframe = shuffle(job_posting_dataframe)  # shuffle df\n",
        "\n",
        "    resume_dataframe      = getFileData('kaggleResumes.csv', 'resumes')\n",
        "\n",
        "    jpCorpus += list(job_posting_dataframe['Job_Desc'])\n",
        "    jpTitles += list(job_posting_dataframe['Job_title'])\n",
        "    #---------------\n",
        "\n",
        "    \"\"\"\n",
        "    # print the dataframes\n",
        "    #print('DataFrame of Job Postings:')\n",
        "    #print(job_posting_dataframe)    \n",
        "    #print()\n",
        "\n",
        "    #print('DataFrame of Resumes:')\n",
        "    #print(resume_dataframe)\n",
        "    #print()\n",
        "    #----------------------\n",
        "    \"\"\"\n",
        "\n",
        "    # fetch resumes by themselves\n",
        "    rCorpus  = list(resume_dataframe['Resume'])\n",
        "    #----------------------\n",
        "\n",
        "    # Number of resume samples to view\n",
        "    NUM_SAMPLES = 10\n",
        "    # number of job postings to view\n",
        "    NUM_JPS = 10\n",
        "\n",
        "    print('\\nCurrently Avaliable Job Postings: ', len(jpCorpus))\n",
        "    print()\n",
        "\n",
        "    # for each JOB POSTING from the corpus\n",
        "    i = 0\n",
        "    all_job_postings = []\n",
        "    for posting in jpCorpus:\n",
        "          \n",
        "        # Wrap this text.\n",
        "        word_list = wrapper.wrap(text=posting)\n",
        "    \n",
        "        print('Job Posting #', i+1)\n",
        "\n",
        "        # Print each line.\n",
        "        for element in word_list:\n",
        "            print(element)    \n",
        "\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(posting)\n",
        "        job_posting_json = extractSkills(text)\n",
        "        all_job_postings.append(text)\n",
        "        # print lines, we are done with this posting\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X postings\n",
        "        if i > NUM_JPS - 1:\n",
        "          break\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    #---------------------- \n",
        "\n",
        "    # for each RESUME from the corpus\n",
        "    i = 0\n",
        "    all_resumes_keywords = []\n",
        "    for resume in rCorpus:\n",
        "\n",
        "        # Wrap this text.\n",
        "        word_list = wrapper.wrap(text = resume)\n",
        "    \n",
        "        print('Resume #', i+1)\n",
        "\n",
        "        # Print each line.\n",
        "        for element in word_list:\n",
        "            print(element)    \n",
        "\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(resume)\n",
        "        resume_json = extractSkills(text)\n",
        "        all_resumes_keywords.append(resume_json)\n",
        "        # print lines, we are done with this resume\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X resumes\n",
        "        if i > NUM_SAMPLES - 1:\n",
        "            break\n",
        "\n",
        "        i += 1\n",
        "    #---------------------- \n",
        "\n",
        "    \n",
        "# keep going!\n",
        "\n",
        "# end of driver code\n",
        "#---------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6HTo9qA4Fxs",
        "outputId": "a4c892b6-c71c-4007-e513-8dca7a4a9a4c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Scientist\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "Currently Avaliable Job Postings:  925\n",
            "\n",
            "Job Posting # 1\n",
            "As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the body worn camera and\n",
            "electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through the transparency,\n",
            "availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist provide                   0.6452\n",
            "electronic records management            0.436\n",
            "law enforcement programs                 0.4179\n",
            "systems                                  0.2913\n",
            "worn camera electronic                   0.2899\n",
            "park service law                         0.2727\n",
            "strengthen public trust                  0.2479\n",
            "transparency availability                0.2332\n",
            "remote position                          0.1724\n",
            "office created                           0.1639\n",
            "branch                                   0.1535\n",
            "accessibility                            0.141\n",
            "specific body                            0.1143\n",
            "national                                 0.094\n",
            "fully                                    0.0283\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "information management\n",
            "records management\n",
            "data management\n",
            "electronic records\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Technology Branch Office                 0.001481109174040813\n",
            "provide data management                  0.004659336811099895\n",
            "records management systems               0.005216835943208545\n",
            "Technology Branch                        0.0063933047560725985\n",
            "body worn camera                         0.006747317732994754\n",
            "electronic records management            0.0072077123711537915\n",
            "Data Scientist                           0.007651743677230578\n",
            "National Park Service                    0.01430530479782861\n",
            "Branch Office                            0.018178522502453803\n",
            "Public Trust                             0.018963124994821837\n",
            "strengthen public trust                  0.019324532985338896\n",
            "provide data                             0.022659745444925165\n",
            "data management                          0.024147626145501592\n",
            "management systems                       0.026987689958630504\n",
            "Park Service law                         0.029608428689612158\n",
            "analytical support                       0.034736319679920705\n",
            "body worn                                0.034736319679920705\n",
            "worn camera                              0.034736319679920705\n",
            "camera and electronic                    0.034736319679920705\n",
            "electronic records                       0.034736319679920705\n",
            "------------------------\n",
            "\n",
            "Job Posting # 2\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics\n",
            "(OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms\n",
            "necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "health informatics obrhi                 0.6482\n",
            "centers medicare medicaid                0.4176\n",
            "group data                               0.3836\n",
            "analytical statistical programming       0.3435\n",
            "position located department              0.3154\n",
            "scientist gs                             0.3021\n",
            "reduction                                0.2062\n",
            "cms                                      0.1765\n",
            "emerging                                 0.1474\n",
            "sets                                     0.1415\n",
            "burden                                   0.1377\n",
            "interpret unique                         0.1183\n",
            "mechanisms necessary collect             0.0853\n",
            "13                                       0.0808\n",
            "highly                                   0.0576\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "service delivery\n",
            "target position\n",
            "integrated data\n",
            "university\n",
            "programming languages\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Emerging Innovations Group               0.00010839280424678759\n",
            "specialized data sets                    0.0005597800499497894\n",
            "highly specialized data                  0.0007454122597000672\n",
            "Human Services                           0.0018855728774306911\n",
            "Medicaid Services                        0.0018855728774306911\n",
            "Centers for Medicare                     0.0019447784389868556\n",
            "Office of Burden                         0.0024500117744031833\n",
            "Emerging Innovations                     0.0024500117744031833\n",
            "Innovations Group                        0.0024500117744031833\n",
            "Health Informatics                       0.0027427897865850287\n",
            "Burden Reduction                         0.003086903891076617\n",
            "Data Scientist                           0.0035038272852105624\n",
            "data sets                                0.005760446985463852\n",
            "implement the analytical                 0.006976324594720111\n",
            "specialized data                         0.007655593716194504\n",
            "position is located                      0.009275175508151086\n",
            "programming mechanisms                   0.009275175508151086\n",
            "interpret unique                         0.009275175508151086\n",
            "unique and highly                        0.009275175508151086\n",
            "highly specialized                       0.009275175508151086\n",
            "------------------------\n",
            "\n",
            "Job Posting # 3\n",
            "DOE's Clean Energy Corps is made up of the staff from more than a dozen offices across DOE. As vacancies become available, applicants may be required to go through the DOE\n",
            "Clean Energy Corps hiring pipeline where you may be evaluated based on competencies through a core competency /technical interview(s). You are strongly encouraged to express\n",
            "interest through the Clean Energy Corps Applicant Portal for consideration. Failure to do so may result in loss of consideration for BIL positions.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "energy corps hiring                      0.7553\n",
            "clean energy                             0.4134\n",
            "consideration bil positions              0.3925\n",
            "competencies core competency             0.3144\n",
            "interview strongly encouraged            0.3062\n",
            "required doe clean                       0.2885\n",
            "offices                                  0.2549\n",
            "pipeline evaluated based                 0.2526\n",
            "portal consideration                     0.1512\n",
            "bil                                      0.1385\n",
            "express                                  0.1123\n",
            "strongly                                 0.0587\n",
            "available                                0.0268\n",
            "failure result loss                      0.0175\n",
            "dozen                                    -0.0111\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "node failure\n",
            "energy conservation\n",
            "pipelined architecture\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Clean Energy Corps                       0.0030179210133411054\n",
            "DOE Clean Energy                         0.0037314771005915945\n",
            "Clean Energy                             0.011540306016574855\n",
            "Energy Corps                             0.013770664567403081\n",
            "Energy Corps Applicant                   0.021161952499571507\n",
            "DOE Clean                                0.02148456860164941\n",
            "Energy Corps hiring                      0.02785608845512113\n",
            "Corps Applicant Portal                   0.051196362574438256\n",
            "Energy                                   0.054688578277208987\n",
            "DOE                                      0.0673324603758151\n",
            "Clean                                    0.06798579532515052\n",
            "dozen offices                            0.07200119175072148\n",
            "Corps                                    0.08060391587401694\n",
            "Corps Applicant                          0.11793440602021261\n",
            "Corps hiring pipeline                    0.1255816432782287\n",
            "Corps hiring                             0.1529234452026279\n",
            "offices across DOE                       0.15762714518590795\n",
            "Applicant Portal                         0.1825757964390524\n",
            "Corps is made                            0.18556297364333152\n",
            "technical interview                      0.23662905266824658\n",
            "------------------------\n",
            "\n",
            "Job Posting # 4\n",
            "DOE's Clean Energy Corps is made up of the staff from more than a dozen offices across DOE. As vacancies become available, applicants may be required to go through the DOE\n",
            "Clean Energy Corps hiring pipeline where you may be evaluated based on competencies through a core competency /technical interview(s). You are strongly encouraged to express\n",
            "interest through the Clean Energy Corps Applicant Portal for consideration. Failure to do so may result in loss of consideration for BIL positions.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "energy corps hiring                      0.7553\n",
            "clean energy                             0.4134\n",
            "consideration bil positions              0.3925\n",
            "competencies core competency             0.3144\n",
            "interview strongly encouraged            0.3062\n",
            "required doe clean                       0.2885\n",
            "offices                                  0.2549\n",
            "pipeline evaluated based                 0.2526\n",
            "portal consideration                     0.1512\n",
            "bil                                      0.1385\n",
            "express                                  0.1123\n",
            "strongly                                 0.0587\n",
            "available                                0.0268\n",
            "failure result loss                      0.0175\n",
            "dozen                                    -0.0111\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "node failure\n",
            "energy conservation\n",
            "pipelined architecture\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Clean Energy Corps                       0.0030179210133411054\n",
            "DOE Clean Energy                         0.0037314771005915945\n",
            "Clean Energy                             0.011540306016574855\n",
            "Energy Corps                             0.013770664567403081\n",
            "Energy Corps Applicant                   0.021161952499571507\n",
            "DOE Clean                                0.02148456860164941\n",
            "Energy Corps hiring                      0.02785608845512113\n",
            "Corps Applicant Portal                   0.051196362574438256\n",
            "Energy                                   0.054688578277208987\n",
            "DOE                                      0.0673324603758151\n",
            "Clean                                    0.06798579532515052\n",
            "dozen offices                            0.07200119175072148\n",
            "Corps                                    0.08060391587401694\n",
            "Corps Applicant                          0.11793440602021261\n",
            "Corps hiring pipeline                    0.1255816432782287\n",
            "Corps hiring                             0.1529234452026279\n",
            "offices across DOE                       0.15762714518590795\n",
            "Applicant Portal                         0.1825757964390524\n",
            "Corps is made                            0.18556297364333152\n",
            "technical interview                      0.23662905266824658\n",
            "------------------------\n",
            "\n",
            "Job Posting # 5\n",
            "This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified candidates to positions in the\n",
            "competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research that shapes operational\n",
            "requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "dod workforce recruit                    0.6384\n",
            "data driven solutions                    0.4056\n",
            "department                               0.4025\n",
            "utilizing dha certain                    0.379\n",
            "candidates positions competitive         0.3597\n",
            "solicitation utilizing                   0.3275\n",
            "position mind work                       0.3064\n",
            "operational requirements                 0.3043\n",
            "research                                 0.2913\n",
            "direct                                   0.2276\n",
            "options                                  0.1728\n",
            "science complex analytical               0.1522\n",
            "driven                                   0.1179\n",
            "certain                                  0.0517\n",
            "shapes                                   0.03\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "labor force\n",
            "integrated data\n",
            "service management\n",
            "correlation analysis\n",
            "social sciences\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Direct Hire Authority                    0.00027525105982331027\n",
            "appoint qualified candidates             0.003381007219420701\n",
            "Hire Authority                           0.0039290832034005185\n",
            "Direct Hire                              0.00485064625852227\n",
            "DoD Workforce                            0.010325830767835139\n",
            "Workforce to recruit                     0.010325830767835139\n",
            "solicitation utilizing                   0.016821661580743108\n",
            "competitive service                      0.016821661580743108\n",
            "recruit and appoint                      0.022117176899686557\n",
            "appoint qualified                        0.022117176899686557\n",
            "qualified candidates                     0.022117176899686557\n",
            "utilizing the DHA                        0.02674237579672678\n",
            "DHA                                      0.03597444408682677\n",
            "Authority                                0.056329921499805445\n",
            "candidates to positions                  0.06553067996962623\n",
            "Department of Army                       0.06793218093732048\n",
            "Put your mind                            0.0687491016102964\n",
            "Direct                                   0.06947827767153035\n",
            "Hire                                     0.06947827767153035\n",
            "Personnel                                0.06947827767153035\n",
            "------------------------\n",
            "\n",
            "Job Posting # 6\n",
            "Interdisciplinary Position - This is an interdisciplinary position that may be filled as a 0401 - Biologist or 1301-Physical Scientist. Duty Station TBD - The duty station of\n",
            "this position may remain that of the selectee. Salary currently reflect Rest of US pay; however, salary will be determined by the duty location of the selectee.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "position interdisciplinary position      0.6821\n",
            "duty location                            0.4456\n",
            "physical scientist duty                  0.4113\n",
            "salary determined duty                   0.3841\n",
            "biologist 1301 physical                  0.3671\n",
            "station position remain                  0.3208\n",
            "selectee salary currently                0.2402\n",
            "determined duty                          0.2318\n",
            "location selectee                        0.2056\n",
            "0401                                     0.1805\n",
            "physical                                 0.1681\n",
            "tbd                                      0.167\n",
            "reflect rest pay                         0.1443\n",
            "filled                                   0.087\n",
            "currently                                0.0675\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "location based\n",
            "location information\n",
            "target position\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Interdisciplinary Position               0.0258159174195142\n",
            "Duty Station TBD                         0.031408119648370994\n",
            "Scientist                                0.049577095220833665\n",
            "Station TBD                              0.06409633480480806\n",
            "Biologist                                0.0667982628577293\n",
            "Duty Station                             0.0705593875975975\n",
            "Position                                 0.08867436765353527\n",
            "Interdisciplinary                        0.09227934086343789\n",
            "Duty                                     0.13625718634980175\n",
            "selectee                                 0.14212775728642185\n",
            "Station                                  0.15126668182424718\n",
            "TBD                                      0.1991035359236946\n",
            "filled                                   0.20857054191678534\n",
            "reflect Rest                             0.22155239505907348\n",
            "Salary                                   0.22846716257880423\n",
            "duty location                            0.3047956889357653\n",
            "Rest                                     0.31737938500692287\n",
            "position may remain                      0.3431859286539983\n",
            "remain                                   0.4778829870780849\n",
            "pay                                      0.4808009968795223\n",
            "------------------------\n",
            "\n",
            "Job Posting # 7\n",
            "This vacancy is for a Statistician (Data Scientist) position in the Department of Commerce located at the U.S. Census Bureau Headquarters in Suitland, Maryland. The Census\n",
            "Bureau is accessible from the Metro Rail Green Line - Suitland Station. This Job Opportunity Announcement may be used to fill other Statistician (Data Scientist)-1530-11/12,\n",
            "FPL GS-12 positions within the Census Bureau in the same geographical location with the same qualifications and specialized experience.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "positions census bureau                  0.7091\n",
            "maryland census                          0.6019\n",
            "vacancy statistician data                0.5521\n",
            "used statistician                        0.4895\n",
            "department commerce located              0.413\n",
            "data                                     0.3855\n",
            "opportunity announcement                 0.2375\n",
            "specialized experience                   0.1978\n",
            "suitland                                 0.1707\n",
            "1530 11                                  0.1368\n",
            "gs                                       0.1156\n",
            "12                                       0.109\n",
            "line                                     0.1029\n",
            "accessible metro rail                    0.0957\n",
            "fpl                                      0.0636\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "target position\n",
            "location information\n",
            "integrated data\n",
            "commerce\n",
            "university\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Census Bureau Headquarters               0.0011586987202485944\n",
            "Department of Commerce                   0.004552060284628119\n",
            "Census Bureau                            0.0056488001111533095\n",
            "Data Scientist                           0.007483275711753406\n",
            "Rail Green Line                          0.007778789921334165\n",
            "Metro Rail Green                         0.008989894877618712\n",
            "Bureau Headquarters                      0.009689401565386425\n",
            "Commerce located                         0.010327715356546653\n",
            "Headquarters in Suitland                 0.023062799020728178\n",
            "Job Opportunity Announcement             0.02324331176875615\n",
            "Suitland Station                         0.024034139070941717\n",
            "Green Line                               0.03635020407126182\n",
            "Census                                   0.03896428975567064\n",
            "Metro Rail                               0.04185191832006176\n",
            "Rail Green                               0.04185191832006176\n",
            "Bureau                                   0.047519309862781134\n",
            "Maryland                                 0.04792545973343635\n",
            "Data                                     0.06071625976506296\n",
            "Scientist                                0.06071625976506296\n",
            "Suitland                                 0.06705613370612083\n",
            "------------------------\n",
            "\n",
            "Job Posting # 8\n",
            "This position(s) is located in the Food and Drug Administration (FDA), Center for Devices and Radiological Heath (CDRH), Office of Strategic Partnership and Technology\n",
            "Innovation (OST). The Center for Devices and Radiological Health (CDRH) is responsible for protecting and promoting the public health. It assures that patients and providers\n",
            "have timely and continued access to safe, effective, and high-quality medical devices and safe radiation-emitting products.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "radiological health cdrh                 0.7607\n",
            "medical devices safe                     0.5581\n",
            "cdrh office strategic                    0.4494\n",
            "ost center devices                       0.3132\n",
            "technology innovation                    0.2958\n",
            "located food drug                        0.2761\n",
            "emitting products                        0.2578\n",
            "promoting public health                  0.2497\n",
            "providers timely                         0.2317\n",
            "heath                                    0.2015\n",
            "effective high                           0.1846\n",
            "protecting                               0.1508\n",
            "access                                   0.1496\n",
            "high quality                             0.0633\n",
            "continued                                0.0279\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "strategic management\n",
            "innovative technologies\n",
            "technology innovation\n",
            "strategic planning\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Drug Administration                      0.0033105510734633015\n",
            "Office of Strategic                      0.0033105510734633015\n",
            "Technology Innovation                    0.0033105510734633015\n",
            "Food and Drug                            0.003770712335579741\n",
            "Strategic Partnership                    0.003770712335579741\n",
            "Partnership and Technology               0.003770712335579741\n",
            "Radiological Heath                       0.005656375471701434\n",
            "Devices and Radiological                 0.006624279482239499\n",
            "Center for Devices                       0.012010446731092493\n",
            "high-quality medical devices             0.03130854948431399\n",
            "CDRH                                     0.03450405476287721\n",
            "Devices                                  0.04171701699617315\n",
            "FDA                                      0.04647674626061604\n",
            "OST                                      0.04647674626061604\n",
            "Center                                   0.0522381937061212\n",
            "Radiological                             0.0522381937061212\n",
            "Administration                           0.0538357133417395\n",
            "Heath                                    0.0538357133417395\n",
            "Office                                   0.0538357133417395\n",
            "Innovation                               0.0538357133417395\n",
            "------------------------\n",
            "\n",
            "Job Posting # 9\n",
            "This position is located in the Division of Complex Institution Supervision and Resolution (CISR), Systemic Risk Branch, Data Analytics Sub-Branch, Data Analytics Section of\n",
            "the Federal Deposit Insurance Corporation. Please see the Clarification from Agency and Additional Information sections below for more information on telework options.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "cisr systemic risk                       0.5507\n",
            "analytics section federal                0.4224\n",
            "telework options                         0.3925\n",
            "deposit insurance                        0.3816\n",
            "risk branch data                         0.3558\n",
            "complex institution                      0.339\n",
            "supervision resolution                   0.3086\n",
            "agency                                   0.2997\n",
            "position                                 0.2891\n",
            "corporation clarification                0.2798\n",
            "sub branch                               0.2607\n",
            "information                              0.2584\n",
            "resolution                               0.1577\n",
            "division complex                         0.1469\n",
            "additional                               0.1371\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "visual analytics\n",
            "big data\n",
            "data analytics\n",
            "business intelligence\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Systemic Risk Branch                     0.0003406588989407056\n",
            "Deposit Insurance Corporation            0.0004598830333509207\n",
            "Data Analytics Sub-Branch                0.000501906952899021\n",
            "Complex Institution Supervision          0.0006210886841871376\n",
            "Federal Deposit Insurance                0.0006210886841871376\n",
            "Data Analytics Section                   0.0010062232142486372\n",
            "Data Analytics                           0.0019538934928934578\n",
            "Supervision and Resolution               0.005363757192029059\n",
            "Systemic Risk                            0.005363757192029059\n",
            "Risk Branch                              0.005363757192029059\n",
            "Insurance Corporation                    0.005363757192029059\n",
            "Division of Complex                      0.007232474398307712\n",
            "Complex Institution                      0.007232474398307712\n",
            "Institution Supervision                  0.007232474398307712\n",
            "Federal Deposit                          0.007232474398307712\n",
            "Deposit Insurance                        0.007232474398307712\n",
            "Analytics Sub-Branch                     0.010568096786205038\n",
            "Analytics Section                        0.021079313962152374\n",
            "Data                                     0.023533716038064905\n",
            "Additional Information sections          0.026973113640074625\n",
            "------------------------\n",
            "\n",
            "Job Posting # 10\n",
            "This position is located in the Division of Complex Institution Supervision and Resolution (CISR), Systemic Risk Branch, Data Analytics Sub-Branch, Data Analytics Section of\n",
            "the Federal Deposit Insurance Corporation. Please see the Clarification from Agency and Additional Information sections below for more information on telework options.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "cisr systemic risk                       0.5507\n",
            "analytics section federal                0.4224\n",
            "telework options                         0.3925\n",
            "deposit insurance                        0.3816\n",
            "risk branch data                         0.3558\n",
            "complex institution                      0.339\n",
            "supervision resolution                   0.3086\n",
            "agency                                   0.2997\n",
            "position                                 0.2891\n",
            "corporation clarification                0.2798\n",
            "sub branch                               0.2607\n",
            "information                              0.2584\n",
            "resolution                               0.1577\n",
            "division complex                         0.1469\n",
            "additional                               0.1371\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "visual analytics\n",
            "big data\n",
            "data analytics\n",
            "business intelligence\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Systemic Risk Branch                     0.0003406588989407056\n",
            "Deposit Insurance Corporation            0.0004598830333509207\n",
            "Data Analytics Sub-Branch                0.000501906952899021\n",
            "Complex Institution Supervision          0.0006210886841871376\n",
            "Federal Deposit Insurance                0.0006210886841871376\n",
            "Data Analytics Section                   0.0010062232142486372\n",
            "Data Analytics                           0.0019538934928934578\n",
            "Supervision and Resolution               0.005363757192029059\n",
            "Systemic Risk                            0.005363757192029059\n",
            "Risk Branch                              0.005363757192029059\n",
            "Insurance Corporation                    0.005363757192029059\n",
            "Division of Complex                      0.007232474398307712\n",
            "Complex Institution                      0.007232474398307712\n",
            "Institution Supervision                  0.007232474398307712\n",
            "Federal Deposit                          0.007232474398307712\n",
            "Deposit Insurance                        0.007232474398307712\n",
            "Analytics Sub-Branch                     0.010568096786205038\n",
            "Analytics Section                        0.021079313962152374\n",
            "Data                                     0.023533716038064905\n",
            "Additional Information sections          0.026973113640074625\n",
            "------------------------\n",
            "\n",
            "Job Posting # 11\n",
            "Positions under this announcement are being filled using a Direct Hire Authority (DHA). Click on \"Learn more about this agency\" button below to view Eligibilities being\n",
            "considered and other IMPORTANT information. WHERE CAN I FIND OUT MORE ABOUT OTHER IRS CAREERS? Visit us on the web at www.jobs.irs.gov #LI-POST\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "jobs irs gov                             0.7541\n",
            "hire authority dha                       0.4012\n",
            "careers visit                            0.266\n",
            "announcement filled                      0.2337\n",
            "hire                                     0.2185\n",
            "dha click learn                          0.1862\n",
            "positions                                0.1503\n",
            "direct                                   0.1316\n",
            "li                                       0.119\n",
            "authority                                0.0715\n",
            "post                                     0.0574\n",
            "considered                               0.0275\n",
            "web                                      0.0143\n",
            "eligibilities considered important       0.0081\n",
            "button view eligibilities                -0.0416\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "world wide web\n",
            "web content\n",
            "ajax\n",
            "user information\n",
            "web information\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Direct Hire Authority                    0.0007879380442722835\n",
            "Hire Authority                           0.007467805400473707\n",
            "Direct Hire                              0.010879665783271309\n",
            "DHA                                      0.0401219746920957\n",
            "Authority                                0.07145025265006083\n",
            "Direct                                   0.10374281946485873\n",
            "Hire                                     0.10374281946485873\n",
            "IRS CAREERS                              0.11546465753176442\n",
            "IMPORTANT information                    0.11877629447882801\n",
            "Positions                                0.1447773057422032\n",
            "view Eligibilities                       0.17170120125489455\n",
            "Eligibilities being considered           0.17170120125489455\n",
            "Learn                                    0.2108873635046048\n",
            "announcement                             0.23133275159292982\n",
            "filled                                   0.23133275159292982\n",
            "CAREERS                                  0.28024437468318186\n",
            "Eligibilities                            0.2867390363489802\n",
            "IMPORTANT                                0.2867390363489802\n",
            "LI-POST                                  0.31813151393529204\n",
            "FIND                                     0.3693655212169614\n",
            "------------------------\n",
            "\n",
            "Resume # 1\n",
            "Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN,\n",
            "Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic\n",
            "Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot,\n",
            "Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education\n",
            "Details     Data Science Assurance Associate     Data Science Assurance Associate - Ernst & Young LLP  Skill Details   JAVASCRIPT- Exprience - 24 months  jQuery- Exprience -\n",
            "24 months  Python- Exprience - 24 monthsCompany Details   company - Ernst & Young LLP  description - Fraud Investigations and Dispute Services   Assurance  TECHNOLOGY ASSISTED\n",
            "REVIEW  TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.  * Core member of a team helped in developing\n",
            "automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in\n",
            "reduced labor costs and time spent during the lawyers review.  * Understand the end to end flow of the solution, doing research and development for classification models,\n",
            "predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.  * TAR assists in\n",
            "predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
            "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment\n",
            "analysis. Matplot lib, Tableau dashboard for reporting.    MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)  TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA *\n",
            "Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4\n",
            "categories.  * Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted\n",
            "Word cloud.  * Created customized tableau dashboards for effective reporting and visualizations.  CHATBOT * Developed a user friendly chatbot for one of our Products which\n",
            "handle simple questions about hours of operation, reservation options and so on.  * This chat bot serves entire product related questions. Giving overview of tool via QA\n",
            "platform and also give recommendation responses so that user question to build chain of relevant answer.  * This too has intelligence to build the pipeline of questions as per\n",
            "user requirement and asks the relevant /recommended questions.    Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis,\n",
            "Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer    INFORMATION GOVERNANCE  Organizations to make informed decisions about all of the information they store. The\n",
            "integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to\n",
            "counter information risk.  * Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic\n",
            "search and created customized, interactive dashboards using kibana.  * Preforming ROT Analysis on the data which give information of data which helps identify content that is\n",
            "either Redundant, Outdated, or Trivial.  * Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable\n",
            "information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.  Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
            "FRAUD ANALYTIC PLATFORM  Fraud Analytics and investigative platform to review all red flag cases.  â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case\n",
            "manager and suite of Analytics for various ERP systems.  * It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be\n",
            "indicators of fraud by running advanced analytics  Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "developing automated review              0.5601\n",
            "associate data science                   0.4096\n",
            "lawyers                                  0.3539\n",
            "portfolio synthesizes intelligence       0.3337\n",
            "tar technology assisted                  0.2238\n",
            "relevant recommended                     0.2234\n",
            "bayes knn random                         0.213\n",
            "platform fraud                           0.2127\n",
            "tableau regular expression               0.196\n",
            "word embedding                           0.1828\n",
            "elasticsearch d3 js                      0.1816\n",
            "exprience 24 months                      0.1683\n",
            "kafka python flask                       0.0694\n",
            "pii personally                           0.0692\n",
            "jquery css bootstrap                     0.0626\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "vehicles\n",
            "text document\n",
            "boosting\n",
            "sentiment analysis\n",
            "deep learning\n",
            "ajax\n",
            "database systems\n",
            "natural language processing\n",
            "text data\n",
            "css\n",
            "user information\n",
            "programming languages\n",
            "machine learning\n",
            "k-nearest neighbors\n",
            "word embedding\n",
            "customer review\n",
            "decision trees\n",
            "recommendation\n",
            "question answering\n",
            "erp system\n",
            "classification models\n",
            "visualization tools\n",
            "cyber-attacks\n",
            "javascript\n",
            "svm\n",
            "dimensionality reduction\n",
            "natural languages\n",
            "visualization\n",
            "computer vision\n",
            "textual data\n",
            "html\n",
            "classifiers\n",
            "random forests\n",
            "java\n",
            "cluster analysis\n",
            "python\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Programming Languages                    0.010927614604646271\n",
            "Science Assurance Associate              0.011850855598096734\n",
            "Data Science Assurance                   0.01884628369710474\n",
            "LLP Skill Details                        0.021232972451533626\n",
            "Young LLP Skill                          0.021644888423718343\n",
            "Natural Language processing              0.0219379848158316\n",
            "Dispute Services Assurance               0.028486093511906674\n",
            "Topic Modelling                          0.028515384907894414\n",
            "Natural Language                         0.028980000433086836\n",
            "TECHNOLOGY ASSISTED REVIEW               0.029066281762730874\n",
            "Assurance Associate Data                 0.02942891833560471\n",
            "Services Assurance TECHNOLOGY            0.03152194966396776\n",
            "Assurance Associate                      0.03158419270725324\n",
            "Assurance TECHNOLOGY ASSISTED            0.03343593688541541\n",
            "Skill Details JAVASCRIPT                 0.0339487296732544\n",
            "Young LLP                                0.03468422940022623\n",
            "Data                                     0.0357887990823203\n",
            "Analysis                                 0.03889168712283793\n",
            "Science Assurance                        0.039640667318250715\n",
            "Details Data Science                     0.04085483920589676\n",
            "------------------------\n",
            "\n",
            "Resume # 2\n",
            "Education Details   May 2013 to May 2017 B.E   UIT-RGPV  Data Scientist     Data Scientist - Matelabs  Skill Details   Python- Exprience - Less than 1 year months\n",
            "Statsmodels- Exprience - 12 months  AWS- Exprience - Less than 1 year months  Machine learning- Exprience - Less than 1 year months  Sklearn- Exprience - Less than 1 year\n",
            "months  Scipy- Exprience - Less than 1 year months  Keras- Exprience - Less than 1 year monthsCompany Details   company - Matelabs  description - ML Platform for business\n",
            "professionals, dummies and enthusiasts.  60/A Koramangala 5th block,  Achievements/Tasks behind sukh sagar, Bengaluru,  India                               Developed and\n",
            "deployed auto preprocessing steps of machine learning mainly missing value  treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.\n",
            "Deployed automated classification and regression model.  linkedin.com/in/aditya-rathore-  b4600b146                           Reasearch and deployed the time series\n",
            "forecasting model ARIMA, SARIMAX, Holt-winter and  Prophet.  Worked on meta-feature extracting problem.  github.com/rathorology  Implemented a state of the art research paper\n",
            "on outlier detection for mixed attributes.  company - Matelabs  description -\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist matelabs                  0.7015\n",
            "mixed attributes company                 0.4055\n",
            "time series forecasting                  0.3483\n",
            "12 months aws                            0.3261\n",
            "keras exprience                          0.2863\n",
            "deployed auto preprocessing              0.244\n",
            "value treatment outlier                  0.2424\n",
            "sukh sagar                               0.2126\n",
            "feature selection dimensionality         0.2047\n",
            "india developed                          0.191\n",
            "winter prophet worked                    0.1604\n",
            "extracting problem github                0.1406\n",
            "block achievements tasks                 0.1246\n",
            "art research paper                       0.0609\n",
            "mainly                                   0.0545\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "machine learning\n",
            "business process\n",
            "forecasting models\n",
            "genetic selection\n",
            "regression model\n",
            "education\n",
            "dimensionality reduction\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Data Scientist Data                      0.0003174445791207147\n",
            "Scientist Data Scientist                 0.0003174445791207147\n",
            "Skill Details Python                     0.0004043432422474764\n",
            "year months Statsmodels                  0.0005222674926622515\n",
            "year months Sklearn                      0.0005222674926622515\n",
            "year months Scipy                        0.0005222674926622515\n",
            "year months Keras                        0.0005222674926622515\n",
            "year monthsCompany Details               0.0009859928547686575\n",
            "UIT-RGPV Data Scientist                  0.0010586795481891626\n",
            "Matelabs Skill Details                   0.0013689781352713495\n",
            "year months Machine                      0.0015125774058462262\n",
            "year months                              0.0015212119151985296\n",
            "Data Scientist                           0.002316893141496774\n",
            "Exprience                                0.0024758769327980754\n",
            "Scientist Data                           0.004633786282993548\n",
            "Details Python                           0.005111245541156262\n",
            "Skill Details                            0.0059019438840864835\n",
            "months Machine learning                  0.00839824727683308\n",
            "months Statsmodels                       0.009007640568633023\n",
            "months AWS                               0.009007640568633023\n",
            "------------------------\n",
            "\n",
            "Resume # 3\n",
            "Areas of Interest Deep Learning, Control System Design, Programming in-Python, Electric Machinery, Web Development, Analytics Technical Activities q Hindustan Aeronautics\n",
            "Limited, Bangalore - For 4 weeks under the guidance of Mr. Satish, Senior Engineer in the hangar of Mirage 2000 fighter aircraft Technical Skills Programming Matlab, Python\n",
            "and Java, LabView, Python WebFrameWork-Django, Flask, LTSPICE-intermediate Languages and and MIPOWER-intermediate, Github (GitBash), Jupyter Notebook, Xampp, MySQL-Basics,\n",
            "Python Software Packages Interpreters-Anaconda, Python2, Python3, Pycharm, Java IDE-Eclipse Operating Systems Windows, Ubuntu, Debian-Kali Linux Education Details   January\n",
            "2019 B.Tech. Electrical and Electronics Engineering  Manipal Institute of Technology  January 2015    DEEKSHA CENTER  January 2013    Little Flower Public School  August 2000\n",
            "Manipal Academy of Higher  DATA SCIENCE     DATA SCIENCE AND ELECTRICAL ENTHUSIAST  Skill Details   Data Analysis- Exprience - Less than 1 year months  excel- Exprience - Less\n",
            "than 1 year months  Machine Learning- Exprience - Less than 1 year months  mathematics- Exprience - Less than 1 year months  Python- Exprience - Less than 1 year months\n",
            "Matlab- Exprience - Less than 1 year months  Electrical Engineering- Exprience - Less than 1 year months  Sql- Exprience - Less than 1 year monthsCompany Details   company -\n",
            "THEMATHCOMPANY  description - I am currently working with a Casino based operator(name not to be disclosed) in Macau.I need to segment the customers who visit their property\n",
            "based on the value the patrons bring into the company.Basically prove that the segmentation can be done in much better way than the current system which they have with proper\n",
            "numbers to back it up.Henceforth they can implement target marketing strategy to attract their customers who add value to the business.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "need segment customers                   0.6534\n",
            "working casino based                     0.4365\n",
            "machine learning exprience               0.3368\n",
            "aircraft technical skills                0.3356\n",
            "development analytics                    0.3341\n",
            "property based value                     0.2691\n",
            "activities hindustan                     0.2141\n",
            "2019 tech electrical                     0.195\n",
            "java labview python                      0.1625\n",
            "xampp mysql basics                       0.1616\n",
            "systems windows ubuntu                   0.1433\n",
            "better way                               0.1356\n",
            "intermediate languages mipower           0.1239\n",
            "intermediate github gitbash              0.0326\n",
            "january 2013                             0.0311\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "operating systems\n",
            "customer relationship management\n",
            "systems design\n",
            "marketing strategy\n",
            "deep learning\n",
            "engineers\n",
            "software\n",
            "control systems\n",
            "education\n",
            "matlab\n",
            "programming languages\n",
            "machine learning\n",
            "java\n",
            "linux\n",
            "correlation analysis\n",
            "social sciences\n",
            "python\n",
            "fighter aircraft\n",
            "interpreter\n",
            "web development\n",
            "electronics engineering\n",
            "engineering\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Analytics Technical Activities           0.00021058230775616635\n",
            "Hindustan Aeronautics Limited            0.00021331167617755036\n",
            "Interest Deep Learning                   0.00036924769641597313\n",
            "Control System Design                    0.0004317951724844159\n",
            "Software Packages Interpreters-Anaconda  0.0004503142573288178\n",
            "Debian-Kali Linux Education              0.0004503142573288178\n",
            "Technical Skills Programming             0.0004537522553505406\n",
            "Linux Education Details                  0.00047347465755201933\n",
            "Python Software Packages                 0.00048227120043999733\n",
            "Operating Systems Windows                0.0004918330305775979\n",
            "Java IDE-Eclipse Operating               0.0005265418908278515\n",
            "Education Details January                0.0006813951331217836\n",
            "Skills Programming Matlab                0.0007000254319497953\n",
            "fighter aircraft Technical               0.0009797615702443806\n",
            "aircraft Technical Skills                0.0010133511005539191\n",
            "IDE-Eclipse Operating Systems            0.0012371491470601476\n",
            "ENTHUSIAST Skill Details                 0.0028171183480491563\n",
            "DEEKSHA CENTER January                   0.0029548569921934815\n",
            "Electric Machinery                       0.0029951112120128348\n",
            "Web Development                          0.0029951112120128348\n",
            "------------------------\n",
            "\n",
            "Resume # 4\n",
            "Skills â¢ R â¢ Python â¢ SAP HANA â¢ Tableau â¢ SAP HANA SQL â¢ SAP HANA PAL â¢ MS SQL â¢ SAP Lumira â¢ C# â¢ Linear Programming â¢ Data Modelling â¢ Advance\n",
            "Analytics â¢ SCM Analytics â¢ Retail Analytics â¢Social Media Analytics â¢ NLP Education Details   January 2017 to January 2018 PGDM Business Analytics  Great Lakes\n",
            "Institute of Management & Illinois Institute of Technology  January 2013 Bachelor of Engineering Electronics and Communication Bengaluru, Karnataka New Horizon College of\n",
            "Engineering, Bangalore Visvesvaraya Technological University  Data Science Consultant     Consultant - Deloitte USI  Skill Details   LINEAR PROGRAMMING- Exprience - 6 months\n",
            "RETAIL- Exprience - 6 months  RETAIL MARKETING- Exprience - 6 months  SCM- Exprience - 6 months  SQL- Exprience - Less than 1 year months  Deep Learning- Exprience - Less than\n",
            "1 year months  Machine learning- Exprience - Less than 1 year months  Python- Exprience - Less than 1 year months  R- Exprience - Less than 1 year monthsCompany Details\n",
            "company - Deloitte USI  description - The project involved analysing historic deals and coming with insights to optimize future deals.  Role: Was given raw data, carried out\n",
            "end to end analysis and presented insights to client.  Key Responsibilities:  â¢ Extract data from client systems across geographies.  â¢ Understand and build reports in\n",
            "tableau. Infer meaningful insights to optimize prices and find out process blockades.  Technical Environment: R, Tableau.    Industry: Cross Industry  Service Area: Cross\n",
            "Industry - Products  Project Name: Handwriting recognition  Consultant: 3 months.  The project involved taking handwritten images and converting them to digital text images by\n",
            "object detection and sentence creation.  Role: I was developing sentence correction functionality.  Key Responsibilities:  â¢ Gather data large enough to capture all English\n",
            "words  â¢ Train LSTM models on words.  Technical Environment: Python.    Industry: Finance  Service Area: Financial Services - BI development Project Name: SWIFT  Consultant:\n",
            "8 months.  The project was to develop an analytics infrastructure on top of SAP S/4, it would user to view  financial reports to respective departments. Reporting also\n",
            "included forecasting expenses.  Role: I was leading the offshore team.  Key Responsibilities:  â¢ Design & Develop data models for reporting.  â¢ Develop ETL for data flow\n",
            "â¢ Validate various reports.  Technical Environment: SAP HANA, Tableau, SAP AO.    Industry: Healthcare Analytics  Service Area: Life Sciences - Product development Project\n",
            "Name: Clinical Healthcare System  Consultant: 2 months.  The project was to develop an analytics infrastructure on top of Argus, it would allow users to query faster and\n",
            "provide advance analytics capabilities.  Role: I was involved from design to deploy phase, performed a lot of data restructuring and built  models for insights.  Key\n",
            "Responsibilities:  â¢ Design & Develop data models for reporting.  â¢ Develop and deploy analytical models.  â¢ Validate various reports.  Technical Environment: Data\n",
            "Modelling, SAP HANA, Tableau, NLP.    Industry: FMCG  Service Area: Trade & Promotion  Project Name: Consumption Based Planning for Flowers Foods Consultant; 8 months.  The\n",
            "project involved setting up of CRM and CBP modules.  Role: I was involved in key data decomposition activities and setting up the base for future year  forecast. Over the\n",
            "course of the project I developed various models and carried out key  performance improvements.  Key Responsibilities:  â¢ Design & Develop HANA models for decomposition.\n",
            "â¢ Develop data flow for forecast.  â¢ Developed various views for reporting of Customer/Sales/Funds.  â¢ Validate various reports in BOBJ.  Technical Environment: Data\n",
            "Modelling, SAP HANA, BOBJ, Time Series Forecasting.    Internal Initiative Industry: FMCG  Customer Segmentation and RFM analysis Consultant; 3 months.  The initiative\n",
            "involved setting up of HANA-Python interface and advance analytics on Python. Over the course I had successfully segmented data into five core segments using K-means and\n",
            "carried out RFM analysis in Python. Also developed algorithm to categorize any new customer under the defined buckets.  Technical Environment: Anaconda3, Python3.6, HANA SPS12\n",
            "Industry: Telecom Invoice state detection Consultant; 1 months.  The initiative was to reduce the manual effort in verifying closed and open invoices manually, it  involved\n",
            "development to a decision tree to classify open/closed invoices. This enabled effort  reduction by 60%.  Technical Environment: R, SAP PAL, SAP HANA SPS12    Accenture\n",
            "Experience  Industry: Analytics - Cross Industry  In Process Analytics for SAP Senior Developer; 19 months.  Accenture Solutions Pvt. Ltd., India  The project involved\n",
            "development of SAP analytics tool - In Process Analytics (IPA) . My role was to develop database objects and data models to provide operational insights to clients.  Role: I\n",
            "have developed various Finance related KPIs and spearheaded various deployments.  Introduced SAP Predictive analytics to reduce development time and reuse functionalities for\n",
            "KPIs and prepared production planning reports.  Key Responsibilities:  â¢ Involved in information gather phase.  â¢ Designed and implemented SAP HANA data modelling using\n",
            "Attribute View, Analytic View, and  Calculation View.  â¢ Developed various KPI's individually using complex SQL scripts in Calculation views.  â¢ Created procedures in HANA\n",
            "Database.  â¢ Took ownership and developed Dashboard functionality.  â¢ Involved in building data processing algorithms to be executed in R server for cluster analysis.\n",
            "Technical Environment: R, SAP HANA, T-SQL.  Industry: Cross Industry  Accenture Testing Accelerator for SAP Database Developer; 21 months.  Accenture Solutions Pvt. Ltd.,\n",
            "India  Role: I have taken care of all development activities for the ATAS tool and have also completed  various deployments of the product.  Apart from these activities I was\n",
            "also actively involved in maintenance of the database servers  (Production & Quality)  Key Responsibilities:  â¢ Analyzing business requirements, understanding the scope,\n",
            "getting requirements clarified  interacting with business and further transform all requirements to generate attribute  mapping documents and reviewing mapping specification\n",
            "documentation  â¢ Create / Update database objects like tables, views, stored procedures, function, and packages  â¢ Monitored SQL Server Error Logs and Application Logs\n",
            "through SQL Server Agent  â¢ Prepared Data Flow Diagrams, Entity Relationship Diagrams using UML  â¢ Responsible for Designing, developing and Normalization of database\n",
            "tables  â¢ Experience in performance tuning using SQL profiler.  â¢ Involved in QA, UAT, knowledge transfer and support activities  Technical Environment: SQL Server\n",
            "2008/2014, Visual Studio 2010, Windows Server, Performance  Monitor, SQL Server Profiler, C#, PL-SQL, T-SQL.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "consultant months project                0.5037\n",
            "tableau industry cross                   0.365\n",
            "mapping documents reviewing              0.3089\n",
            "machine learning exprience               0.3077\n",
            "pal sap hana                             0.2333\n",
            "successfully segmented                   0.2071\n",
            "india role                               0.1897\n",
            "performance tuning                       0.1704\n",
            "customer sales funds                     0.1703\n",
            "reports bobj                             0.1698\n",
            "words                                    0.1676\n",
            "data flow validate                       0.1615\n",
            "database took ownership                  0.1534\n",
            "images converting digital                0.132\n",
            "environment anaconda3 python3            0.1252\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "deep learning\n",
            "sales\n",
            "database systems\n",
            "natural language processing\n",
            "knowledge transfer\n",
            "servers\n",
            "application servers\n",
            "long short term memory neural networks\n",
            "k-means algorithm\n",
            "education\n",
            "university\n",
            "engineering\n",
            "cluster analysis\n",
            "communication\n",
            "object detection\n",
            "decision trees\n",
            "retail stores\n",
            "business intelligence\n",
            "etl\n",
            "sql\n",
            "unified modeling language\n",
            "business requirement\n",
            "data analytics\n",
            "social media\n",
            "trade\n",
            "customer segmentation\n",
            "handwriting recognition\n",
            "crm\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Education Details January                0.00018162522897720009\n",
            "USI Skill Details                        0.0002894187042412499\n",
            "Deloitte USI Skill                       0.0003210828278811116\n",
            "Great Lakes Institute                    0.0003383773288338457\n",
            "Bangalore Visvesvaraya Technological     0.0004157407886629153\n",
            "Visvesvaraya Technological University    0.00042618195466056304\n",
            "Details LINEAR PROGRAMMING               0.00043472956641101105\n",
            "Skill Details LINEAR                     0.00044697034872666374\n",
            "Deloitte USI description                 0.0010580615714846408\n",
            "NLP Education Details                    0.0015761238924950052\n",
            "Social Media Analytics                   0.0016363032400360166\n",
            "Analytics Great Lakes                    0.0016779837282868825\n",
            "Deloitte USI                             0.0016880669658700348\n",
            "Technological University Data            0.00198009242331086\n",
            "months RETAIL MARKETING                  0.0020033375034951644\n",
            "Exprience                                0.0021725312610131856\n",
            "Details January                          0.002409479965681607\n",
            "months Deep Learning                     0.002923111959002946\n",
            "months Machine learning                  0.002923111959002946\n",
            "Technology January                       0.0035427804003320064\n",
            "------------------------\n",
            "\n",
            "Resume # 5\n",
            "Education Details    MCA   YMCAUST,  Faridabad,  Haryana  Data Science internship       Skill Details   Data Structure- Exprience - Less than 1 year months  C- Exprience -\n",
            "Less than 1 year months  Data Analysis- Exprience - Less than 1 year months  Python- Exprience - Less than 1 year months  Core Java- Exprience - Less than 1 year months\n",
            "Database Management- Exprience - Less than 1 year monthsCompany Details   company - Itechpower  description -\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data science internship                  0.6472\n",
            "haryana data                             0.4553\n",
            "data                                     0.4009\n",
            "details company itechpower               0.3907\n",
            "monthscompany details company            0.3799\n",
            "database management exprience            0.3678\n",
            "details data structure                   0.3593\n",
            "education details                        0.3283\n",
            "core java exprience                      0.3084\n",
            "ymcaust faridabad                        0.2667\n",
            "analysis                                 0.2572\n",
            "mca                                      0.2376\n",
            "skill                                    0.2134\n",
            "structure exprience year                 0.2095\n",
            "year months python                       0.1772\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "java\n",
            "database systems\n",
            "database management\n",
            "education\n",
            "social sciences\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Details MCA YMCAUST                      0.0002181765288601245\n",
            "Haryana Data Science                     0.00021817652886012452\n",
            "Details Data Structure                   0.00026816225632926756\n",
            "Skill Details Data                       0.00032496687907962923\n",
            "year months Python                       0.00037008125734277217\n",
            "months Core Java                         0.0003920697809047036\n",
            "months Database Management               0.0003920697809047036\n",
            "Education Details MCA                    0.00039849074478743516\n",
            "Science internship Skill                 0.0004131567435625943\n",
            "year months Core                         0.00044865152977931986\n",
            "year months Database                     0.00044865152977931986\n",
            "year monthsCompany Details               0.0004796287471625304\n",
            "months Data Analysis                     0.00048224612941244797\n",
            "Data Science internship                  0.0005081322571257719\n",
            "internship Skill Details                 0.0005081322571257719\n",
            "year months Data                         0.0005518830795950124\n",
            "monthsCompany Details company            0.0007670564891835166\n",
            "year months                              0.0014871732265611297\n",
            "MCA YMCAUST                              0.002954973973139864\n",
            "Core Java                                0.002954973973139864\n",
            "------------------------\n",
            "\n",
            "Resume # 6\n",
            "SKILLS C Basics, IOT, Python, MATLAB, Data Science, Machine Learning, HTML, Microsoft Word, Microsoft Excel, Microsoft Powerpoint. RECOGNITION Academic Secured First place in\n",
            "B.Tech.Education Details   August 2014 to May 2018 B.Tech.  Ghatkesar, Andhra Pradesh Aurora's Scientific and Technological Institute  June 2012 to May 2014  Secondary\n",
            "Education Warangal, Telangana SR Junior College  Data Science       Skill Details   MS OFFICE- Exprience - Less than 1 year months  C- Exprience - Less than 1 year months\n",
            "machine learning- Exprience - Less than 1 year months  data science- Exprience - Less than 1 year months  Matlab- Exprience - Less than 1 year monthsCompany Details   company\n",
            "-   description -\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "tech ghatkesar andhra                    0.6135\n",
            "data science skill                       0.5292\n",
            "recognition academic                     0.3973\n",
            "monthscompany details company            0.331\n",
            "machine learning html                    0.3264\n",
            "science machine                          0.2997\n",
            "ms office exprience                      0.2814\n",
            "python matlab data                       0.2635\n",
            "basics iot                               0.2179\n",
            "august 2014 2018                         0.1868\n",
            "warangal                                 0.1793\n",
            "junior                                   0.1421\n",
            "aurora                                   0.1128\n",
            "details                                  0.1018\n",
            "secured place                            0.0925\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "machine learning\n",
            "matlab\n",
            "python\n",
            "html\n",
            "education\n",
            "internet of things\n",
            "social sciences\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Microsoft Word                           0.0039402825158235625\n",
            "Microsoft Excel                          0.0039402825158235625\n",
            "Microsoft Powerpoint                     0.0039402825158235625\n",
            "RECOGNITION Academic Secured             0.012020935001603176\n",
            "Data Science                             0.018520397960923644\n",
            "Data Science Skill                       0.018521554648800512\n",
            "College Data Science                     0.019132037357389234\n",
            "Microsoft                                0.019895816065672152\n",
            "Exprience                                0.020339847540416362\n",
            "Science Skill Details                    0.023508586264412948\n",
            "Machine Learning                         0.024303149958452685\n",
            "Secondary Education Warangal             0.025888846810871387\n",
            "Junior College Data                      0.026383463851783653\n",
            "months machine learning                  0.029741421772272694\n",
            "Andhra Pradesh Aurora                    0.030734529585472424\n",
            "Technological Institute June             0.030734529585472424\n",
            "months data science                      0.034231689966687365\n",
            "Pradesh Aurora Scientific                0.03653756793846523\n",
            "year months                              0.042431442955554934\n",
            "year months Matlab                       0.04276885853764269\n",
            "------------------------\n",
            "\n",
            "Resume # 7\n",
            "Skills â¢ Python â¢ Tableau â¢ Data Visualization â¢ R Studio â¢ Machine Learning â¢ Statistics IABAC Certified Data Scientist with versatile experience over 1+ years in\n",
            "managing business, data science consulting and leading innovation projects, bringing business ideas to working real world solutions. Being a strong advocator of augmented era,\n",
            "where human capabilities are enhanced by machines, Fahed is passionate about bringing business concepts in area of machine learning, AI, robotics etc., to real life\n",
            "solutions.Education Details   January 2017 B. Tech Computer Science & Engineering Mohali, Punjab Indo Global College of Engineering  Data Science Consultant     Data Science\n",
            "Consultant - Datamites  Skill Details   MACHINE LEARNING- Exprience - 13 months  PYTHON- Exprience - 24 months  SOLUTIONS- Exprience - 24 months  DATA SCIENCE- Exprience - 24\n",
            "months  DATA VISUALIZATION- Exprience - 24 months  Tableau- Exprience - 24 monthsCompany Details   company - Datamites  description - â¢ Analyzed and processed complex data\n",
            "sets using advanced querying, visualization and analytics tools.  â¢ Responsible for loading, extracting and validation of client data.  â¢ Worked on manipulating, cleaning\n",
            "& processing data using python.  â¢ Used Tableau for data visualization.  company - Heretic Solutions Pvt Ltd  description - â¢ Worked closely with business to identify\n",
            "issues and used data to propose solutions for effective decision making.  â¢ Manipulating, cleansing & processing data using Python, Excel and R.  â¢ Analyzed raw data,\n",
            "drawing conclusions & developing recommendations.  â¢ Used machine learning tools and statistical techniques to produce solutions to problems.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "consultant datamites skill               0.6591\n",
            "python tableau data                      0.4652\n",
            "machines                                 0.3136\n",
            "advanced querying                        0.2965\n",
            "python exprience 24                      0.2735\n",
            "global college engineering               0.2686\n",
            "propose solutions                        0.2449\n",
            "fahed                                    0.2299\n",
            "learning ai robotics                     0.2293\n",
            "mohali punjab indo                       0.2175\n",
            "manipulating cleansing processing        0.2169\n",
            "company heretic                          0.2031\n",
            "advocator augmented era                  0.1527\n",
            "january 2017                             0.1182\n",
            "pvt description                          0.1089\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "machine learning\n",
            "data visualization\n",
            "computer science\n",
            "business process\n",
            "validation\n",
            "correlation analysis\n",
            "social sciences\n",
            "recommendation\n",
            "visualization\n",
            "robotics\n",
            "education\n",
            "visualization tools\n",
            "engineering\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Statistics IABAC Certified               0.00036864100169818967\n",
            "IABAC Certified Data                     0.0018253787910745807\n",
            "Certified Data Scientist                 0.0018253787910745807\n",
            "Statistics IABAC                         0.00498816575891728\n",
            "IABAC Certified                          0.005385710836311923\n",
            "Data Science Consultant                  0.007369983095034629\n",
            "leading innovation projects              0.012485995108105228\n",
            "data science                             0.013493776407393807\n",
            "Datamites Skill Details                  0.014160315718467618\n",
            "Science Consultant Data                  0.014739966190069256\n",
            "Consultant Data Science                  0.014739966190069257\n",
            "Details MACHINE LEARNING                 0.015620814051861864\n",
            "Skill Details MACHINE                    0.01569603676591743\n",
            "Engineering Data Science                 0.015698142521511697\n",
            "Scientist with versatile                 0.01707668684175852\n",
            "Machine Learning                         0.017181357403311653\n",
            "Exprience                                0.01744961645991836\n",
            "working real world                       0.019544085332003934\n",
            "Science Consultant                       0.02024687089655692\n",
            "months DATA SCIENCE                      0.020309660301799058\n",
            "------------------------\n",
            "\n",
            "Resume # 8\n",
            "Education Details    B.Tech   Rayat and Bahra Institute of Engineering and Biotechnology  Data Science     Data Science  Skill Details   Numpy- Exprience - Less than 1 year\n",
            "months  Machine Learning- Exprience - Less than 1 year months  Tensorflow- Exprience - Less than 1 year months  Scikit- Exprience - Less than 1 year months  Python- Exprience\n",
            "- Less than 1 year months  GCP- Exprience - Less than 1 year months  Pandas- Exprience - Less than 1 year months  Neural Network- Exprience - Less than 1 year monthsCompany\n",
            "Details   company - Wipro  description - Bhawana Aggarwal  E-Mail:bhawana.chd@gmail.com  Phone: 09876971076  VVersatile, high-energy professional targeting challenging\n",
            "assignments in Machine  PROFILE SUMMARY  âª An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine  Learning, Deep Learning, Data\n",
            "Science, Python, Software Development.  âª Skilled in managing end-to-end development and software products / projects from inception, requirement  specs, planning,\n",
            "designing, implementation, configuration and documentation.  âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,  NLP, GCP.\n",
            "âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.  âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias\n",
            "variance Trade off,  Support vector Machine(SVM),Logistic Regression, Neural networks.  âª Have knowledge on unsupervised, Supervised and reinforcement data.  âª Programming\n",
            "experience in relational platforms like MySQL,Oracle.  âª Have knowledge on Some programming language like C++,Java.  âª Experience in cloud based environment like Google\n",
            "Cloud.  âª Working on different Operating System like Linux, Ubuntu, Windows.  âª Good interpersonal and communication skills.  âª Problem solving skills with the ability\n",
            "to think laterally, and to think with a medium term and long term  perspective  âª Flexibility and an open attitude to change.  âª Ability to create, define and own\n",
            "frameworks with a strong emphasis on code reusability.  TECHNICAL SKILLS  Programming Languages Python, C  Libraries Seaborn, Numpy, Pandas, Cufflinks, Matplotlib  Algorithms\n",
            "KNN, Decision Tree, Linear regression, Logistic Regression, Neural Networks, K means clustering,  Tensorflow, SVM  Databases SQL, Oracle  Operating Systems Linux, Window\n",
            "Development Environments NetBeans, Notebooks, Sublime  Ticketing tools Service Now, Remedy  Education  UG Education:  B.Tech (Computer Science) from Rayat and Bahra Institute\n",
            "of Engineering and Biotechnology passed with 78.4%in  2016.  Schooling:  XII in 2012 from Moti Ram Arya Sr. Secondary School(Passed with 78.4%)  X in 2010 from Valley Public\n",
            "School (Passed with 9.4 CGPA)  WORK EXPERINCE  Title : Wipro Neural Intelligence Platform  Team Size : 5  Brief: Wiproâs Neural Intelligence Platform harnesses the power of\n",
            "automation and artificial intelligence  technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform  comprises three layers: a\n",
            "data engagement platform that can easily access and manage multiple structured and  unstructured data sources; an âintent assessment and reasoningâ engine that includes\n",
            "sentiment and predictive  analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed  automating responses to user queries at\n",
            "the earliest. The Monster Bot using the power of Deep Machine Learning,  NLP to handle such queries. User can see the how their queries can be answered quickly like allL1\n",
            "activities can be  eliminated.  Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like  dates, names, places,\n",
            "contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using  Tensor flow for further learning of new entities.  Classifier ->This involves the\n",
            "classifications of classes, training of dataset and predicting the output using the SKLearn  classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the\n",
            "user queries with the best  suited response and make the system efficient.  NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like\n",
            "Roles, Skills,  Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities  using Keras TensorFlow framework.  OTHER\n",
            "PROJECTS  Title : Diabetes Detection  Brief : Developed the software which can detect whether the person is suffering from Diabetes or not and got the third  prize in it.\n",
            "TRAINING AND CERTIFICATIONS  Title: Python Training, Machine Learning, Data Science, Deep Learning  Organization: Udemy, Coursera (Machine Learning, Deep Learning)  Personal\n",
            "Profile  Fatherâs Name :Mr. Tirlok Aggarwal  Language Known : English & Hindi  Marital Status :Single  Date of Birth(Gender):1993-12-20(YYYY-MM-DD) (F)  company - Wipro\n",
            "description - Developing programs in Python.  company - Wipro  description - Title : Wipro Neural Intelligence Platform  Team Size : 5  Brief: Wiproâs Neural Intelligence\n",
            "Platform harnesses the power of automation and artificial intelligence  technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The\n",
            "platform  comprises three layers: a data engagement platform that can easily access and manage multiple structured and  unstructured data sources; an âintent assessment and\n",
            "reasoningâ engine that includes sentiment and predictive  analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed\n",
            "automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,  NLP to handle such queries. User can see the how their queries\n",
            "can be answered quickly like allL1 activities can be  eliminated.  Entity Extractor -> This involves text extraction and NLP for fetching out important information from the\n",
            "text like  dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using  Tensor flow for further learning of new entities.\n",
            "Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn  classifier (MNB, SVM, SGD as Classifier) and SGD\n",
            "for the optimization to map the user queries with the best  suited response and make the system efficient.  NER: A Deep Learning NER Model is trained to extract the entities\n",
            "from the text. Entities like Roles, Skills,  Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities  using Keras\n",
            "TensorFlow framework.  company - Wipro Technologies  description - An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine  Learning, Deep\n",
            "Learning, Data Science, Python, Software Development.  âª Skilled in managing end-to-end development and software products / projects from inception, requirement  specs,\n",
            "planning, designing, implementation, configuration and documentation.  âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,\n",
            "NLP, GCP.  âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.  âª Knowledge on different algorithms in Machine learning like KNN, Decision\n",
            "Tree, Bias variance Trade off,  Support vector Machine(SVM),Logistic Regression, Neural networks.  âª Have knowledge on unsupervised, Supervised and reinforcement data.  âª\n",
            "Programming experience in relational platforms like MySQL,Oracle.  âª Have knowledge on Some programming language like C++,Java.  âª Experience in cloud based environment\n",
            "like Google Cloud.  âª Working on different Operating System like Linux, Ubuntu, Windows.  âª Good interpersonal and communication skills.  âª Problem solving skills with\n",
            "the ability to think laterally, and to think with a medium term and long term  perspective  âª Flexibility and an open attitude to change.  âª Ability to create, define and\n",
            "own frameworks with a strong emphasis on code reusability.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "python machine learning                  0.5357\n",
            "details company wipro                    0.4169\n",
            "inception requirement specs              0.3608\n",
            "targeting challenging assignments        0.2715\n",
            "sgd optimization map                     0.2253\n",
            "engineering biotechnology passed         0.2245\n",
            "best suited                              0.2005\n",
            "involves regex bluemix                   0.1643\n",
            "aggarwal language                        0.1572\n",
            "cloud working different                  0.1484\n",
            "netbeans notebooks sublime               0.1448\n",
            "rnn lstm bidirectional                   0.1348\n",
            "change ability create                    0.1324\n",
            "single date birth                        0.1029\n",
            "alll1 activities eliminated              0.0262\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "operating systems\n",
            "deep learning\n",
            "logistic regression\n",
            "network architecture\n",
            "neural networks\n",
            "database systems\n",
            "natural language processing\n",
            "learning organizations\n",
            "feedforward neural networks\n",
            "long short term memory neural networks\n",
            "recurrent neural networks\n",
            "libraries\n",
            "programming languages\n",
            "machine learning\n",
            "engine\n",
            "k-nearest neighbors\n",
            "communication\n",
            "linux\n",
            "syntactic parsing\n",
            "software frameworks\n",
            "decision trees\n",
            "cloud services\n",
            "software project\n",
            "social sciences\n",
            "trade\n",
            "automation\n",
            "engineering\n",
            "named entity recognition\n",
            "computer science\n",
            "svm\n",
            "software development\n",
            "clustering algorithms\n",
            "artificial intelligence\n",
            "support vector machine\n",
            "online learning\n",
            "education\n",
            "optimization\n",
            "logistics\n",
            "classifiers\n",
            "java\n",
            "software\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "deep machine learning                    0.0017753946748531575\n",
            "Machine Learning                         0.001834859167631472\n",
            "Exprience                                0.0028189533900553226\n",
            "months Machine Learning                  0.002849906957653728\n",
            "year months Scikit                       0.00286386283810162\n",
            "Deep Learning                            0.004801173317492964\n",
            "months Neural Network                    0.00504759763495954\n",
            "year months                              0.005339357479634245\n",
            "Science Skill Details                    0.005654272306782169\n",
            "Skill Details Numpy                      0.006032587811780748\n",
            "Learning                                 0.006423233559942249\n",
            "months Scikit                            0.006638816721362282\n",
            "Neural Intelligence Platform             0.006763290647851393\n",
            "year months Machine                      0.007057804211869089\n",
            "Deep Learning NER                        0.00780456445268448\n",
            "Machine PROFILE SUMMARY                  0.008113952381294929\n",
            "year months GCP                          0.008251943752127427\n",
            "year months Neural                       0.008264719338530069\n",
            "year monthsCompany Details               0.008334593874315255\n",
            "Machine                                  0.008720961985693517\n",
            "------------------------\n",
            "\n",
            "Resume # 9\n",
            "Personal Skills â¢ Ability to quickly grasp technical aspects and willingness to learn â¢ High energy levels & Result oriented. Education Details   January 2018 Master of\n",
            "Engineering Computer Technology & Application Bhopal, Madhya Pradesh Truba Institute of Engineering & Information Technology  January 2010 B.E. computer science Bhopal, Madhya\n",
            "Pradesh RKDF Institute of Science and Technology College of Engineering  January 2006 Polytechnic Information Technology Vidisha, Madhya Pradesh SATI Engineering College in\n",
            "Vidisha  January 2003 M.tech Thesis Detail  BMCH School in Ganj basoda  Data science     I have six month experience in Data Science. Key Skills: - Experience in Machine\n",
            "Learning, Deep Leaning, NLP, Python, SQL, Web Scraping Good knowledge in computer subjects and ability to update  Skill Details   Experience in Machine Learning, Deep\n",
            "Learning, NLP, Python, SQL, Web Crawling, HTML,CSS.- Exprience - Less than 1 year monthsCompany Details   company - RNT.AI Technology Solution  description - Text\n",
            "classification using Machine learning Algorithms with python.  Practical knowledge of Deep learning algorithms such as Â Recurrent Neural Networks(RNN).  Develop custom data\n",
            "models and algorithms to apply to dataset  Experience with Python packages like Pandas, Scikit-learn, Tensor Flow, Numpy, Matplotliv, NLTK.  Comfort with SQL, Â MYSQL\n",
            "Sentiment analysis.  Â Apply leave Dataset using classification technique like Tf--idf , LSA with cosine similarity using Machine learning Algorithms.  Web crawling using\n",
            "Selenium web driver and Beautiful Soup with python.  company - Life Insurance Corporation of India Bhopal  description - Ã¼Â Explaining policy features and the benefits  Ã¼\n",
            "Updated knowledge of life insurance products and shared with customers\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "key skills experience                    0.5118\n",
            "ai technology solution                   0.4639\n",
            "pandas scikit learn                      0.3492\n",
            "mysql sentiment                          0.2693\n",
            "rnn develop custom                       0.2662\n",
            "data                                     0.2604\n",
            "idf lsa cosine                           0.2267\n",
            "apply leave                              0.1941\n",
            "crawling using selenium                  0.1697\n",
            "ã¼â explaining policy                    0.1568\n",
            "madhya                                   0.1386\n",
            "insurance products shared                0.0998\n",
            "like tf                                  0.088\n",
            "january 2010 computer                    0.0638\n",
            "energy levels result                     0.0303\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "machine learning\n",
            "computer science\n",
            "information technology\n",
            "deep learning\n",
            "sentiment analysis\n",
            "recurrent neural networks\n",
            "text classification\n",
            "classification models\n",
            "social sciences\n",
            "natural language processing\n",
            "web crawling\n",
            "html\n",
            "css\n",
            "education\n",
            "engineering\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "High energy levels                       0.0037195479629737512\n",
            "Madhya Pradesh Truba                     0.006246440089410667\n",
            "Madhya Pradesh RKDF                      0.006246440089410667\n",
            "Madhya Pradesh SATI                      0.006246440089410667\n",
            "Madhya Pradesh                           0.008334896367877256\n",
            "Information Technology January           0.0092166745587366\n",
            "Pradesh SATI Engineering                 0.009510262799333358\n",
            "Pradesh Truba Institute                  0.010296206822821044\n",
            "Pradesh RKDF Institute                   0.010296206822821044\n",
            "SATI Engineering College                 0.011754094971427546\n",
            "Polytechnic Information Technology       0.012205997951683409\n",
            "Information Technology Vidisha           0.01357993727243707\n",
            "Result oriented                          0.014394519218481667\n",
            "M.tech Thesis Detail                     0.01470051340509247\n",
            "Machine learning Algorithms              0.015099171661402224\n",
            "Machine Learning                         0.016729322130960542\n",
            "Thesis Detail BMCH                       0.016730598664725045\n",
            "Detail BMCH School                       0.016730598664725045\n",
            "quickly grasp technical                  0.016819814090819535\n",
            "grasp technical aspects                  0.016819814090819535\n",
            "------------------------\n",
            "\n",
            "Resume # 10\n",
            "Expertise â Data and Quantitative Analysis â Decision Analytics â Predictive Modeling â Data-Driven Personalization â KPI Dashboards â Big Data Queries and\n",
            "Interpretation â Data Mining and Visualization Tools â Machine Learning Algorithms â Business Intelligence (BI) â Research, Reports and Forecasts Education Details\n",
            "PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business   B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES\n",
            "IPS Academy  Data Scientist     Data Scientist with PR Canada  Skill Details   Algorithms- Exprience - 6 months  BI- Exprience - 6 months  Business Intelligence- Exprience - 6\n",
            "months  Machine Learning- Exprience - 24 months  Visualization- Exprience - 24 months  spark- Exprience - 24 months  python- Exprience - 36 months  tableau- Exprience - 36\n",
            "months  Data Analysis- Exprience - 24 monthsCompany Details   company - Aegis school of Data Science & Business  description - Mostly working on industry project for providing\n",
            "solution along with Teaching Appointments: Teach undergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis School of Data\n",
            "Science, Mumbai (2017 to Present)  company - Aegis school of Data & Business  description - Data Science Intern, Nov 2015 to Jan 2016    Furnish executive leadership team with\n",
            "insights, analytics, reports and recommendations enabling effective strategic planning across all business units, distribution channels and product lines.    â Chat Bot\n",
            "using AWS LEX and Tensor flow  Python  The goal of project creates a chat bot for an academic institution or university to handle queries related courses offered by that\n",
            "institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this companies handle their client 24x7. In this case companies are\n",
            "academic institutions and clients are participants or students.  â Web scraping using Selenium web driver   Python  The task is to scrap the data from the online messaging\n",
            "portal in a text format and have to find the pattern form it.  â Data Visualization and Data insights   Hadoop Eco System, Hive, PySpark, QlikSense  The goal of this project\n",
            "is to build a Business Solutions to a Internet Service Provider Company, like handling data which is generated per day basis, for that we have to visualize that data and find\n",
            "the usage pattern form it and have a generate a reports.  â Image Based Fraud Detection   Microsoft Face API, PySpark, Open CV  The main goal of project is Recognize\n",
            "similarity for a face to given Database images. Face recognition is the recognizing a special face from set of different faces. Face is extracted and then compared with the\n",
            "database Image if that Image recognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how we are going to prevent\n",
            "the frauds in the initial stage itself.  â Churn Analysis for Internet Service Provider   R, Python, Machine Learning, Hadoop  The objective is to identify the customer who\n",
            "is likely to churn in a given period of time; we have to pretend the customer giving incentive offers.  â Sentiment Analysis   Python, NLP, Apache Spark service in IBM\n",
            "Bluemix.  This project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the\n",
            "expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.    Quantifiable Results:  â Mentored 7-12 Data Science\n",
            "Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.  â Reviewed and evaluated 20-40 Research Papers on Data Science\n",
            "for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.  â Heading a solution providing organization called Data\n",
            "Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.  â Working for some social cause with the help of\n",
            "Data Science for Social Goods Committee, where our team developed a product called \"Let's find a missing Child\" for helping society.  company - IBM India pvt ltd  description\n",
            "- Mostly worked on blumix and IBM Watson for Data science.\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "bot academic institution                 0.5872\n",
            "big data queries                         0.3848\n",
            "task scrap data                          0.3788\n",
            "cv main goal                             0.3139\n",
            "reports forecasts                        0.2734\n",
            "client 24x7 case                         0.2414\n",
            "pyspark qliksense                        0.2386\n",
            "emotions tweets comparative              0.2299\n",
            "recommendations enabling                 0.2214\n",
            "selenium web driver                      0.2054\n",
            "microsoft face api                       0.1656\n",
            "reduce man errors                        0.1211\n",
            "pattern form generate                    0.0962\n",
            "offered                                  0.0725\n",
            "child                                    0.0526\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "faculty\n",
            "sentiment analysis\n",
            "internet\n",
            "data mining\n",
            "database systems\n",
            "natural language processing\n",
            "strategic planning\n",
            "hadoop\n",
            "big data\n",
            "visualization\n",
            "personalizations\n",
            "education\n",
            "face recognition\n",
            "university\n",
            "machine learning\n",
            "data visualization\n",
            "communication\n",
            "product lines\n",
            "internet service providers\n",
            "twitter\n",
            "database images\n",
            "business intelligence\n",
            "social sciences\n",
            "recommendation\n",
            "mobile networks\n",
            "visualization tools\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Skill Details Algorithms                 0.00023051827981567454\n",
            "Forecasts Education Details              0.0002767963940471423\n",
            "Education Details PGP                    0.0002767963940471423\n",
            "Canada Skill Details                     0.0002767963940471423\n",
            "Madhya Pradesh IES                       0.0003744199053228935\n",
            "Pradesh IES IPS                          0.0003892958387099617\n",
            "IES IPS Academy                          0.0003892958387099617\n",
            "Scientist Data Scientist                 0.0006790926404393161\n",
            "Academy Data Scientist                   0.0008492360800934367\n",
            "months Business Intelligence             0.0010016307270116834\n",
            "IPS Academy Data                         0.0010621583950900066\n",
            "Exprience                                0.002010061963063472\n",
            "Data Scientist Data                      0.00234633261834776\n",
            "Data Science                             0.0026355050094243214\n",
            "Big Data Queries                         0.0026889278849998644\n",
            "Data Science Intern                      0.002827801548972571\n",
            "Communication Electronics                0.0031450837805850527\n",
            "Details Algorithms                       0.003148508890738625\n",
            "Education Details                        0.0037775714079601136\n",
            "Details PGP                              0.0037775714079601136\n",
            "------------------------\n",
            "\n",
            "Resume # 11\n",
            "Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN,\n",
            "Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic\n",
            "Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot,\n",
            "Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education\n",
            "Details     Data Science Assurance Associate     Data Science Assurance Associate - Ernst & Young LLP  Skill Details   JAVASCRIPT- Exprience - 24 months  jQuery- Exprience -\n",
            "24 months  Python- Exprience - 24 monthsCompany Details   company - Ernst & Young LLP  description - Fraud Investigations and Dispute Services   Assurance  TECHNOLOGY ASSISTED\n",
            "REVIEW  TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.  * Core member of a team helped in developing\n",
            "automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in\n",
            "reduced labor costs and time spent during the lawyers review.  * Understand the end to end flow of the solution, doing research and development for classification models,\n",
            "predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.  * TAR assists in\n",
            "predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
            "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment\n",
            "analysis. Matplot lib, Tableau dashboard for reporting.    MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)  TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA *\n",
            "Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4\n",
            "categories.  * Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted\n",
            "Word cloud.  * Created customized tableau dashboards for effective reporting and visualizations.  CHATBOT * Developed a user friendly chatbot for one of our Products which\n",
            "handle simple questions about hours of operation, reservation options and so on.  * This chat bot serves entire product related questions. Giving overview of tool via QA\n",
            "platform and also give recommendation responses so that user question to build chain of relevant answer.  * This too has intelligence to build the pipeline of questions as per\n",
            "user requirement and asks the relevant /recommended questions.    Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis,\n",
            "Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer    INFORMATION GOVERNANCE  Organizations to make informed decisions about all of the information they store. The\n",
            "integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to\n",
            "counter information risk.  * Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic\n",
            "search and created customized, interactive dashboards using kibana.  * Preforming ROT Analysis on the data which give information of data which helps identify content that is\n",
            "either Redundant, Outdated, or Trivial.  * Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable\n",
            "information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.  Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
            "FRAUD ANALYTIC PLATFORM  Fraud Analytics and investigative platform to review all red flag cases.  â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case\n",
            "manager and suite of Analytics for various ERP systems.  * It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be\n",
            "indicators of fraud by running advanced analytics  Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js\n",
            "\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "developing automated review              0.5601\n",
            "associate data science                   0.4096\n",
            "lawyers                                  0.3539\n",
            "portfolio synthesizes intelligence       0.3337\n",
            "tar technology assisted                  0.2238\n",
            "relevant recommended                     0.2234\n",
            "bayes knn random                         0.213\n",
            "platform fraud                           0.2127\n",
            "tableau regular expression               0.196\n",
            "word embedding                           0.1828\n",
            "elasticsearch d3 js                      0.1816\n",
            "exprience 24 months                      0.1683\n",
            "kafka python flask                       0.0694\n",
            "pii personally                           0.0692\n",
            "jquery css bootstrap                     0.0626\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "vehicles\n",
            "text document\n",
            "boosting\n",
            "sentiment analysis\n",
            "deep learning\n",
            "ajax\n",
            "database systems\n",
            "natural language processing\n",
            "text data\n",
            "css\n",
            "user information\n",
            "programming languages\n",
            "machine learning\n",
            "k-nearest neighbors\n",
            "word embedding\n",
            "customer review\n",
            "decision trees\n",
            "recommendation\n",
            "question answering\n",
            "erp system\n",
            "classification models\n",
            "visualization tools\n",
            "cyber-attacks\n",
            "javascript\n",
            "svm\n",
            "dimensionality reduction\n",
            "natural languages\n",
            "visualization\n",
            "computer vision\n",
            "textual data\n",
            "html\n",
            "classifiers\n",
            "random forests\n",
            "java\n",
            "cluster analysis\n",
            "python\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Programming Languages                    0.010927614604646271\n",
            "Science Assurance Associate              0.011850855598096734\n",
            "Data Science Assurance                   0.01884628369710474\n",
            "LLP Skill Details                        0.021232972451533626\n",
            "Young LLP Skill                          0.021644888423718343\n",
            "Natural Language processing              0.0219379848158316\n",
            "Dispute Services Assurance               0.028486093511906674\n",
            "Topic Modelling                          0.028515384907894414\n",
            "Natural Language                         0.028980000433086836\n",
            "TECHNOLOGY ASSISTED REVIEW               0.029066281762730874\n",
            "Assurance Associate Data                 0.02942891833560471\n",
            "Services Assurance TECHNOLOGY            0.03152194966396776\n",
            "Assurance Associate                      0.03158419270725324\n",
            "Assurance TECHNOLOGY ASSISTED            0.03343593688541541\n",
            "Skill Details JAVASCRIPT                 0.0339487296732544\n",
            "Young LLP                                0.03468422940022623\n",
            "Data                                     0.0357887990823203\n",
            "Analysis                                 0.03889168712283793\n",
            "Science Assurance                        0.039640667318250715\n",
            "Details Data Science                     0.04085483920589676\n",
            "------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume and Job Matching"
      ],
      "metadata": {
        "id": "MsExUakXPC_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume and Job Posting Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, all_resumes_keywords)\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQHvfMS9l7Qt",
        "outputId": "895a50d0-b928-41af-d1c3-ff8643309eff"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 1\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 15.02 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 19.14 %\n",
            "Matched JP Description: This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction\n",
            "and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and\n",
            "programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.82 %\n",
            "Matched JP Description: This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction\n",
            "and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and\n",
            "programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 2\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.88 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 15.73 %\n",
            "Matched JP Description: This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction\n",
            "and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and\n",
            "programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Statistician (Data Scientist)\n",
            "With a score of: 12.95 %\n",
            "Matched JP Description: This vacancy is for a Statistician (Data Scientist) position in the Department of Commerce located at the U.S. Census Bureau Headquarters in Suitland,\n",
            "Maryland. The Census Bureau is accessible from the Metro Rail Green Line - Suitland Station. This Job Opportunity Announcement may be used to fill other Statistician (Data\n",
            "Scientist)-1530-11/12, FPL GS-12 positions within the Census Bureau in the same geographical location with the same qualifications and specialized experience.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 3\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist, CG-1560-13/14\n",
            "With a score of: 14.08 %\n",
            "Matched JP Description: This position is located in the Division of Complex Institution Supervision and Resolution (CISR), Systemic Risk Branch, Data Analytics Sub-Branch,\n",
            "Data Analytics Section of the Federal Deposit Insurance Corporation. Please see the Clarification from Agency and Additional Information sections below for more information on\n",
            "telework options.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 22.49 %\n",
            "Matched JP Description: As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the\n",
            "body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through\n",
            "the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 13.61 %\n",
            "Matched JP Description: This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction\n",
            "and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and\n",
            "programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 4\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist, CG-1560-13/14\n",
            "With a score of: 13.23 %\n",
            "Matched JP Description: This position is located in the Division of Complex Institution Supervision and Resolution (CISR), Systemic Risk Branch, Data Analytics Sub-Branch,\n",
            "Data Analytics Section of the Federal Deposit Insurance Corporation. Please see the Clarification from Agency and Additional Information sections below for more information on\n",
            "telework options.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist, CG-1560-13/14\n",
            "With a score of: 23.78 %\n",
            "Matched JP Description: This position is located in the Division of Complex Institution Supervision and Resolution (CISR), Systemic Risk Branch, Data Analytics Sub-Branch,\n",
            "Data Analytics Section of the Federal Deposit Insurance Corporation. Please see the Clarification from Agency and Additional Information sections below for more information on\n",
            "telework options.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist, CG-1560-13/14\n",
            "With a score of: 29.68 %\n",
            "Matched JP Description: This position is located in the Division of Complex Institution Supervision and Resolution (CISR), Systemic Risk Branch, Data Analytics Sub-Branch,\n",
            "Data Analytics Section of the Federal Deposit Insurance Corporation. Please see the Clarification from Agency and Additional Information sections below for more information on\n",
            "telework options.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 5\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 21.68 %\n",
            "Matched JP Description: As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the\n",
            "body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through\n",
            "the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 34.4 %\n",
            "Matched JP Description: As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the\n",
            "body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through\n",
            "the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 15.31 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 6\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 17.64 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 2.31 %\n",
            "Matched JP Description: Interdisciplinary Position - This is an interdisciplinary position that may be filled as a 0401 - Biologist or 1301-Physical Scientist. Duty Station\n",
            "TBD - The duty station of this position may remain that of the selectee. Salary currently reflect Rest of US pay; however, salary will be determined by the duty location of\n",
            "the selectee.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 18.24 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 7\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 18.24 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 18.24 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 17.92 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 8\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 0.0 %\n",
            "Matched JP Description: As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the\n",
            "body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through\n",
            "the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 17.2 %\n",
            "Matched JP Description: As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the\n",
            "body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through\n",
            "the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.68 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 9\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Mathematical Statistician (Data Scientist) or Statistician (Data Scientist) 12 Month Roster - DH\n",
            "With a score of: 12.3 %\n",
            "Matched JP Description: Positions under this announcement are being filled using a Direct Hire Authority (DHA). Click on \"Learn more about this agency\" button below to view\n",
            "Eligibilities being considered and other IMPORTANT information. WHERE CAN I FIND OUT MORE ABOUT OTHER IRS CAREERS? Visit us on the web at www.jobs.irs.gov #LI-POST\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Mathematical Statistician (Data Scientist) or Statistician (Data Scientist) 12 Month Roster - DH\n",
            "With a score of: 11.77 %\n",
            "Matched JP Description: Positions under this announcement are being filled using a Direct Hire Authority (DHA). Click on \"Learn more about this agency\" button below to view\n",
            "Eligibilities being considered and other IMPORTANT information. WHERE CAN I FIND OUT MORE ABOUT OTHER IRS CAREERS? Visit us on the web at www.jobs.irs.gov #LI-POST\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 12.42 %\n",
            "Matched JP Description: As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the\n",
            "body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through\n",
            "the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 10\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist, CG-1560-13/14\n",
            "With a score of: 15.01 %\n",
            "Matched JP Description: This position is located in the Division of Complex Institution Supervision and Resolution (CISR), Systemic Risk Branch, Data Analytics Sub-Branch,\n",
            "Data Analytics Section of the Federal Deposit Insurance Corporation. Please see the Clarification from Agency and Additional Information sections below for more information on\n",
            "telework options.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 15.16 %\n",
            "Matched JP Description: As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the\n",
            "body worn camera and electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through\n",
            "the transparency, availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 15.25 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 11\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 15.02 %\n",
            "Matched JP Description: This is a Direct Hire Authority (DHA) solicitation utilizing the DHA for Certain Personnel of the DoD Workforce to recruit and appoint qualified\n",
            "candidates to positions in the competitive service. About the Position: Put your mind to work! Do Data Science for complex analytical, mathematical, and statistical research\n",
            "that shapes operational requirements and provide options on how to implement data driven solutions for the Department of Army.\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 19.14 %\n",
            "Matched JP Description: This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction\n",
            "and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and\n",
            "programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.82 %\n",
            "Matched JP Description: This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction\n",
            "and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and\n",
            "programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "---POST PROCESSING ANALYSIS----\n",
            "-------------------------------\n",
            "\n",
            "Average Matching Scores for Skill Extraction Method\n",
            "BERT:  14.46 %\n",
            "CSO :  18.12 %\n",
            "YAKE:  16.88 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Upload"
      ],
      "metadata": {
        "id": "xxp4OHVTXpSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and Drop a Data Scientist resum into Colab \n",
        "\n",
        "Make sure the file is named **resume.pdf**\n",
        "\n",
        "There is a dummy resume located in the `/data` directory.\n",
        "\n",
        "\n",
        "\n",
        "< - - - - - - -"
      ],
      "metadata": {
        "id": "1RCg41ohJkHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPWWB4B9YP5j",
        "outputId": "db892923-91a6-4091-b038-97f5b766aee5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer3 in /usr/local/lib/python3.8/dist-packages (2018.12.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (2.4.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (3.16.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer3.layout import LAParams, LTTextBox\n",
        "from pdfminer3.pdfpage import PDFPage\n",
        "from pdfminer3.pdfinterp import PDFResourceManager\n",
        "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer3.converter import PDFPageAggregator\n",
        "from pdfminer3.converter import TextConverter\n",
        "import io\n",
        "import sys\n",
        "from contextlib import redirect_stdout\n",
        "from io import StringIO "
      ],
      "metadata": {
        "id": "hnaZCNoQK53g"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resource_manager = PDFResourceManager()\n",
        "fake_file_handle = io.StringIO()\n",
        "converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
        "page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "\n",
        "with open('/content/data/resume.pdf', 'rb') as fh:\n",
        "\n",
        "    for page in PDFPage.get_pages(fh,\n",
        "                                  caching=True,\n",
        "                                  check_extractable=True):\n",
        "        page_interpreter.process_page(page)\n",
        "\n",
        "    text = fake_file_handle.getvalue()\n",
        "\n",
        "# close open handles\n",
        "converter.close()\n",
        "fake_file_handle.close()\n",
        "\n",
        "# normalize and extract skills \n",
        "text = normalizeCorpus(text)\n",
        "resume_kw = extractSkills(text)\n",
        "\n",
        "# Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, [resume_kw])\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vsrbN-UXsG8",
        "outputId": "85d83869-d201-43e0-d0f3-0e2bea5e60ce"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist conento                   0.6736\n",
            "naive bayes gradient                     0.2802\n",
            "storm visualization                      0.2706\n",
            "dynamodb                                 0.2613\n",
            "path pagerank algorithms                 0.2317\n",
            "monitoring features                      0.2183\n",
            "churn mobile network                     0.2088\n",
            "tableau pipeline combines                0.2003\n",
            "forecasts based exponential              0.1974\n",
            "3x                                       0.1743\n",
            "juanjosecarin increased                  0.1656\n",
            "matplotlib kaggle competition            0.1646\n",
            "ratio students passing                   0.1619\n",
            "gmms redefining job                      0.1464\n",
            "housing cost fresh                       0.009\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "association rules\n",
            "knowledge discovery\n",
            "communication networks\n",
            "purchasing\n",
            "logistic regression\n",
            "engineers\n",
            "sales\n",
            "neural networks\n",
            "network architecture\n",
            "hadoop\n",
            "big data\n",
            "stochastic processes\n",
            "optimization problems\n",
            "customer services\n",
            "machine learning\n",
            "communication\n",
            "retail price\n",
            "communication systems\n",
            "cloud services\n",
            "radio\n",
            "social sciences\n",
            "mobile networks\n",
            "naive bayes classifiers\n",
            "radar\n",
            "mobile terminal\n",
            "search process\n",
            "hbase\n",
            "visualization tools\n",
            "engineering\n",
            "business process\n",
            "shortest path\n",
            "sensors\n",
            "data mining\n",
            "hdfs\n",
            "naive bayes\n",
            "wireless communications\n",
            "visualization\n",
            "anomaly detection\n",
            "facial images\n",
            "map-reduce\n",
            "education\n",
            "university\n",
            "optimization\n",
            "logistics\n",
            "radio transmission\n",
            "stochastic\n",
            "data visualization\n",
            "random forests\n",
            "motion detection\n",
            "radio communication\n",
            "wireless communication system\n",
            "data warehouses\n",
            "correlation analysis\n",
            "etl\n",
            "bayesian methods\n",
            "numerical methods\n",
            "potential customers\n",
            "purchase\n",
            "anomaly detection methods\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "Scientist Professional Profile           0.0011802553461450445\n",
            "Professional Profile Mountain            0.0016637299682911364\n",
            "Data Scientist Professional              0.003604770059932564\n",
            "Professional Profile                     0.00494610788030988\n",
            "Optimization Techniques GPA              0.0050258570731319605\n",
            "Telecommunication Engineering GPA        0.00631574607545285\n",
            "Data Data Analysis                       0.006775670181383585\n",
            "Scientist CONENTO Madrid                 0.007161255429548713\n",
            "Profile Mountain View                    0.007909877197576914\n",
            "Monte Carlo Techniques                   0.00799724118050148\n",
            "Data Scientist CONENTO                   0.008080363264430963\n",
            "Carin Data Scientist                     0.008853177270497462\n",
            "Data                                     0.009370210232043024\n",
            "Statistical Learning Relevant            0.009430624645991715\n",
            "Analyzing Data Data                      0.009918950671507304\n",
            "Jose Carin Data                          0.012476023793362614\n",
            "Data Scientist                           0.012521912486879255\n",
            "Universidad Politécnica                  0.012972695088099381\n",
            "Experience DATA SCIENCE                  0.013287894514760605\n",
            "data science                             0.014963748692963503\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 1\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: OT/ICS Systems Engineer\n",
            "With a score of: 10.5 %\n",
            "Matched JP Description: What We Are Doing:  Forescout Technologies is the leader in device visibility and control. Our unified security platform enables enterprises and\n",
            "government agencies to gain complete situational awareness of their extended enterprise environments and orchestrate actions to reduce cyber and operational risk. Our\n",
            "technology works with disparate security tools to help accelerate incident response, break down silos, automate workflows and optimize existing investments. We are a company\n",
            "filled with people who are cyber obsessed and passionate about our products. We are all about inclusion and diversity! We work as a team, and everyone matters! Join us as we\n",
            "secure the world with our products.  What you will do:  Are you a cybersecurity or IT pre-sales expert? The OT sales team is looking for an out-going and experienced pre-sales\n",
            "consultant with experience within cybersecurity, industrial control systems (ICS) and/or enterprise-grade IT project deployments. For this role, you would be working closely\n",
            "with customers and partners and must have knowledge of SCADA, DCS, MES and other control systems (ICS) technology. If you love problem-solving, finding efficient solutions for\n",
            "technical issues for geo-distributed customers and are comfortable working with a small dynamic team, then this job is for you!  What You Bring to Forescout:    10+ years of\n",
            "relevant work experience Support account managers to demonstrate the capabilities of our products to customers Ability to manage and drive complex solution integrations with\n",
            "industrial and enterprise customersStrong understanding of the Purdue Enterprise Reference Architecture within one or more Critical Infrastructure vertical businesses (i.e.\n",
            "Utility/energy, process automation, discrete automation, transportation, etc.) Strong knowledge of OT/ICS fieldStrong IT & Security skill-sets and knowledge including but not\n",
            "limited to:Risk Management (Operational & Cybersecurity)Secure Network ArchitectureCommon IT (i.e. DNS, HTTP, SMB, etc.) and OT protocols (Modbus, DNP3, BACnet etc.)Ability to\n",
            "read and write in a common scripting language such as Bash, PowerShell, Lua, etc.Willingness to travel, expected +50%Excellent written and verbal communications skillsAbility\n",
            "to articulate a message across ALL levels of an organizationCarry out Proof of Value (PoV) design, product delivery, and deployment schedulesPerform network assessments and\n",
            "network communication traffic analysisWork with customers on implementing and conducting detailed network analysisPlan and deliver technical product trainingLove of technology\n",
            "Ability to manage and drive complex solution integrations with industrial and enterprise customers. Self-motivated with the ability to work unattended while meeting tight\n",
            "deadlines. Support account managers to demonstrate the capabilities of our products to customers  What Forescout Offers You:  Competitive compensation and benefits–we cover\n",
            "95% of employee and dependents’ benefits premiums (the U.S. only), 401K match, generous PTO policy, and much moreCollaborative and innovative environment –make an impact on\n",
            "worldwide security while working on the hottest technologyLeadership that supports and encourages professional growth and developmentWant a glimpse of Life @ Forescout? Check\n",
            "us out on Facebook and InstagramLearn more @ www.forescout.com  #LI-SH2\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: OT/ICS Systems Engineer\n",
            "With a score of: 10.09 %\n",
            "Matched JP Description: What We Are Doing:  Forescout Technologies is the leader in device visibility and control. Our unified security platform enables enterprises and\n",
            "government agencies to gain complete situational awareness of their extended enterprise environments and orchestrate actions to reduce cyber and operational risk. Our\n",
            "technology works with disparate security tools to help accelerate incident response, break down silos, automate workflows and optimize existing investments. We are a company\n",
            "filled with people who are cyber obsessed and passionate about our products. We are all about inclusion and diversity! We work as a team, and everyone matters! Join us as we\n",
            "secure the world with our products.  What you will do:  Are you a cybersecurity or IT pre-sales expert? The OT sales team is looking for an out-going and experienced pre-sales\n",
            "consultant with experience within cybersecurity, industrial control systems (ICS) and/or enterprise-grade IT project deployments. For this role, you would be working closely\n",
            "with customers and partners and must have knowledge of SCADA, DCS, MES and other control systems (ICS) technology. If you love problem-solving, finding efficient solutions for\n",
            "technical issues for geo-distributed customers and are comfortable working with a small dynamic team, then this job is for you!  What You Bring to Forescout:    10+ years of\n",
            "relevant work experience Support account managers to demonstrate the capabilities of our products to customers Ability to manage and drive complex solution integrations with\n",
            "industrial and enterprise customersStrong understanding of the Purdue Enterprise Reference Architecture within one or more Critical Infrastructure vertical businesses (i.e.\n",
            "Utility/energy, process automation, discrete automation, transportation, etc.) Strong knowledge of OT/ICS fieldStrong IT & Security skill-sets and knowledge including but not\n",
            "limited to:Risk Management (Operational & Cybersecurity)Secure Network ArchitectureCommon IT (i.e. DNS, HTTP, SMB, etc.) and OT protocols (Modbus, DNP3, BACnet etc.)Ability to\n",
            "read and write in a common scripting language such as Bash, PowerShell, Lua, etc.Willingness to travel, expected +50%Excellent written and verbal communications skillsAbility\n",
            "to articulate a message across ALL levels of an organizationCarry out Proof of Value (PoV) design, product delivery, and deployment schedulesPerform network assessments and\n",
            "network communication traffic analysisWork with customers on implementing and conducting detailed network analysisPlan and deliver technical product trainingLove of technology\n",
            "Ability to manage and drive complex solution integrations with industrial and enterprise customers. Self-motivated with the ability to work unattended while meeting tight\n",
            "deadlines. Support account managers to demonstrate the capabilities of our products to customers  What Forescout Offers You:  Competitive compensation and benefits–we cover\n",
            "95% of employee and dependents’ benefits premiums (the U.S. only), 401K match, generous PTO policy, and much moreCollaborative and innovative environment –make an impact on\n",
            "worldwide security while working on the hottest technologyLeadership that supports and encourages professional growth and developmentWant a glimpse of Life @ Forescout? Check\n",
            "us out on Facebook and InstagramLearn more @ www.forescout.com  #LI-SH2\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Senior Salesforce Developer\n",
            "With a score of: 5.43 %\n",
            "Matched JP Description: Principle Duties & Responsibilities:  Analyze complex systems and troubleshoot and isolate system issues; Understand requirements for business users\n",
            "and translate into design specifications, utilizing thorough understanding of the Salesforce platform, Salesforce products and licensing models; Utilize thorough understanding\n",
            "of application development, project lifecycle, and methodologies and ability to work under tight deadlines and handle multiple detail-oriented tasks; Apply knowledge of\n",
            "Salesforce developmentand customizations, with APEX, Visual Force, API, Force.com and Workflows, taking into account com best practices, support mechanisms, procedures, and\n",
            "limitations, as well as NDR's unique needs; Responsible for Salesforce administration, release management and deployment as well as management of Salesforce.com sandboxes,\n",
            "including their integrations; Design and execute Salesforce.com configuration changes, leveraging the Salesforce interface to sync with internal tracking systems; Design,\n",
            "develop, and maintain integration and synchronization programs; Design the data model, user interface, business logic, and security for custom applications; and Design,\n",
            "develop, and customize software solutions for end users by using analysis and mathematical models to effectively predict and measure the results of the design using Chatter,\n",
            "Communities and other Salesforce applications.  Requirements:  Bachelor of Science degree or foreign equivalent in Information Systems, Computer Science, Computer Engineering,\n",
            "Software Engineering or a related field 3 years of experience with the Salesforce platform, specifically: development with Apex, VisualForce, and Force.com; Design and execute\n",
            "Salesforce.com configuration changes, leveraging the Salesforce interface to sync with internal tracking systems; Salesforce administration, release management, and deployment\n",
            "Salesforce products and licensing models Management of Salesforce.com sandboxes, including their integrations; Chatter, Communities, and other Salesforce apps com best\n",
            "practices, support mechanisms, procedures, and limitations.  What We Offer:  We believe in a team-first culture, full of rewards and recognition for our employees. We are\n",
            "dedicated to our employees' success and growth within the company, through our employee mentorship and leadership programs.  Our extensive benefits package includes:\n",
            "Medical, Dental, and Vision Benefits 401(k) Match Paid Holidays, Volunteer Time Off, Sick Days, and Vacation 10 Weeks Paid Parental Leave Pre-tax Transit Benefits Discounted\n",
            "Gym Membership No-cost Life Insurance Benefits  About National Debt Relief:  National Debt Relief is one of the country's largest and most reputable debt settlement companies.\n",
            "We are made up of energetic, smart, and compassionate individuals who are passionate about helping thousands of Americans with debt relief. Most importantly, we're all about\n",
            "helping our customers through a tough financial time in their lives with education and individual customer service.  We are dedicated to helping individuals and families rid\n",
            "their lives of burdensome debt. We specialize in debt settlement and have negotiated settlements for thousands of creditor and collections accounts. We provide our clients\n",
            "with both our expertise and our proven results. This means helping consumers in their time of hardship to get out of debt with the least possible cost. It can also mean\n",
            "conducting financial consultations, educating the consumer, and recommending the appropriate solution. Our core services offer debt settlement as an alternative to bankruptcy,\n",
            "credit counseling, and debt consolidation. We become our clients' number one advocate to help them reestablish financial stability as quickly as possible.  #ZR\n",
            "\n",
            "---POST PROCESSING ANALYSIS----\n",
            "-------------------------------\n",
            "\n",
            "Average Matching Scores for Skill Extraction Method\n",
            "BERT:  10.5 %\n",
            "CSO :  10.09 %\n",
            "YAKE:  5.43 %\n"
          ]
        }
      ]
    }
  ]
}