{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#README.txt"
      ],
      "metadata": {
        "id": "VqvVC9cM4PPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and drop a ZIPPED version of the folder `/data` from GitHub repo into Colab.\n",
        "\n",
        "< - - - - -"
      ],
      "metadata": {
        "id": "SkYHORUB4Rig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# then run this and refresh directory...\n",
        "\n",
        "# unzip datasets\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxJqWMSD5rS3",
        "outputId": "de7bce4b-1034-4e1d-c33d-ce5c1a2624e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/job-postings/\n",
            "  inflating: data/job-postings/Data_Job_NY.csv  \n",
            "  inflating: data/job-postings/Data_Job_SF.csv  \n",
            "  inflating: data/job-postings/Data_Job_TX.csv  \n",
            "  inflating: data/job-postings/Data_Job_WA.csv  \n",
            "  inflating: data/README.txt         \n",
            "  inflating: data/resume.pdf         \n",
            "   creating: data/resumes/\n",
            "  inflating: data/resumes/kaggleResumes.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# API"
      ],
      "metadata": {
        "id": "anBUXima7kjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v4wViL775dD",
        "outputId": "0dc11e6f-75ae-4c50-8d38-f9fb73cf1402"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: config in /usr/local/lib/python3.8/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import config\n",
        "# API Docs found here: https://developer.usajobs.gov/Tutorials/Search-Jobs"
      ],
      "metadata": {
        "id": "uNcEHO9t7rKr"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.US_JOBS_API_KEY = \"xVc0TZiLfhcr17ci7Ngk6bLAetdRVFgntm2pZgWNtww=\"\n",
        "config.EMAIL_ADDRESS = \"gjacobthomas@gmail.com\""
      ],
      "metadata": {
        "id": "sobm0urbDUff"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I'm assuming this API has some limiter on it so we don't want to lose access. -tyler"
      ],
      "metadata": {
        "id": "89Ei-XMSDhIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# more parameters are found here: https://developer.usajobs.gov/API-Reference/GET-api-Search\n",
        "def queryJobsAPI(keyword, location):\n",
        "  host = 'data.usajobs.gov' \n",
        "  # add these values in the config.py file\n",
        "  userAgent = config.EMAIL_ADDRESS\n",
        "  authKey = config.US_JOBS_API_KEY\n",
        "\n",
        "  base_url = \"https://data.usajobs.gov/api/search\"\n",
        "\n",
        "  parameters = {\n",
        "      \"Keyword\": keyword,\n",
        "      \"LocationName\": location\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "      \"Host\": host,          \n",
        "      \"User-Agent\": userAgent,          \n",
        "      \"Authorization-Key\": authKey  \n",
        "  }\n",
        "\n",
        "  resp = requests.request(\"GET\", base_url,headers=headers, params=parameters)\n",
        "  result = resp.json()['SearchResult']['SearchResultItems']\n",
        "\n",
        "  # get Job Title \n",
        "  print(result[1]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "  # get Job Summary\n",
        "  print(result[1]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "SVoxo2K47j_l"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# KeyBERT Extraction Function"
      ],
      "metadata": {
        "id": "GVU9pbVd3-hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "-RUKzBOv5-_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5f595f-54bd-47a0-aaa6-9c4452932c08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
            "Collecting sentence-transformers>=0.3.8\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.21.6)\n",
            "Collecting rich>=10.4.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (4.1.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.13.1+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 67.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.9.24)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Building wheels for collected packages: keybert, sentence-transformers\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23800 sha256=7d03e4ee3a19d7307c78831aa1b1ed4b15cd46b88f3f8772a7fa19ca7d1b2208\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/bc/8b/a51bee77aec33895e6c8c236144b4cc10875659c4d2c80f070\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=356242bd0129c9050436fb16cce045bac8fc828626bb85f0959fa9d37eae7279\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built keybert sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, commonmark, sentence-transformers, rich, keybert\n",
            "Successfully installed commonmark-0.9.1 huggingface-hub-0.11.1 keybert-0.7.0 rich-12.6.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "HIogy6WO3hvX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from keybert import KeyBERT # pip install keybert (give it a minute...)\n",
        "from statistics import mean\n",
        "\n",
        "'''\n",
        "/*---------------------------------------------------------------------\n",
        " |  Method: extractKeywordsBERT\n",
        " |\n",
        " |  Purpose: Uses the KeyBert Keyword Extraction Tool to extract\n",
        " |           and return keywords from a given corpus. \n",
        " |      \n",
        " |  Author: Tyler Parks\n",
        " |  Created On: 10/30/22\n",
        " |\n",
        " |  Parameters:\n",
        " |      normalized_corpus -- A single string containing all text of the\n",
        " |                           normalized corpus.\n",
        " |\n",
        " |  Returns: \n",
        " |      keywords -- List of collected keywords\n",
        " |      scores -- List of those keyword's scores\n",
        " |\n",
        " |  References: https://maartengr.github.io/KeyBERT/#usage\n",
        " |\n",
        " *-------------------------------------------------------------------*/\n",
        "''' \n",
        "def extractKeywordsBERT(normalized_corpus):   \n",
        "    print('---KeyBert Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    # init. language model \n",
        "    language_model = KeyBERT(model = 'all-mpnet-base-v2')\n",
        "\n",
        "    # extract those keywords!\n",
        "    data = language_model.extract_keywords( normalized_corpus, \n",
        "                                            keyphrase_ngram_range=(1, 3), \n",
        "                                            stop_words='english',\n",
        "                                            use_maxsum=False, \n",
        "                                            use_mmr=True,\n",
        "                                            diversity=0.7,\n",
        "                                            nr_candidates=20, \n",
        "                                            top_n=15\n",
        "                                        )\n",
        "\n",
        "    # zip the lists\n",
        "    zipped = list(map(list, zip(*data)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "    print('-Skill-'.ljust(40), '-Score-')\n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "    print()\n",
        "\n",
        "    # print(\"type: \"+str(type(scores))+\"length: \"+ str(len(scores)))\n",
        "\n",
        "    avg = mean(scores[:14])\n",
        "    print(\"Score Average: \" + str(avg))\n",
        "\n",
        "    return keywords, scores, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# CSO-Classifier Extraction Function"
      ],
      "metadata": {
        "id": "vmHHZLv57ex9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install cso-classifier"
      ],
      "metadata": {
        "id": "CXlOaYQj7jDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't restart runtime if the terminal says so! Keep going.**"
      ],
      "metadata": {
        "id": "A0Ga47IZ8cH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# update spacy\n",
        "!pip install -U spacy\n",
        "import spacy\n",
        "from cso_classifier import CSOClassifier      # import classifier tool"
      ],
      "metadata": {
        "id": "Ip21zn4U8Rxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update the most recent model\n",
        "CSOClassifier.update() \n",
        "\n",
        "# define the model object\n",
        "CSO_Extractor = CSOClassifier(modules = \"both\", enhancement = \"first\", explanation = False)"
      ],
      "metadata": {
        "id": "xLaIVJli8qOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsCSO(normalized_corpus):\n",
        "\n",
        "  # run the extraction\n",
        "  result = CSO_Extractor.run(normalized_corpus)\n",
        "\n",
        "  print('\\n-----CSO Extraction-----')\n",
        "  print('------------------------\\n')\n",
        "  \n",
        "  for keyword in result['union']:\n",
        "    print(keyword)\n",
        "\n",
        "  return result['union']"
      ],
      "metadata": {
        "id": "Cctwpsvv86iZ"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YAKE Extractor**"
      ],
      "metadata": {
        "id": "-4h4ZTbXu9CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install yake\n",
        "import yake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNiHvEtdvF0G",
        "outputId": "138fb142-971d-4045-dd13-933014df405a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yake in /usr/local/lib/python3.8/dist-packages (0.4.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from yake) (1.21.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from yake) (0.8.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from yake) (2.6.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.8/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.8/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.8/dist-packages (from yake) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from segtok->yake) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsYAKE(normalized_corpus):\n",
        "    print()\n",
        "    print('\\n---YAKE Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    language = \"en\"\n",
        "    max_ngram_size = 3\n",
        "    deduplication_threshold = 0.9\n",
        "    deduplication_algo = 'seqm'\n",
        "    windowSize = 1\n",
        "    numOfKeywords = 20\n",
        "\n",
        "    kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
        "    keywords = kw_extractor.extract_keywords(normalized_corpus)\n",
        "\n",
        "    zipped = list(map(list, zip(*keywords)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "    print('-Skill-'.ljust(40), '-Score-')\n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "\n",
        "    avg = mean(scores[:19])\n",
        "    print(\"Score Average: \" + str(avg)+ \"\\n\")\n",
        "    \n",
        "    return keywords, scores, avg"
      ],
      "metadata": {
        "id": "JGGvwVeYvaRv"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TextRank Extractor**"
      ],
      "metadata": {
        "id": "kaDH9wIYaOCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install summa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJoqryZ5aPym",
        "outputId": "c9bef9ea-459f-4440-ad49-b304d82479a7"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.8/dist-packages (from summa) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy>=0.19->summa) (1.21.6)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=1c58eef039ccb6b6597897017e208866675a77947119c97e3ae3506efb821047\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/6a/dd/209eb19d5f2266b9cfd06827539bf70435b0ad5fe8244e52d3\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summa import keywords"
      ],
      "metadata": {
        "id": "dEj9gBdYaT3L"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsTextRank(normalized_corpus):\n",
        "  print('\\n')\n",
        "  print('---TextRank Extraction---')\n",
        "  print('------------------------\\n')\n",
        "\n",
        "  #extract\n",
        "  TR_keywords = keywords.keywords(normalized_corpus, scores=True)\n",
        "\n",
        "  #zip into list\n",
        "  zipped = list(map(list, zip(*TR_keywords)))\n",
        "  TR_keywords = zipped[0]\n",
        "  scores = zipped[1]\n",
        "  \n",
        "  #print(TR_keywords[0:20])\n",
        "  print('-Skill-'.ljust(40), '-Score-')\n",
        "  for i, value in enumerate(TR_keywords[0:19]):\n",
        "    print(value.ljust(40), scores[i])\n",
        "    \n",
        "  avg = mean(scores[:19])\n",
        "  print(\"\\nScore Average: \" + str(avg))\n",
        "  \n",
        "  return TR_keywords, scores, avg"
      ],
      "metadata": {
        "id": "_fhSFVa1aQ6A"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Skill Matching**"
      ],
      "metadata": {
        "id": "Oao0RKq6aDyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install jieba\n",
        "\n",
        "from functools import reduce\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import jieba\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJF-rEVU3ZOy",
        "outputId": "9c0dd078-8ca8-47ff-c14f-b44f3b3b0d12"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.8.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (0.42.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=175)  # wrapper options"
      ],
      "metadata": {
        "id": "aXYDvHXw10kL"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkillsMatching:\n",
        "  def __init__(self, all_job_postings_keywords, all_jp_titles, all_resumes_keywords):\n",
        "      self.all_job_postings_keywords = all_job_postings_keywords\n",
        "      self.all_jp_titles = all_jp_titles\n",
        "      self.all_resumes_keywords = all_resumes_keywords\n",
        "      \n",
        "\n",
        "  # output JSON with matching job postings for each resume\n",
        "  def skill_match(self):\n",
        "      results = []\n",
        "      \n",
        "      for resume_json in self.all_resumes_keywords:\n",
        "          resume_arr = []\n",
        "          for key in resume_json:\n",
        "            # loop through the keys = every classifier \n",
        "            classifier_json = {key: {\"matching_job\": \"\",\n",
        "                      \"job_title\": \"\"}}\n",
        "            resume_keywords = resume_json[key]\n",
        "            scores = self.sim_score(self.all_job_postings_keywords, resume_keywords)\n",
        "\n",
        "            # grab largest score\n",
        "            largestScore = max(scores)\n",
        "\n",
        "            # grab the index of the largest score in the arr\n",
        "            max_idx = np.argmax(scores)\n",
        "\n",
        "            classifier_json[key]['matching_job'] = self.all_job_postings_keywords[max_idx]\n",
        "            classifier_json[key]['job_title'] = self.all_jp_titles[max_idx]\n",
        "            classifier_json[key]['score'] = largestScore\n",
        "\n",
        "            resume_arr.append(classifier_json)\n",
        "          results.append(resume_arr)\n",
        "      return results\n",
        "\n",
        "\n",
        "  def split_and_join_arr(self, arr):\n",
        "    new_arr = []\n",
        "    for w in arr:\n",
        "      word_arr = re.split('\\W+', w.lower())\n",
        "      new_arr = new_arr + word_arr\n",
        "    # print(new_arr)\n",
        "    return new_arr\n",
        "  \n",
        "  def sim_score(self, docs, keywords):\n",
        "    keywords = self.split_and_join_arr(keywords)\n",
        "    gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in docs]\n",
        "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "    tf_idf = gensim.models.TfidfModel(corpus)\n",
        "    sims = gensim.similarities.Similarity('/usr/workdir',tf_idf[corpus],\n",
        "                                      num_features=len(dictionary))\n",
        "\n",
        "    query_doc_bow = dictionary.doc2bow(keywords)\n",
        "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "    # print(sims[query_doc_tf_idf])\n",
        "    return sims[query_doc_tf_idf]\n",
        "\n",
        "  def print_results(self, match_job_postings):\n",
        "\n",
        "    # init average scores\n",
        "    avgScoreBERT = 0\n",
        "    avgScoreCSO = 0\n",
        "    avgScoreYAKE = 0\n",
        "\n",
        "    # for each resume\n",
        "    for i, resume in enumerate(match_job_postings):\n",
        "      print('\\n------------------------')\n",
        "      print(\"Matching Job Posting for Resume %s\" % (i + 1))\n",
        "      for classifier_obj in resume:\n",
        "        classifier_type = list(classifier_obj.keys())[0]\n",
        "\n",
        "        # if BERT\n",
        "        if classifier_type == 'keywordsBERT':\n",
        "          avgScoreBERT += round(classifier_obj[classifier_type]['score'] * 100, 2)\n",
        "\n",
        "        # else if CSO\n",
        "        elif classifier_type == 'keywordsCSO':\n",
        "          avgScoreCSO += round(classifier_obj[classifier_type]['score'] * 100, 2)\n",
        "\n",
        "        # else YAKE\n",
        "        else:\n",
        "          avgScoreYAKE += round(classifier_obj[classifier_type]['score'] * 100, 2)\n",
        "\n",
        "        print()\n",
        "        print('Skill Extracted Using:', classifier_type)\n",
        "        print('Matched JP Title:', classifier_obj[classifier_type]['job_title'])\n",
        "        print('With a score of:', round(classifier_obj[classifier_type]['score'] * 100, 2), '%')\n",
        "\n",
        "        # Wrap this text.\n",
        "        word_list = wrapper.wrap(text = \n",
        "              'Matched JP Description: ' + classifier_obj[classifier_type]['matching_job'])\n",
        "\n",
        "        # Print each line.\n",
        "        for element in word_list:\n",
        "            print(element)\n",
        "\n",
        "    avgScoreBERT /= len(match_job_postings)\n",
        "    avgScoreCSO /= len(match_job_postings)\n",
        "    avgScoreYAKE /= len(match_job_postings)\n",
        "\n",
        "    print()\n",
        "    print('---POST PROCESSING ANALYSIS----')\n",
        "    print('-------------------------------')\n",
        "    print()\n",
        "    print('Average Matching Scores for Skill Extraction Method')\n",
        "    print('BERT: ', round(avgScoreBERT, 2), '%')\n",
        "    print('CSO : ', round(avgScoreCSO, 2), '%')\n",
        "    print('YAKE: ', round(avgScoreYAKE, 2), '%')"
      ],
      "metadata": {
        "id": "F9IBrSshaKbM"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Driver Code"
      ],
      "metadata": {
        "id": "FIi95ge84CGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iGMXmWeahMX",
        "outputId": "c836f83a-855d-4156-ae6e-47e41cc19766"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 55.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd     # pip install pandas. usage: loading data from csv files into dataframes\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import contractions\n",
        "import spacy\n",
        "from numpy.lib.npyio import savez_compressed\n",
        "from array import *\n",
        "nltk.download(\"all\")\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "YpAjFMcT6H3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e14e55-286c-4197-e987-1cdf60d6ad14"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Helper Functions\n",
        "\n",
        "# Function to retrieve text data.\n",
        "# (either 1 or more job postings or resumes)?\n",
        "def getFileData(filename, dir):\n",
        "    return pd.read_csv('data/' + dir + '/' + filename)\n",
        "\n",
        "# Function to normalize text data. \n",
        "# (some skill extraction tools will normalize text for us; however, if not, this function is here)\n",
        "# includes removing stopwords, punctuation, dates, links, etc...\n",
        "def normalizeCorpus(corpus):\n",
        "\n",
        "    # import nltk for stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # convert to lower case\n",
        "    lower_corpus = corpus.lower()\n",
        "\n",
        "    # remove numbers\n",
        "    no_number_corpus = re.sub(r'\\d+','',lower_corpus)\n",
        "\n",
        "    # remove all punctuation except words and space\n",
        "    no_punc_corpus = re.sub(r'[^\\w\\s]','', no_number_corpus)\n",
        "\n",
        "    # remove white spaces\n",
        "    no_wspace_corpus = no_punc_corpus.strip()\n",
        "    no_wspace_corpus\n",
        "\n",
        "    # convert string to list of words\n",
        "    lst_corpus = [no_wspace_corpus][0].split()\n",
        "    print(lst_corpus)\n",
        "\n",
        "    # remove stopwords\n",
        "    no_stpwords_corpus=\"\"\n",
        "    for i in lst_corpus:\n",
        "\t    if not i in stop_words:\n",
        "\t      no_stpwords_corpus += i+' '\n",
        "\t\t\n",
        "    # removing last space\n",
        "    no_stpwords_corpus = no_stpwords_corpus[:-1]\n",
        "\n",
        "    # output\n",
        "    print(no_stpwords_corpus)\n",
        "\n",
        "    return no_stpwords_corpus\n",
        "\n",
        "    # need to fix this \n",
        "    # nltk_tokenList = word_tokenize(corpus)\n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # nltk_lemmaList = []\n",
        "    # for word in nltk_tokenList:\n",
        "    #     nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # normalized_corpus = []  \n",
        "    # nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "    # for w in nltk_lemmaList:  \n",
        "    #     if w not in nltk_stop_words:  \n",
        "    #         normalized_corpus.append(w)\n",
        "\n",
        "    # punctuation = \";:.,?!\"\n",
        "    # for word in normalized_corpus:\n",
        "    #     if word in punctuation:\n",
        "    #         normalized_corpus.remove(word)\n",
        "\n",
        "    #still need to add dates and links\n",
        "    # return normalized_corpus\n",
        "\n",
        "def similiartychecker(tokenA, tokenB):\n",
        "\n",
        "    A = nlp(tokenA)\n",
        "    B = nlp(tokenB)\n",
        "  \n",
        "    score = A.similarity(B)\n",
        "  \n",
        "    return score\n",
        "\n",
        "# Function to extract skill words from a given corpus.\n",
        "# ideally, this function will output a set of skills extracted from the corpus\n",
        "def extractSkills(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT, avgBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    keywordsYAKE, scoresYAKE, avgYAKE = extractKeywordsYAKE(corpus)\n",
        "\n",
        "    # extraction method 3\n",
        "    keywordsTextRank, scoresTextRank, avgTextRank = extractKeywordsTextRank(corpus)\n",
        "\n",
        "    # return keywordsBERT, keywordsYAKE[0], keywordsTextRank[0]\n",
        "    # return keywordsBERT, keywordsCSO, keywordsYAKE, keywordsTextRank,\n",
        "    return {\"keywordsBERT\": keywordsBERT,\n",
        "            \"keywordsCSO\": keywordsCSO,\n",
        "            \"keywordsYAKE\": keywordsYAKE}\n",
        "            # \"keywordsTextRAnk\": keywordsTextRank}\n",
        "\n",
        "def extractSkillsforcomp(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT, avgBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    keywordsYAKE, scoresYAKE, avgYAKE = extractKeywordsYAKE(corpus)\n",
        "\n",
        "    # extraction method 3\n",
        "    keywordsTextRank, scoresTextRank, avgTextRank = extractKeywordsTextRank(corpus)\n",
        "\n",
        "    # return keywordsBERT, keywordsYAKE[0], keywordsTextRank[0]\n",
        "    return keywordsBERT, keywordsCSO, keywordsYAKE, keywordsTextRank\n",
        "\n",
        "### Driver Code\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # fetch the data\n",
        "    jpCorpus = []\n",
        "    jpTitles = []\n",
        "\n",
        "    # --- FETCH MORE DATA VIA API ---\n",
        "    job_posting_obj = queryJobsAPI(\"Data Scientist\", \"Washington, DC\")\n",
        "    for i in range(len(job_posting_obj)):\n",
        "      jpCorpus.append(job_posting_obj[i]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "      jpTitles.append(job_posting_obj[i]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "\n",
        "    # --- FETCH DATA FROM /CONTENT/DATA/..\n",
        "\n",
        "\n",
        "    # collect all Job Postings\n",
        "    dfNY = getFileData('Data_Job_NY.csv', 'job-postings')\n",
        "    dfSF = getFileData('Data_Job_SF.csv', 'job-postings')\n",
        "    dfTX = getFileData('Data_Job_TX.csv', 'job-postings')\n",
        "    dfWA = getFileData('Data_Job_WA.csv', 'job-postings')\n",
        "\n",
        "    frames = [dfNY, dfSF, dfTX, dfWA] # build frame\n",
        "    job_posting_dataframe = pd.concat(frames) # concat frames\n",
        "    job_posting_dataframe = shuffle(job_posting_dataframe)  # shuffle df\n",
        "\n",
        "    resume_dataframe      = getFileData('kaggleResumes.csv', 'resumes')\n",
        "\n",
        "    jpCorpus += list(job_posting_dataframe['Job_Desc'])\n",
        "    jpTitles += list(job_posting_dataframe['Job_title'])\n",
        "    #---------------\n",
        "\n",
        "    \"\"\"\n",
        "    # print the dataframes\n",
        "    #print('DataFrame of Job Postings:')\n",
        "    #print(job_posting_dataframe)    \n",
        "    #print()\n",
        "\n",
        "    #print('DataFrame of Resumes:')\n",
        "    #print(resume_dataframe)\n",
        "    #print()\n",
        "    #----------------------\n",
        "    \"\"\"\n",
        "\n",
        "    # fetch resumes by themselves\n",
        "    rCorpus  = list(resume_dataframe['Resume'])\n",
        "    #----------------------\n",
        "\n",
        "    # Number of resume samples to view\n",
        "    NUM_SAMPLES = 1\n",
        "    # number of job postings to view\n",
        "    NUM_JPS = 1\n",
        "\n",
        "    print('\\nCurrently Avaliable Job Postings: ', len(jpCorpus))\n",
        "    print()\n",
        "\n",
        "    # for each JOB POSTING from the corpus\n",
        "    i = 0\n",
        "    all_job_postings = []\n",
        "    for posting in jpCorpus:\n",
        "          \n",
        "        # Wrap this text.\n",
        "        word_list = wrapper.wrap(text=posting)\n",
        "    \n",
        "        print('Job Posting #', i+1)\n",
        "\n",
        "        # Print each line.\n",
        "        for element in word_list:\n",
        "            print(element)    \n",
        "\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(posting)\n",
        "        job_posting_json = extractSkills(text)\n",
        "        all_job_postings.append(text)\n",
        "\n",
        "        #for similiarity referencer:\n",
        "        jobKeyBERTsimComp, jobCSOsimComp, jobYAKEsimComp, jobTextRanksimComp = extractSkillsforcomp(text)\n",
        "\n",
        "        # print lines, we are done with this posting\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X postings\n",
        "        if i > NUM_JPS - 1:\n",
        "          break\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    #---------------------- \n",
        "\n",
        "    # for each RESUME from the corpus\n",
        "    i = 0\n",
        "    all_resumes_keywords = []\n",
        "    for resume in rCorpus:\n",
        "\n",
        "        # Wrap this text.\n",
        "        word_list = wrapper.wrap(text = resume)\n",
        "    \n",
        "        print('Resume #', i+1)\n",
        "\n",
        "        # Print each line.\n",
        "        for element in word_list:\n",
        "            print(element)    \n",
        "\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(resume)\n",
        "        resume_json = extractSkills(text)\n",
        "        all_resumes_keywords.append(resume_json)\n",
        "\n",
        "        #for similiarity referencer:\n",
        "        resKeyBERTsimComp, resCSOsimComp, resYAKEsimComp, resTextRanksimComp = extractSkillsforcomp(text)\n",
        "\n",
        "        # print lines, we are done with this resume\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X resumes\n",
        "        if i > NUM_SAMPLES - 1:\n",
        "            break\n",
        "\n",
        "        i += 1\n",
        "    #---------------------- \n",
        "\n",
        "    '''\n",
        "    ----------------------------------------------------------------------|\n",
        "    |coss reference similiary of resume and job posting extracted keywords|\n",
        "    |-addtion by Tyrell Richardson                                        |\n",
        "    |                                                                     |\n",
        "    |---------------------------------------------------------------------|\n",
        "    '''\n",
        "\n",
        "    keyBERTSimiliarity = np.zeros(14)\n",
        "\n",
        "    for k in range (0,14):\n",
        "        keyBERTSimiliarity[k] = similiartychecker(jobKeyBERTsimComp[k], resKeyBERTsimComp[k])\n",
        "\n",
        "    print('KeyBERT match Similarity Mean: '+str(np.mean(keyBERTSimiliarity)))\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    CSOSimilarity = np.zeros(len(min(jobCSOsimComp, resCSOsimComp)))\n",
        "\n",
        "    for h in range (0,3):\n",
        "      CSOSimilarity[h] = similiartychecker(jobCSOsimComp[h],resCSOsimComp[h])\n",
        "\n",
        "    print('CSO match Similiarity Mean: '+str(np.mean(CSOSimilarity)))\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    YakeSimilarity = np.zeros(len(min(jobYAKEsimComp, resYAKEsimComp)))\n",
        "    for i in range (0,19):\n",
        "      YakeSimilarity[i] = similiartychecker(jobYAKEsimComp[i],resYAKEsimComp[i])\n",
        "\n",
        "    print('Yake match Similiarity Mean: '+str(np.mean(YakeSimilarity)))\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    TextRankSimiliarity = np.zeros(5)\n",
        "    for j in range (0, 5):\n",
        "      TextRankSimiliarity[j] = similiartychecker(jobTextRanksimComp[j], resTextRanksimComp[j])\n",
        "\n",
        "    print('TextRank match Similarity Mean: '+str(np.mean(TextRankSimiliarity)))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# keep going!\n",
        "\n",
        "# end of driver code\n",
        "#---------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6HTo9qA4Fxs",
        "outputId": "74ff58bf-8618-4e4f-9a35-59e3ffc858a0"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Scientist\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "Currently Avaliable Job Postings:  3349\n",
            "\n",
            "Job Posting # 1\n",
            "As a Data Scientist, you will provide data management and analytical support to the Technology Branch Office, Office of Public Trust, specific to the body worn camera and\n",
            "electronic records management systems. This office was created to strengthen public trust in the National Park Service's law enforcement programs through the transparency,\n",
            "availability, and accessibility of information. This is a fully remote position.\n",
            "\n",
            "['as', 'a', 'data', 'scientist', 'you', 'will', 'provide', 'data', 'management', 'and', 'analytical', 'support', 'to', 'the', 'technology', 'branch', 'office', 'office', 'of', 'public', 'trust', 'specific', 'to', 'the', 'body', 'worn', 'camera', 'and', 'electronic', 'records', 'management', 'systems', 'this', 'office', 'was', 'created', 'to', 'strengthen', 'public', 'trust', 'in', 'the', 'national', 'park', 'services', 'law', 'enforcement', 'programs', 'through', 'the', 'transparency', 'availability', 'and', 'accessibility', 'of', 'information', 'this', 'is', 'a', 'fully', 'remote', 'position']\n",
            "data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote position\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "law enforcement                          0.5326\n",
            "provide data management                  0.4489\n",
            "office public trust                      0.4378\n",
            "technology branch office                 0.3831\n",
            "scientist provide                        0.334\n",
            "trust national park                      0.3194\n",
            "transparency availability accessibility  0.2718\n",
            "records                                  0.2425\n",
            "electronic                               0.2121\n",
            "fully remote position                    0.1861\n",
            "created                                  0.1774\n",
            "specific body                            0.1719\n",
            "worn camera                              0.1449\n",
            "strengthen                               0.0879\n",
            "fully                                    0.0779\n",
            "\n",
            "Score Average: 0.2821714285714286\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "information management\n",
            "records management\n",
            "data management\n",
            "electronic records\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "fully remote position                    0.0019142982031786582\n",
            "analytical support technology            0.002873572627569998\n",
            "support technology branch                0.002873572627569998\n",
            "specific body worn                       0.002873572627569998\n",
            "body worn camera                         0.002873572627569998\n",
            "worn camera electronic                   0.002873572627569998\n",
            "camera electronic records                0.002873572627569998\n",
            "national park services                   0.002873572627569998\n",
            "park services law                        0.002873572627569998\n",
            "services law enforcement                 0.002873572627569998\n",
            "law enforcement programs                 0.002873572627569998\n",
            "enforcement programs transparency        0.002873572627569998\n",
            "programs transparency availability       0.002873572627569998\n",
            "transparency availability accessibility  0.002873572627569998\n",
            "availability accessibility information   0.002873572627569998\n",
            "accessibility information fully          0.002873572627569998\n",
            "information fully remote                 0.002873572627569998\n",
            "trust specific body                      0.0029865057316926695\n",
            "created strengthen public                0.0029865057316926695\n",
            "trust national park                      0.0029865057316926695\n",
            "Score Average: 0.0028349721951412615\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "management                               0.26937774962671174\n",
            "office                                   0.23981126332883315\n",
            "trust                                    0.213847813620145\n",
            "data                                     0.21332363295542517\n",
            "fully remote                             0.20168549278175896\n",
            "\n",
            "Score Average: 0.2276091904625748\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "law enforcement                          0.5326\n",
            "provide data management                  0.4489\n",
            "office public trust                      0.4378\n",
            "technology branch office                 0.3831\n",
            "scientist provide                        0.334\n",
            "trust national park                      0.3194\n",
            "transparency availability accessibility  0.2718\n",
            "records                                  0.2425\n",
            "electronic                               0.2121\n",
            "fully remote position                    0.1861\n",
            "created                                  0.1774\n",
            "specific body                            0.1719\n",
            "worn camera                              0.1449\n",
            "strengthen                               0.0879\n",
            "fully                                    0.0779\n",
            "\n",
            "Score Average: 0.2821714285714286\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "information management\n",
            "records management\n",
            "data management\n",
            "electronic records\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "fully remote position                    0.0019142982031786582\n",
            "analytical support technology            0.002873572627569998\n",
            "support technology branch                0.002873572627569998\n",
            "specific body worn                       0.002873572627569998\n",
            "body worn camera                         0.002873572627569998\n",
            "worn camera electronic                   0.002873572627569998\n",
            "camera electronic records                0.002873572627569998\n",
            "national park services                   0.002873572627569998\n",
            "park services law                        0.002873572627569998\n",
            "services law enforcement                 0.002873572627569998\n",
            "law enforcement programs                 0.002873572627569998\n",
            "enforcement programs transparency        0.002873572627569998\n",
            "programs transparency availability       0.002873572627569998\n",
            "transparency availability accessibility  0.002873572627569998\n",
            "availability accessibility information   0.002873572627569998\n",
            "accessibility information fully          0.002873572627569998\n",
            "information fully remote                 0.002873572627569998\n",
            "trust specific body                      0.0029865057316926695\n",
            "created strengthen public                0.0029865057316926695\n",
            "trust national park                      0.0029865057316926695\n",
            "Score Average: 0.0028349721951412615\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "management                               0.26937774962671174\n",
            "office                                   0.23981126332883315\n",
            "trust                                    0.213847813620145\n",
            "data                                     0.21332363295542517\n",
            "fully remote                             0.20168549278175896\n",
            "\n",
            "Score Average: 0.2276091904625748\n",
            "------------------------\n",
            "\n",
            "Job Posting # 2\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics\n",
            "(OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms\n",
            "necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "\n",
            "['this', 'position', 'is', 'located', 'in', 'the', 'department', 'of', 'health', 'human', 'services', 'hhs', 'centers', 'for', 'medicare', 'medicaid', 'services', 'cms', 'office', 'of', 'burden', 'reduction', 'and', 'health', 'informatics', 'obrhi', 'emerging', 'innovations', 'group', 'as', 'a', 'data', 'scientist', 'gs', 'you', 'will', 'design', 'develop', 'and', 'implement', 'the', 'analytical', 'statistical', 'and', 'programming', 'mechanisms', 'necessary', 'to', 'collect', 'organize', 'analyze', 'and', 'interpret', 'unique', 'and', 'highly', 'specialized', 'data', 'sets']\n",
            "position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly specialized data sets\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "health informatics                       0.6339\n",
            "analytical statistical programming       0.4036\n",
            "group data                               0.3918\n",
            "medicaid                                 0.3757\n",
            "position located department              0.3249\n",
            "emerging innovations                     0.2476\n",
            "analyze interpret                        0.2452\n",
            "gs design develop                        0.2427\n",
            "cms office burden                        0.2234\n",
            "reduction                                0.1954\n",
            "human                                    0.146\n",
            "centers                                  0.1354\n",
            "unique highly specialized                0.1341\n",
            "obrhi emerging                           0.1083\n",
            "mechanisms necessary                     0.0224\n",
            "\n",
            "Score Average: 0.272\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "service delivery\n",
            "target position\n",
            "integrated data\n",
            "correlation analysis\n",
            "programming languages\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "position located department              0.003683438125747622\n",
            "specialized data sets                    0.005385420690931887\n",
            "hhs centers medicare                     0.006172028049662181\n",
            "centers medicare medicaid                0.006172028049662181\n",
            "cms office burden                        0.006172028049662181\n",
            "office burden reduction                  0.006172028049662181\n",
            "informatics obrhi emerging               0.006172028049662181\n",
            "obrhi emerging innovations               0.006172028049662181\n",
            "emerging innovations group               0.006172028049662181\n",
            "design develop implement                 0.006172028049662181\n",
            "develop implement analytical             0.006172028049662181\n",
            "implement analytical statistical         0.006172028049662181\n",
            "analytical statistical programming       0.006172028049662181\n",
            "statistical programming mechanisms       0.006172028049662181\n",
            "collect organize analyze                 0.006172028049662181\n",
            "organize analyze interpret               0.006172028049662181\n",
            "analyze interpret unique                 0.006172028049662181\n",
            "interpret unique highly                  0.006172028049662181\n",
            "unique highly specialized                0.006172028049662181\n",
            "located department health                0.009062863878918739\n",
            "Score Average: 0.005999649245312452\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.28584294577822333\n",
            "health                                   0.2692405194482652\n",
            "services                                 0.26191487903200117\n",
            "located                                  0.1707514088341824\n",
            "programming mechanisms necessary         0.15780425521312472\n",
            "\n",
            "Score Average: 0.22911080166115938\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "health informatics                       0.6339\n",
            "analytical statistical programming       0.4036\n",
            "group data                               0.3918\n",
            "medicaid                                 0.3757\n",
            "position located department              0.3249\n",
            "emerging innovations                     0.2476\n",
            "analyze interpret                        0.2452\n",
            "gs design develop                        0.2427\n",
            "cms office burden                        0.2234\n",
            "reduction                                0.1954\n",
            "human                                    0.146\n",
            "centers                                  0.1354\n",
            "unique highly specialized                0.1341\n",
            "obrhi emerging                           0.1083\n",
            "mechanisms necessary                     0.0224\n",
            "\n",
            "Score Average: 0.272\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "service delivery\n",
            "target position\n",
            "integrated data\n",
            "correlation analysis\n",
            "programming languages\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "position located department              0.003683438125747622\n",
            "specialized data sets                    0.005385420690931887\n",
            "hhs centers medicare                     0.006172028049662181\n",
            "centers medicare medicaid                0.006172028049662181\n",
            "cms office burden                        0.006172028049662181\n",
            "office burden reduction                  0.006172028049662181\n",
            "informatics obrhi emerging               0.006172028049662181\n",
            "obrhi emerging innovations               0.006172028049662181\n",
            "emerging innovations group               0.006172028049662181\n",
            "design develop implement                 0.006172028049662181\n",
            "develop implement analytical             0.006172028049662181\n",
            "implement analytical statistical         0.006172028049662181\n",
            "analytical statistical programming       0.006172028049662181\n",
            "statistical programming mechanisms       0.006172028049662181\n",
            "collect organize analyze                 0.006172028049662181\n",
            "organize analyze interpret               0.006172028049662181\n",
            "analyze interpret unique                 0.006172028049662181\n",
            "interpret unique highly                  0.006172028049662181\n",
            "unique highly specialized                0.006172028049662181\n",
            "located department health                0.009062863878918739\n",
            "Score Average: 0.005999649245312452\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.28584294577822333\n",
            "health                                   0.2692405194482652\n",
            "services                                 0.26191487903200117\n",
            "located                                  0.1707514088341824\n",
            "programming mechanisms necessary         0.15780425521312472\n",
            "\n",
            "Score Average: 0.22911080166115938\n",
            "------------------------\n",
            "\n",
            "Resume # 1\n",
            "Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN,\n",
            "Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic\n",
            "Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot,\n",
            "Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education\n",
            "Details     Data Science Assurance Associate     Data Science Assurance Associate - Ernst & Young LLP  Skill Details   JAVASCRIPT- Exprience - 24 months  jQuery- Exprience -\n",
            "24 months  Python- Exprience - 24 monthsCompany Details   company - Ernst & Young LLP  description - Fraud Investigations and Dispute Services   Assurance  TECHNOLOGY ASSISTED\n",
            "REVIEW  TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.  * Core member of a team helped in developing\n",
            "automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in\n",
            "reduced labor costs and time spent during the lawyers review.  * Understand the end to end flow of the solution, doing research and development for classification models,\n",
            "predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.  * TAR assists in\n",
            "predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
            "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment\n",
            "analysis. Matplot lib, Tableau dashboard for reporting.    MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)  TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA *\n",
            "Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4\n",
            "categories.  * Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted\n",
            "Word cloud.  * Created customized tableau dashboards for effective reporting and visualizations.  CHATBOT * Developed a user friendly chatbot for one of our Products which\n",
            "handle simple questions about hours of operation, reservation options and so on.  * This chat bot serves entire product related questions. Giving overview of tool via QA\n",
            "platform and also give recommendation responses so that user question to build chain of relevant answer.  * This too has intelligence to build the pipeline of questions as per\n",
            "user requirement and asks the relevant /recommended questions.    Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis,\n",
            "Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer    INFORMATION GOVERNANCE  Organizations to make informed decisions about all of the information they store. The\n",
            "integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to\n",
            "counter information risk.  * Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic\n",
            "search and created customized, interactive dashboards using kibana.  * Preforming ROT Analysis on the data which give information of data which helps identify content that is\n",
            "either Redundant, Outdated, or Trivial.  * Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable\n",
            "information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.  Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
            "FRAUD ANALYTIC PLATFORM  Fraud Analytics and investigative platform to review all red flag cases.  â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case\n",
            "manager and suite of Analytics for various ERP systems.  * It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be\n",
            "indicators of fraud by running advanced analytics  Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js\n",
            "\n",
            "['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikitlearn', 'matplotlib', 'sql', 'java', 'javascriptjquery', 'machine', 'learning', 'regression', 'svm', 'naãve', 'bayes', 'knn', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural', 'language', 'processing', 'dimensionality', 'reduction', 'topic', 'modelling', 'lda', 'nmf', 'pca', 'neural', 'nets', 'database', 'visualizations', 'mysql', 'sqlserver', 'cassandra', 'hbase', 'elasticsearch', 'djs', 'dcjs', 'plotly', 'kibana', 'matplotlib', 'ggplot', 'tableau', 'others', 'regular', 'expression', 'html', 'css', 'angular', 'logstash', 'kafka', 'python', 'flask', 'git', 'docker', 'computer', 'vision', 'open', 'cv', 'and', 'understanding', 'of', 'deep', 'learningeducation', 'details', 'data', 'science', 'assurance', 'associate', 'data', 'science', 'assurance', 'associate', 'ernst', 'young', 'llp', 'skill', 'details', 'javascript', 'exprience', 'months', 'jquery', 'exprience', 'months', 'python', 'exprience', 'monthscompany', 'details', 'company', 'ernst', 'young', 'llp', 'description', 'fraud', 'investigations', 'and', 'dispute', 'services', 'assurance', 'technology', 'assisted', 'review', 'tar', 'technology', 'assisted', 'review', 'assists', 'in', 'accelerating', 'the', 'review', 'process', 'and', 'run', 'analytics', 'and', 'generate', 'reports', 'core', 'member', 'of', 'a', 'team', 'helped', 'in', 'developing', 'automated', 'review', 'platform', 'tool', 'from', 'scratch', 'for', 'assisting', 'e', 'discovery', 'domain', 'this', 'tool', 'implements', 'predictive', 'coding', 'and', 'topic', 'modelling', 'by', 'automating', 'reviews', 'resulting', 'in', 'reduced', 'labor', 'costs', 'and', 'time', 'spent', 'during', 'the', 'lawyers', 'review', 'understand', 'the', 'end', 'to', 'end', 'flow', 'of', 'the', 'solution', 'doing', 'research', 'and', 'development', 'for', 'classification', 'models', 'predictive', 'analysis', 'and', 'mining', 'of', 'the', 'information', 'present', 'in', 'text', 'data', 'worked', 'on', 'analyzing', 'the', 'outputs', 'and', 'precision', 'monitoring', 'for', 'the', 'entire', 'tool', 'tar', 'assists', 'in', 'predictive', 'coding', 'topic', 'modelling', 'from', 'the', 'evidence', 'by', 'following', 'ey', 'standards', 'developed', 'the', 'classifier', 'models', 'in', 'order', 'to', 'identify', 'red', 'flags', 'and', 'fraudrelated', 'issues', 'tools', 'technologies', 'python', 'scikitlearn', 'tfidf', 'wordvec', 'docvec', 'cosine', 'similarity', 'naãve', 'bayes', 'lda', 'nmf', 'for', 'topic', 'modelling', 'vader', 'and', 'text', 'blob', 'for', 'sentiment', 'analysis', 'matplot', 'lib', 'tableau', 'dashboard', 'for', 'reporting', 'multiple', 'data', 'science', 'and', 'analytic', 'projects', 'usa', 'clients', 'text', 'analytics', 'motor', 'vehicle', 'customer', 'review', 'data', 'received', 'customer', 'feedback', 'survey', 'data', 'for', 'past', 'one', 'year', 'performed', 'sentiment', 'positive', 'negative', 'neutral', 'and', 'time', 'series', 'analysis', 'on', 'customer', 'comments', 'across', 'all', 'categories', 'created', 'heat', 'map', 'of', 'terms', 'by', 'survey', 'category', 'based', 'on', 'frequency', 'of', 'words', 'extracted', 'positive', 'and', 'negative', 'words', 'across', 'all', 'the', 'survey', 'categories', 'and', 'plotted', 'word', 'cloud', 'created', 'customized', 'tableau', 'dashboards', 'for', 'effective', 'reporting', 'and', 'visualizations', 'chatbot', 'developed', 'a', 'user', 'friendly', 'chatbot', 'for', 'one', 'of', 'our', 'products', 'which', 'handle', 'simple', 'questions', 'about', 'hours', 'of', 'operation', 'reservation', 'options', 'and', 'so', 'on', 'this', 'chat', 'bot', 'serves', 'entire', 'product', 'related', 'questions', 'giving', 'overview', 'of', 'tool', 'via', 'qa', 'platform', 'and', 'also', 'give', 'recommendation', 'responses', 'so', 'that', 'user', 'question', 'to', 'build', 'chain', 'of', 'relevant', 'answer', 'this', 'too', 'has', 'intelligence', 'to', 'build', 'the', 'pipeline', 'of', 'questions', 'as', 'per', 'user', 'requirement', 'and', 'asks', 'the', 'relevant', 'recommended', 'questions', 'tools', 'technologies', 'python', 'natural', 'language', 'processing', 'nltk', 'spacy', 'topic', 'modelling', 'sentiment', 'analysis', 'word', 'embedding', 'scikitlearn', 'javascriptjquery', 'sqlserver', 'information', 'governance', 'organizations', 'to', 'make', 'informed', 'decisions', 'about', 'all', 'of', 'the', 'information', 'they', 'store', 'the', 'integrated', 'information', 'governance', 'portfolio', 'synthesizes', 'intelligence', 'across', 'unstructured', 'data', 'sources', 'and', 'facilitates', 'action', 'to', 'ensure', 'organizations', 'are', 'best', 'positioned', 'to', 'counter', 'information', 'risk', 'scan', 'data', 'from', 'multiple', 'sources', 'of', 'formats', 'and', 'parse', 'different', 'file', 'formats', 'extract', 'meta', 'data', 'information', 'push', 'results', 'for', 'indexing', 'elastic', 'search', 'and', 'created', 'customized', 'interactive', 'dashboards', 'using', 'kibana', 'preforming', 'rot', 'analysis', 'on', 'the', 'data', 'which', 'give', 'information', 'of', 'data', 'which', 'helps', 'identify', 'content', 'that', 'is', 'either', 'redundant', 'outdated', 'or', 'trivial', 'preforming', 'fulltext', 'search', 'analysis', 'on', 'elastic', 'search', 'with', 'predefined', 'methods', 'which', 'can', 'tag', 'as', 'pii', 'personally', 'identifiable', 'information', 'social', 'security', 'numbers', 'addresses', 'names', 'etc', 'which', 'frequently', 'targeted', 'during', 'cyberattacks', 'tools', 'technologies', 'python', 'flask', 'elastic', 'search', 'kibana', 'fraud', 'analytic', 'platform', 'fraud', 'analytics', 'and', 'investigative', 'platform', 'to', 'review', 'all', 'red', 'flag', 'cases', 'â', 'fap', 'is', 'a', 'fraud', 'analytics', 'and', 'investigative', 'platform', 'with', 'inbuilt', 'case', 'manager', 'and', 'suite', 'of', 'analytics', 'for', 'various', 'erp', 'systems', 'it', 'can', 'be', 'used', 'by', 'clients', 'to', 'interrogate', 'their', 'accounting', 'systems', 'for', 'identifying', 'the', 'anomalies', 'which', 'can', 'be', 'indicators', 'of', 'fraud', 'by', 'running', 'advanced', 'analytics', 'tools', 'technologies', 'html', 'javascript', 'sqlserver', 'jquery', 'css', 'bootstrap', 'nodejs', 'djs', 'dcjs']\n",
            "skills programming languages python pandas numpy scipy scikitlearn matplotlib sql java javascriptjquery machine learning regression svm naãve bayes knn random forest decision trees boosting techniques cluster analysis word embedding sentiment analysis natural language processing dimensionality reduction topic modelling lda nmf pca neural nets database visualizations mysql sqlserver cassandra hbase elasticsearch djs dcjs plotly kibana matplotlib ggplot tableau others regular expression html css angular logstash kafka python flask git docker computer vision open cv understanding deep learningeducation details data science assurance associate data science assurance associate ernst young llp skill details javascript exprience months jquery exprience months python exprience monthscompany details company ernst young llp description fraud investigations dispute services assurance technology assisted review tar technology assisted review assists accelerating review process run analytics generate reports core member team helped developing automated review platform tool scratch assisting e discovery domain tool implements predictive coding topic modelling automating reviews resulting reduced labor costs time spent lawyers review understand end end flow solution research development classification models predictive analysis mining information present text data worked analyzing outputs precision monitoring entire tool tar assists predictive coding topic modelling evidence following ey standards developed classifier models order identify red flags fraudrelated issues tools technologies python scikitlearn tfidf wordvec docvec cosine similarity naãve bayes lda nmf topic modelling vader text blob sentiment analysis matplot lib tableau dashboard reporting multiple data science analytic projects usa clients text analytics motor vehicle customer review data received customer feedback survey data past one year performed sentiment positive negative neutral time series analysis customer comments across categories created heat map terms survey category based frequency words extracted positive negative words across survey categories plotted word cloud created customized tableau dashboards effective reporting visualizations chatbot developed user friendly chatbot one products handle simple questions hours operation reservation options chat bot serves entire product related questions giving overview tool via qa platform also give recommendation responses user question build chain relevant answer intelligence build pipeline questions per user requirement asks relevant recommended questions tools technologies python natural language processing nltk spacy topic modelling sentiment analysis word embedding scikitlearn javascriptjquery sqlserver information governance organizations make informed decisions information store integrated information governance portfolio synthesizes intelligence across unstructured data sources facilitates action ensure organizations best positioned counter information risk scan data multiple sources formats parse different file formats extract meta data information push results indexing elastic search created customized interactive dashboards using kibana preforming rot analysis data give information data helps identify content either redundant outdated trivial preforming fulltext search analysis elastic search predefined methods tag pii personally identifiable information social security numbers addresses names etc frequently targeted cyberattacks tools technologies python flask elastic search kibana fraud analytic platform fraud analytics investigative platform review red flag cases â fap fraud analytics investigative platform inbuilt case manager suite analytics various erp systems used clients interrogate accounting systems identifying anomalies indicators fraud running advanced analytics tools technologies html javascript sqlserver jquery css bootstrap nodejs djs dcjs\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "text analytics                           0.5406\n",
            "categories created                       0.3252\n",
            "ey standards                             0.2298\n",
            "flask elastic search                     0.2127\n",
            "deep learningeducation details           0.2003\n",
            "reviews resulting reduced                0.1973\n",
            "governance portfolio                     0.1942\n",
            "review tar technology                    0.1859\n",
            "scipy scikitlearn matplotlib             0.1623\n",
            "flag cases fap                           0.1531\n",
            "months                                   0.1368\n",
            "end                                      0.1255\n",
            "javascript sqlserver jquery              0.1012\n",
            "regular expression html                  0.0628\n",
            "git docker computer                      0.0504\n",
            "\n",
            "Score Average: 0.20197857142857142\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "association rules\n",
            "vehicles\n",
            "boosting\n",
            "sentiment analysis\n",
            "database systems\n",
            "natural language processing\n",
            "text data\n",
            "css\n",
            "user information\n",
            "programming languages\n",
            "machine learning\n",
            "k-nearest neighbors\n",
            "word embedding\n",
            "customer review\n",
            "decision trees\n",
            "social sciences\n",
            "recommendation\n",
            "question answering\n",
            "erp system\n",
            "classification models\n",
            "visualization tools\n",
            "cyber-attacks\n",
            "javascript\n",
            "svm\n",
            "dimensionality reduction\n",
            "natural languages\n",
            "visualization\n",
            "computer vision\n",
            "textual data\n",
            "html\n",
            "classifiers\n",
            "principle component analysis\n",
            "random forests\n",
            "cluster analysis\n",
            "java\n",
            "python\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "coding topic modelling                   0.00019373089847710953\n",
            "tools technologies python                0.00020533252874614465\n",
            "predictive coding topic                  0.00022660120047345922\n",
            "science assurance associate              0.00023597521189463587\n",
            "ernst young llp                          0.00025489956207999924\n",
            "natural language processing              0.00028034724728379535\n",
            "fraud analytics investigative            0.0003747231806369725\n",
            "technology assisted review               0.00038159055373030847\n",
            "data science assurance                   0.00038914530362812435\n",
            "analytics investigative platform         0.00041165922398054097\n",
            "analysis word embedding                  0.00043046293272864524\n",
            "topic modelling lda                      0.00044486609829637617\n",
            "nmf topic modelling                      0.00044486609829637617\n",
            "topic modelling sentiment                0.00046518759084287364\n",
            "technologies python natural              0.00047524101711444295\n",
            "technologies python flask                0.00047524101711444295\n",
            "topic modelling automating               0.00047817022156228104\n",
            "topic modelling evidence                 0.00047817022156228104\n",
            "topic modelling vader                    0.00047817022156228104\n",
            "reduction topic modelling                0.00047817022156228115\n",
            "Score Average: 0.0003749673857900574\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "information                              0.21707094170071853\n",
            "informed                                 0.21707094170071853\n",
            "analytics                                0.17028322773356402\n",
            "analytic                                 0.17028322773356402\n",
            "details data                             0.17016559159094552\n",
            "models                                   0.1498117414997547\n",
            "questions                                0.14548657836337686\n",
            "analysis word                            0.1453646611595867\n",
            "platform tool                            0.13991718971442807\n",
            "customer                                 0.13494592894668267\n",
            "customized                               0.13494592894668267\n",
            "identify                                 0.13293069204525032\n",
            "identifiable                             0.13293069204525032\n",
            "identifying                              0.13293069204525032\n",
            "python                                   0.13229443958981782\n",
            "tools technologies                       0.1312619231349776\n",
            "development                              0.12430786157985285\n",
            "developed                                0.12430786157985285\n",
            "fraud                                    0.11653807488695846\n",
            "\n",
            "Score Average: 0.1485709576840649\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "text analytics                           0.5406\n",
            "categories created                       0.3252\n",
            "ey standards                             0.2298\n",
            "flask elastic search                     0.2127\n",
            "deep learningeducation details           0.2003\n",
            "reviews resulting reduced                0.1973\n",
            "governance portfolio                     0.1942\n",
            "review tar technology                    0.1859\n",
            "scipy scikitlearn matplotlib             0.1623\n",
            "flag cases fap                           0.1531\n",
            "months                                   0.1368\n",
            "end                                      0.1255\n",
            "javascript sqlserver jquery              0.1012\n",
            "regular expression html                  0.0628\n",
            "git docker computer                      0.0504\n",
            "\n",
            "Score Average: 0.20197857142857142\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "association rules\n",
            "vehicles\n",
            "boosting\n",
            "sentiment analysis\n",
            "database systems\n",
            "natural language processing\n",
            "text data\n",
            "css\n",
            "user information\n",
            "programming languages\n",
            "machine learning\n",
            "k-nearest neighbors\n",
            "word embedding\n",
            "customer review\n",
            "decision trees\n",
            "social sciences\n",
            "recommendation\n",
            "question answering\n",
            "erp system\n",
            "classification models\n",
            "visualization tools\n",
            "cyber-attacks\n",
            "javascript\n",
            "svm\n",
            "dimensionality reduction\n",
            "natural languages\n",
            "visualization\n",
            "computer vision\n",
            "textual data\n",
            "html\n",
            "classifiers\n",
            "principle component analysis\n",
            "random forests\n",
            "cluster analysis\n",
            "java\n",
            "python\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "coding topic modelling                   0.00019373089847710953\n",
            "tools technologies python                0.00020533252874614465\n",
            "predictive coding topic                  0.00022660120047345922\n",
            "science assurance associate              0.00023597521189463587\n",
            "ernst young llp                          0.00025489956207999924\n",
            "natural language processing              0.00028034724728379535\n",
            "fraud analytics investigative            0.0003747231806369725\n",
            "technology assisted review               0.00038159055373030847\n",
            "data science assurance                   0.00038914530362812435\n",
            "analytics investigative platform         0.00041165922398054097\n",
            "analysis word embedding                  0.00043046293272864524\n",
            "topic modelling lda                      0.00044486609829637617\n",
            "nmf topic modelling                      0.00044486609829637617\n",
            "topic modelling sentiment                0.00046518759084287364\n",
            "technologies python natural              0.00047524101711444295\n",
            "technologies python flask                0.00047524101711444295\n",
            "topic modelling automating               0.00047817022156228104\n",
            "topic modelling evidence                 0.00047817022156228104\n",
            "topic modelling vader                    0.00047817022156228104\n",
            "reduction topic modelling                0.00047817022156228115\n",
            "Score Average: 0.0003749673857900574\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "information                              0.21707094170071853\n",
            "informed                                 0.21707094170071853\n",
            "analytics                                0.17028322773356402\n",
            "analytic                                 0.17028322773356402\n",
            "details data                             0.17016559159094552\n",
            "models                                   0.1498117414997547\n",
            "questions                                0.14548657836337686\n",
            "analysis word                            0.1453646611595867\n",
            "platform tool                            0.13991718971442807\n",
            "customer                                 0.13494592894668267\n",
            "customized                               0.13494592894668267\n",
            "identify                                 0.13293069204525032\n",
            "identifiable                             0.13293069204525032\n",
            "identifying                              0.13293069204525032\n",
            "python                                   0.13229443958981782\n",
            "tools technologies                       0.1312619231349776\n",
            "development                              0.12430786157985285\n",
            "developed                                0.12430786157985285\n",
            "fraud                                    0.11653807488695846\n",
            "\n",
            "Score Average: 0.1485709576840649\n",
            "------------------------\n",
            "\n",
            "Resume # 2\n",
            "Education Details   May 2013 to May 2017 B.E   UIT-RGPV  Data Scientist     Data Scientist - Matelabs  Skill Details   Python- Exprience - Less than 1 year months\n",
            "Statsmodels- Exprience - 12 months  AWS- Exprience - Less than 1 year months  Machine learning- Exprience - Less than 1 year months  Sklearn- Exprience - Less than 1 year\n",
            "months  Scipy- Exprience - Less than 1 year months  Keras- Exprience - Less than 1 year monthsCompany Details   company - Matelabs  description - ML Platform for business\n",
            "professionals, dummies and enthusiasts.  60/A Koramangala 5th block,  Achievements/Tasks behind sukh sagar, Bengaluru,  India                               Developed and\n",
            "deployed auto preprocessing steps of machine learning mainly missing value  treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.\n",
            "Deployed automated classification and regression model.  linkedin.com/in/aditya-rathore-  b4600b146                           Reasearch and deployed the time series\n",
            "forecasting model ARIMA, SARIMAX, Holt-winter and  Prophet.  Worked on meta-feature extracting problem.  github.com/rathorology  Implemented a state of the art research paper\n",
            "on outlier detection for mixed attributes.  company - Matelabs  description -\n",
            "\n",
            "['education', 'details', 'may', 'to', 'may', 'be', 'uitrgpv', 'data', 'scientist', 'data', 'scientist', 'matelabs', 'skill', 'details', 'python', 'exprience', 'less', 'than', 'year', 'months', 'statsmodels', 'exprience', 'months', 'aws', 'exprience', 'less', 'than', 'year', 'months', 'machine', 'learning', 'exprience', 'less', 'than', 'year', 'months', 'sklearn', 'exprience', 'less', 'than', 'year', 'months', 'scipy', 'exprience', 'less', 'than', 'year', 'months', 'keras', 'exprience', 'less', 'than', 'year', 'monthscompany', 'details', 'company', 'matelabs', 'description', 'ml', 'platform', 'for', 'business', 'professionals', 'dummies', 'and', 'enthusiasts', 'a', 'koramangala', 'th', 'block', 'achievementstasks', 'behind', 'sukh', 'sagar', 'bengaluru', 'india', 'developed', 'and', 'deployed', 'auto', 'preprocessing', 'steps', 'of', 'machine', 'learning', 'mainly', 'missing', 'value', 'treatment', 'outlier', 'detection', 'encoding', 'scaling', 'feature', 'selection', 'and', 'dimensionality', 'reduction', 'deployed', 'automated', 'classification', 'and', 'regression', 'model', 'linkedincominadityarathore', 'bb', 'reasearch', 'and', 'deployed', 'the', 'time', 'series', 'forecasting', 'model', 'arima', 'sarimax', 'holtwinter', 'and', 'prophet', 'worked', 'on', 'metafeature', 'extracting', 'problem', 'githubcomrathorology', 'implemented', 'a', 'state', 'of', 'the', 'art', 'research', 'paper', 'on', 'outlier', 'detection', 'for', 'mixed', 'attributes', 'company', 'matelabs', 'description']\n",
            "education details may may uitrgpv data scientist data scientist matelabs skill details python exprience less year months statsmodels exprience months aws exprience less year months machine learning exprience less year months sklearn exprience less year months scipy exprience less year months keras exprience less year monthscompany details company matelabs description ml platform business professionals dummies enthusiasts koramangala th block achievementstasks behind sukh sagar bengaluru india developed deployed auto preprocessing steps machine learning mainly missing value treatment outlier detection encoding scaling feature selection dimensionality reduction deployed automated classification regression model linkedincominadityarathore bb reasearch deployed time series forecasting model arima sarimax holtwinter prophet worked metafeature extracting problem githubcomrathorology implemented state art research paper outlier detection mixed attributes company matelabs description\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist matelabs                  0.6191\n",
            "time series forecasting                  0.3737\n",
            "months aws exprience                     0.3223\n",
            "deployed auto preprocessing              0.251\n",
            "extracting problem githubcomrathorology  0.2247\n",
            "treatment outlier                        0.2139\n",
            "scaling feature                          0.1992\n",
            "india developed                          0.1436\n",
            "value                                    0.142\n",
            "prophet worked                           0.1358\n",
            "art research paper                       0.1122\n",
            "block achievementstasks                  0.1025\n",
            "detection encoding                       0.0939\n",
            "state                                    0.0725\n",
            "mainly                                   0.0446\n",
            "\n",
            "Score Average: 0.21474285714285712\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "machine learning\n",
            "business process\n",
            "dimensionality reduction\n",
            "learning environments\n",
            "forecasting models\n",
            "genetic selection\n",
            "regression model\n",
            "education\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "company matelabs description             0.00046903504138587023\n",
            "year months machine                      0.0006165309513983214\n",
            "year months statsmodels                  0.0006377329958494993\n",
            "year months sklearn                      0.0006377329958494993\n",
            "year months scipy                        0.0006377329958494993\n",
            "year months keras                        0.0006377329958494993\n",
            "year monthscompany details               0.0007262375320516979\n",
            "data scientist matelabs                  0.0009380700827717407\n",
            "data scientist data                      0.0009447459079394153\n",
            "scientist data scientist                 0.0009447459079394153\n",
            "scientist matelabs skill                 0.0009703941122482417\n",
            "attributes company matelabs              0.0009703941122482417\n",
            "uitrgpv data scientist                   0.0009773018423854854\n",
            "steps machine learning                   0.0009773018423854854\n",
            "treatment outlier detection              0.0009773018423854854\n",
            "paper outlier detection                  0.0009773018423854854\n",
            "outlier detection encoding               0.0009773018423854857\n",
            "outlier detection mixed                  0.0009773018423854857\n",
            "detection encoding scaling               0.0010109890842991507\n",
            "detection mixed attributes               0.0010109890842991507\n",
            "Score Average: 0.0008424150405259476\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "months                                   0.2612063193938972\n",
            "exprience                                0.260656637394355\n",
            "deployed                                 0.25328244113823245\n",
            "details                                  0.19875685454684705\n",
            "model                                    0.1771486046046705\n",
            "matelabs                                 0.17110520320284117\n",
            "outlier detection                        0.13646130011191365\n",
            "company                                  0.12482946331638518\n",
            "sagar                                    0.12069108696200458\n",
            "th block                                 0.11984781557708413\n",
            "data                                     0.11326414692040532\n",
            "machine learning                         0.11301717859560359\n",
            "\n",
            "Score Average: 0.17085558764702\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist matelabs                  0.6191\n",
            "time series forecasting                  0.3737\n",
            "months aws exprience                     0.3223\n",
            "deployed auto preprocessing              0.251\n",
            "extracting problem githubcomrathorology  0.2247\n",
            "treatment outlier                        0.2139\n",
            "scaling feature                          0.1992\n",
            "india developed                          0.1436\n",
            "value                                    0.142\n",
            "prophet worked                           0.1358\n",
            "art research paper                       0.1122\n",
            "block achievementstasks                  0.1025\n",
            "detection encoding                       0.0939\n",
            "state                                    0.0725\n",
            "mainly                                   0.0446\n",
            "\n",
            "Score Average: 0.21474285714285712\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "machine learning\n",
            "business process\n",
            "dimensionality reduction\n",
            "learning environments\n",
            "forecasting models\n",
            "genetic selection\n",
            "regression model\n",
            "education\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "company matelabs description             0.00046903504138587023\n",
            "year months machine                      0.0006165309513983214\n",
            "year months statsmodels                  0.0006377329958494993\n",
            "year months sklearn                      0.0006377329958494993\n",
            "year months scipy                        0.0006377329958494993\n",
            "year months keras                        0.0006377329958494993\n",
            "year monthscompany details               0.0007262375320516979\n",
            "data scientist matelabs                  0.0009380700827717407\n",
            "data scientist data                      0.0009447459079394153\n",
            "scientist data scientist                 0.0009447459079394153\n",
            "scientist matelabs skill                 0.0009703941122482417\n",
            "attributes company matelabs              0.0009703941122482417\n",
            "uitrgpv data scientist                   0.0009773018423854854\n",
            "steps machine learning                   0.0009773018423854854\n",
            "treatment outlier detection              0.0009773018423854854\n",
            "paper outlier detection                  0.0009773018423854854\n",
            "outlier detection encoding               0.0009773018423854857\n",
            "outlier detection mixed                  0.0009773018423854857\n",
            "detection encoding scaling               0.0010109890842991507\n",
            "detection mixed attributes               0.0010109890842991507\n",
            "Score Average: 0.0008424150405259476\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "months                                   0.2612063193938972\n",
            "exprience                                0.260656637394355\n",
            "deployed                                 0.25328244113823245\n",
            "details                                  0.19875685454684705\n",
            "model                                    0.1771486046046705\n",
            "matelabs                                 0.17110520320284117\n",
            "outlier detection                        0.13646130011191365\n",
            "company                                  0.12482946331638518\n",
            "sagar                                    0.12069108696200458\n",
            "th block                                 0.11984781557708413\n",
            "data                                     0.11326414692040532\n",
            "machine learning                         0.11301717859560359\n",
            "\n",
            "Score Average: 0.17085558764702\n",
            "------------------------\n",
            "\n",
            "KeyBERT match Similarity Mean: 0.36621243999212566\n",
            "CSO match Similiarity Mean: 0.23352156469753726\n",
            "Yake match Similiarity Mean: 0.29001494354092194\n",
            "TextRank match Similarity Mean: 0.24275117212956615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume and Job Matching"
      ],
      "metadata": {
        "id": "MsExUakXPC_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume and Job Posting Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, all_resumes_keywords)\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQHvfMS9l7Qt",
        "outputId": "6ec775b6-d59e-4309-942a-19e91301a8c0"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 1\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 23.57 %\n",
            "Matched JP Description: data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records\n",
            "management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote\n",
            "position\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.9 %\n",
            "Matched JP Description: position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging\n",
            "innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly\n",
            "specialized data sets\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 11.95 %\n",
            "Matched JP Description: position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging\n",
            "innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly\n",
            "specialized data sets\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 2\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 0.0 %\n",
            "Matched JP Description: data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records\n",
            "management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote\n",
            "position\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.9 %\n",
            "Matched JP Description: position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging\n",
            "innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly\n",
            "specialized data sets\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 0.0 %\n",
            "Matched JP Description: data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records\n",
            "management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote\n",
            "position\n",
            "\n",
            "---POST PROCESSING ANALYSIS----\n",
            "-------------------------------\n",
            "\n",
            "Average Matching Scores for Skill Extraction Method\n",
            "BERT:  11.78 %\n",
            "CSO :  16.9 %\n",
            "YAKE:  5.98 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Upload"
      ],
      "metadata": {
        "id": "xxp4OHVTXpSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and Drop a Data Scientist resum into Colab \n",
        "\n",
        "Make sure the file is named **resume.pdf**\n",
        "\n",
        "There is a dummy resume located in the `/data` directory.\n",
        "\n",
        "\n",
        "\n",
        "< - - - - - - -"
      ],
      "metadata": {
        "id": "1RCg41ohJkHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPWWB4B9YP5j",
        "outputId": "db892923-91a6-4091-b038-97f5b766aee5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer3 in /usr/local/lib/python3.8/dist-packages (2018.12.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (2.4.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (3.16.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer3.layout import LAParams, LTTextBox\n",
        "from pdfminer3.pdfpage import PDFPage\n",
        "from pdfminer3.pdfinterp import PDFResourceManager\n",
        "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer3.converter import PDFPageAggregator\n",
        "from pdfminer3.converter import TextConverter\n",
        "import io\n",
        "import sys\n",
        "from contextlib import redirect_stdout\n",
        "from io import StringIO "
      ],
      "metadata": {
        "id": "hnaZCNoQK53g"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resource_manager = PDFResourceManager()\n",
        "fake_file_handle = io.StringIO()\n",
        "converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
        "page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "\n",
        "with open('/content/data/resume.pdf', 'rb') as fh:\n",
        "\n",
        "    for page in PDFPage.get_pages(fh,\n",
        "                                  caching=True,\n",
        "                                  check_extractable=True):\n",
        "        page_interpreter.process_page(page)\n",
        "\n",
        "    text = fake_file_handle.getvalue()\n",
        "\n",
        "# close open handles\n",
        "converter.close()\n",
        "fake_file_handle.close()\n",
        "\n",
        "# normalize and extract skills \n",
        "text = normalizeCorpus(text)\n",
        "resume_kw = extractSkills(text)\n",
        "\n",
        "# Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, [resume_kw])\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vsrbN-UXsG8",
        "outputId": "65a381b4-592e-4eb1-c301-0d68836e9b76"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['juan', 'jose', 'carin', 'data', 'scientist', 'professional', 'profile', 'mountain', 'view', 'ca', 'juanjosecaringmailcom', 'juanjocaringithubio', 'linkedincominjuanjosecarin', 'passionate', 'about', 'data', 'analysis', 'and', 'experiments', 'mainly', 'focused', 'on', 'user', 'behavior', 'experience', 'and', 'engagement', 'with', 'a', 'solid', 'background', 'in', 'data', 'science', 'and', 'statistics', 'and', 'extensive', 'experience', 'using', 'data', 'insights', 'to', 'drive', 'business', 'growth', 'education', 'relevant', 'courses', 'university', 'of', 'california', 'berkeley', 'machine', 'learning', 'machine', 'learning', 'at', 'scale', 'storing', 'and', 'retrieving', 'data', 'master', 'of', 'information', 'and', 'data', 'science', 'field', 'experiments', 'applied', 'regression', 'and', 'time', 'series', 'data', 'visualization', 'and', 'communication', 'gpa', 'analysis', 'research', 'design', 'and', 'applications', 'for', 'exploring', 'and', 'analyzing', 'data', 'data', 'analysis', 'universidad', 'politécnica', 'de', 'madrid', 'ms', 'in', 'statistical', 'and', 'computational', 'information', 'processing', 'neural', 'networks', 'and', 'statistical', 'learning', 'relevant', 'courses', 'data', 'mining', 'multivariate', 'analysis', 'time', 'series', 'monte', 'carlo', 'techniques', 'numerical', 'methods', 'in', 'finance', 'stochastic', 'models', 'in', 'finance', 'bayesian', 'networks', 'regression', 'and', 'prediction', 'methods', 'optimization', 'techniques', 'gpa', 'universidad', 'politécnica', 'de', 'madrid', 'ms', 'in', 'telecommunication', 'engineering', 'gpa', 'focus', 'area', 'fellowship', 'radio', 'communication', 'systems', 'radar', 'and', 'mobile', 'first', 'year', 'at', 'university', 'due', 'to', 'honors', 'obtained', 'last', 'year', 'at', 'high', 'school', 'skills', 'proficient', 'intermediate', 'basic', 'programming', 'statistics', 'r', 'python', 'sql', 'spss', 'sas', 'matlab', 'eviews', 'demetra', 'big', 'data', 'hadoop', 'hive', 'mrjob', 'spark', 'storm', 'visualization', 'tableau', 'djs', 'others', 'git', 'aws', 'bash', 'gephi', 'neoj', 'qgis', 'experience', 'data', 'science', 'jan', 'mar', 'jun', 'sep', 'data', 'scientist', 'conento', 'madrid', 'spain', 'working', 'remotely', 'designed', 'and', 'implemented', 'the', 'etl', 'pipeline', 'for', 'a', 'predictive', 'model', 'of', 'traffic', 'on', 'the', 'main', 'roads', 'in', 'eastern', 'spain', 'a', 'project', 'for', 'the', 'spanish', 'government', 'automated', 'scripts', 'in', 'r', 'to', 'extract', 'transform', 'clean', 'incl', 'anomaly', 'detection', 'and', 'load', 'into', 'mysql', 'data', 'from', 'multiple', 'data', 'sources', 'road', 'traffic', 'sensors', 'accidents', 'road', 'works', 'weather', 'data', 'scientist', 'conento', 'madrid', 'spain', 'designed', 'an', 'experiment', 'for', 'google', 'spain', 'conducted', 'in', 'october', 'to', 'measure', 'the', 'impact', 'of', 'youtube', 'ads', 'on', 'the', 'sales', 'of', 'a', 'car', 'manufacturers', 'dealer', 'network', 'a', 'matchedpair', 'clusterrandomized', 'design', 'which', 'involved', 'selecting', 'the', 'test', 'and', 'control', 'groups', 'from', 'a', 'sample', 'of', 'cities', 'in', 'spain', 'where', 'geotargeted', 'ads', 'were', 'possible', 'based', 'on', 'their', 'sales', 'wise', 'similarity', 'over', 'time', 'using', 'wavelets', 'and', 'r', 'management', 'sales', 'electrical', 'eng', 'feb', 'aug', 'head', 'of', 'sales', 'spain', 'portugal', 'test', 'measurement', 'dept', 'yokogawa', 'madrid', 'spain', 'applied', 'analysis', 'of', 'sales', 'and', 'market', 'trends', 'to', 'decide', 'the', 'direction', 'of', 'the', 'department', 'led', 'a', 'team', 'of', 'people', 'of', 'juan', 'jose', 'carin', 'data', 'scientist', 'mountain', 'view', 'ca', 'juanjosecaringmailcom', 'juanjocaringithubio', 'linkedincominjuanjosecarin', 'increased', 'revenue', 'by', 'gross', 'profit', 'by', 'and', 'operating', 'income', 'by', 'and', 'achieved', 'a', 'ratio', 'of', 'new', 'customers', 'x', 'growth', 'by', 'entering', 'new', 'markets', 'and', 'improving', 'customer', 'service', 'and', 'training', 'sales', 'electrical', 'eng', 'telecom', 'apr', 'jan', 'sales', 'engineer', 'test', 'measurement', 'dept', 'yokogawa', 'promoted', 'to', 'head', 'of', 'sales', 'after', 'months', 'leading', 'the', 'sales', 'team', 'madrid', 'spain', 'sep', 'mar', 'sales', 'application', 'engineer', 'ayscom', 'madrid', 'spain', 'exceeded', 'sales', 'target', 'every', 'year', 'from', 'to', 'achieved', 'of', 'the', 'target', 'in', 'the', 'first', 'months', 'education', 'of', 'jul', 'jun', 'tutor', 'of', 'differential', 'integral', 'calculus', 'physics', 'and', 'digital', 'electronic', 'circuits', 'academia', 'universitaria', 'madrid', 'spain', 'highestrated', 'professor', 'in', 'student', 'surveys', 'in', 'of', 'the', 'terms', 'increased', 'ratio', 'of', 'students', 'passing', 'the', 'course', 'by', 'projects', 'see', 'juanjocaringithubio', 'for', 'additional', 'information', 'smartcam', 'capstone', 'a', 'scalable', 'cloudbased', 'video', 'monitoring', 'system', 'that', 'features', 'motion', 'detection', 'face', 'counting', 'and', 'image', 'recognition', 'python', 'opencv', 'tensorflow', 'aws', 'ec', 's', 'dynamodb', 'implementation', 'of', 'the', 'shortest', 'path', 'and', 'pagerank', 'algorithms', 'with', 'the', 'wikipedia', 'graph', 'dataset', 'machine', 'learning', 'at', 'scale', 'using', 'a', 'graph', 'dataset', 'of', 'almost', 'half', 'a', 'million', 'nodes', 'hadoop', 'mrjob', 'python', 'aws', 'ec', 'aws', 's', 'forest', 'cover', 'type', 'prediction', 'machine', 'learning', 'python', 'scikitlearn', 'matplotlib', 'a', 'kaggle', 'competition', 'predictions', 'of', 'the', 'predominant', 'kind', 'of', 'tree', 'cover', 'from', 'strictly', 'cartographic', 'variables', 'such', 'as', 'elevation', 'and', 'soil', 'type', 'using', 'random', 'forests', 'svms', 'knns', 'naive', 'bayes', 'gradient', 'descent', 'gmms', 'redefining', 'the', 'job', 'search', 'process', 'storing', 'and', 'retrieving', 'data', 'hadoop', 'hdfs', 'hive', 'spark', 'python', 'aws', 'ec', 'tableau', 'a', 'pipeline', 'that', 'combines', 'data', 'from', 'indeed', 'api', 'and', 'the', 'us', 'census', 'bureau', 'to', 'select', 'the', 'best', 'locations', 'for', 'data', 'scientists', 'based', 'on', 'the', 'number', 'of', 'job', 'postings', 'housing', 'cost', 'etc', 'a', 'fresh', 'perspective', 'on', 'citi', 'bike', 'data', 'visualization', 'and', 'communication', 'an', 'interactive', 'website', 'to', 'visualize', 'nyc', 'citi', 'bike', 'bicycle', 'sharing', 'service', 'tableau', 'sqlite', 'investigating', 'the', 'effect', 'of', 'competition', 'on', 'the', 'ability', 'to', 'solve', 'arithmetic', 'problems', 'field', 'experiments', 'r', 'a', 'randomized', 'controlled', 'trial', 'in', 'which', 'participants', 'were', 'assigned', 'to', 'a', 'control', 'group', 'or', 'one', 'of', 'two', 'test', 'groups', 'to', 'evaluate', 'the', 'effect', 'of', 'competition', 'being', 'compared', 'to', 'no', 'one', 'or', 'someone', 'better', 'or', 'worse', 'prediction', 'of', 'customer', 'churn', 'for', 'a', 'mobile', 'network', 'carrier', 'data', 'mining', 'predictions', 'from', 'a', 'sample', 'of', 'customers', 'using', 'tree', 'decisions', 'logistic', 'regression', 'and', 'neural', 'networks', 'sas', 'different', 'models', 'of', 'harmonized', 'index', 'of', 'consumer', 'prices', 'hicp', 'in', 'spain', 'time', 'series', 'spss', 'demetra', 'forecasts', 'based', 'on', 'exponential', 'smoothing', 'arima', 'and', 'transfer', 'function', 'using', 'petrol', 'price', 'as', 'independent', 'variable', 'models', 'of']\n",
            "juan jose carin data scientist professional profile mountain view ca juanjosecaringmailcom juanjocaringithubio linkedincominjuanjosecarin passionate data analysis experiments mainly focused user behavior experience engagement solid background data science statistics extensive experience using data insights drive business growth education relevant courses university california berkeley machine learning machine learning scale storing retrieving data master information data science field experiments applied regression time series data visualization communication gpa analysis research design applications exploring analyzing data data analysis universidad politécnica de madrid ms statistical computational information processing neural networks statistical learning relevant courses data mining multivariate analysis time series monte carlo techniques numerical methods finance stochastic models finance bayesian networks regression prediction methods optimization techniques gpa universidad politécnica de madrid ms telecommunication engineering gpa focus area fellowship radio communication systems radar mobile first year university due honors obtained last year high school skills proficient intermediate basic programming statistics r python sql spss sas matlab eviews demetra big data hadoop hive mrjob spark storm visualization tableau djs others git aws bash gephi neoj qgis experience data science jan mar jun sep data scientist conento madrid spain working remotely designed implemented etl pipeline predictive model traffic main roads eastern spain project spanish government automated scripts r extract transform clean incl anomaly detection load mysql data multiple data sources road traffic sensors accidents road works weather data scientist conento madrid spain designed experiment google spain conducted october measure impact youtube ads sales car manufacturers dealer network matchedpair clusterrandomized design involved selecting test control groups sample cities spain geotargeted ads possible based sales wise similarity time using wavelets r management sales electrical eng feb aug head sales spain portugal test measurement dept yokogawa madrid spain applied analysis sales market trends decide direction department led team people juan jose carin data scientist mountain view ca juanjosecaringmailcom juanjocaringithubio linkedincominjuanjosecarin increased revenue gross profit operating income achieved ratio new customers x growth entering new markets improving customer service training sales electrical eng telecom apr jan sales engineer test measurement dept yokogawa promoted head sales months leading sales team madrid spain sep mar sales application engineer ayscom madrid spain exceeded sales target every year achieved target first months education jul jun tutor differential integral calculus physics digital electronic circuits academia universitaria madrid spain highestrated professor student surveys terms increased ratio students passing course projects see juanjocaringithubio additional information smartcam capstone scalable cloudbased video monitoring system features motion detection face counting image recognition python opencv tensorflow aws ec dynamodb implementation shortest path pagerank algorithms wikipedia graph dataset machine learning scale using graph dataset almost half million nodes hadoop mrjob python aws ec aws forest cover type prediction machine learning python scikitlearn matplotlib kaggle competition predictions predominant kind tree cover strictly cartographic variables elevation soil type using random forests svms knns naive bayes gradient descent gmms redefining job search process storing retrieving data hadoop hdfs hive spark python aws ec tableau pipeline combines data indeed api us census bureau select best locations data scientists based number job postings housing cost etc fresh perspective citi bike data visualization communication interactive website visualize nyc citi bike bicycle sharing service tableau sqlite investigating effect competition ability solve arithmetic problems field experiments r randomized controlled trial participants assigned control group one two test groups evaluate effect competition compared one someone better worse prediction customer churn mobile network carrier data mining predictions sample customers using tree decisions logistic regression neural networks sas different models harmonized index consumer prices hicp spain time series spss demetra forecasts based exponential smoothing arima transfer function using petrol price independent variable models\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist conento                   0.6133\n",
            "best locations data                      0.4084\n",
            "market trends decide                     0.2959\n",
            "customer churn mobile                    0.2449\n",
            "anomaly detection load                   0.2293\n",
            "path pagerank                            0.2249\n",
            "tableau pipeline combines                0.2216\n",
            "matchedpair clusterrandomized design     0.1975\n",
            "compared better worse                    0.1678\n",
            "gmms redefining job                      0.158\n",
            "sharing                                  0.1567\n",
            "time using wavelets                      0.14\n",
            "aws forest cover                         0.1164\n",
            "students passing course                  0.0903\n",
            "housing cost fresh                       0.047\n",
            "\n",
            "Score Average: 0.2332142857142857\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "logistic regression\n",
            "sales\n",
            "engineers\n",
            "neural networks\n",
            "hadoop\n",
            "big data\n",
            "stochastic processes\n",
            "optimization problems\n",
            "customer services\n",
            "machine learning\n",
            "communication\n",
            "communication systems\n",
            "radio\n",
            "mobile networks\n",
            "search process\n",
            "location data\n",
            "radar\n",
            "engineering\n",
            "shortest path\n",
            "sensors\n",
            "data mining\n",
            "hdfs\n",
            "naive bayes\n",
            "anomaly detection\n",
            "visualization\n",
            "revenue\n",
            "education\n",
            "university\n",
            "matlab\n",
            "optimization\n",
            "logistics\n",
            "data visualization\n",
            "motion detection\n",
            "random forests\n",
            "radio communication\n",
            "wavelet\n",
            "correlation analysis\n",
            "etl\n",
            "bayesian methods\n",
            "python\n",
            "numerical methods\n",
            "stochastic\n",
            "\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "scientist conento madrid                 8.083000465253629e-05\n",
            "conento madrid spain                     9.64359413369047e-05\n",
            "carin data scientist                     0.00011423353732105836\n",
            "data scientist conento                   0.00011423353732105839\n",
            "machine learning scale                   0.00011658834038379349\n",
            "jose carin data                          0.0001407745663388744\n",
            "test measurement dept                    0.00014208263179894855\n",
            "juanjosecaringmailcom juanjocaringithubio linkedincominjuanjosecarin 0.00014380152332808\n",
            "juan jose carin                          0.00014598170008535674\n",
            "measurement dept yokogawa                0.00014598170008535676\n",
            "data visualization communication         0.00014666648020632232\n",
            "storing retrieving data                  0.00015201418880168976\n",
            "sales electrical eng                     0.00016250975369964385\n",
            "learning machine learning                0.00018782682798614385\n",
            "machine learning machine                 0.00019833351858601582\n",
            "yokogawa madrid spain                    0.00020825048788790734\n",
            "dept yokogawa madrid                     0.0002150617159428972\n",
            "madrid spain designed                    0.0002240970105136908\n",
            "madrid spain applied                     0.0002240970105136908\n",
            "madrid spain sep                         0.0002240970105136908\n",
            "Score Average: 0.00015577897246262998\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "spain                                    0.21125645893871936\n",
            "data scientist                           0.20759198563438622\n",
            "prediction                               0.15840888911611561\n",
            "ads sales                                0.1555199524961802\n",
            "experience                               0.15044630258025993\n",
            "experiment                               0.15044630258025993\n",
            "analysis experiments                     0.1292840786949367\n",
            "competition predictions                  0.12017984638987114\n",
            "networks                                 0.1185314924073192\n",
            "network                                  0.1185314924073192\n",
            "r python                                 0.1176569901024381\n",
            "statistics                               0.10497871696484738\n",
            "statistical                              0.10497871696484738\n",
            "design                                   0.10397465185628042\n",
            "madrid                                   0.0971159479726746\n",
            "models                                   0.09677425947056065\n",
            "aws                                      0.09198259897472891\n",
            "pipeline predictive model traffic        0.09171983470563505\n",
            "based                                    0.0881402289515816\n",
            "\n",
            "Score Average: 0.12723782880047166\n",
            "\n",
            "------------------------\n",
            "Matching Job Posting for Resume 1\n",
            "\n",
            "Skill Extracted Using: keywordsBERT\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.9 %\n",
            "Matched JP Description: position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging\n",
            "innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly\n",
            "specialized data sets\n",
            "\n",
            "Skill Extracted Using: keywordsCSO\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 16.67 %\n",
            "Matched JP Description: data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records\n",
            "management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote\n",
            "position\n",
            "\n",
            "Skill Extracted Using: keywordsYAKE\n",
            "Matched JP Title: Data Scientist\n",
            "With a score of: 0.0 %\n",
            "Matched JP Description: data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records\n",
            "management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote\n",
            "position\n",
            "\n",
            "---POST PROCESSING ANALYSIS----\n",
            "-------------------------------\n",
            "\n",
            "Average Matching Scores for Skill Extraction Method\n",
            "BERT:  16.9 %\n",
            "CSO :  16.67 %\n",
            "YAKE:  0.0 %\n"
          ]
        }
      ]
    }
  ]
}