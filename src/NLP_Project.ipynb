{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#README.txt"
      ],
      "metadata": {
        "id": "VqvVC9cM4PPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and drop a ZIPPED version of the folder `/data` from GitHub repo into Colab.\n",
        "\n",
        "< - - - - -"
      ],
      "metadata": {
        "id": "SkYHORUB4Rig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# then run this and refresh directory...\n",
        "\n",
        "# unzip datasets\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "kxJqWMSD5rS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# API"
      ],
      "metadata": {
        "id": "anBUXima7kjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v4wViL775dD",
        "outputId": "3dd099f9-fec7-4740-cfa2-44e171611eb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: config in /usr/local/lib/python3.8/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import config\n",
        "# API Docs found here: https://developer.usajobs.gov/Tutorials/Search-Jobs"
      ],
      "metadata": {
        "id": "uNcEHO9t7rKr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.US_JOBS_API_KEY = \"xVc0TZiLfhcr17ci7Ngk6bLAetdRVFgntm2pZgWNtww=\"\n",
        "config.EMAIL_ADDRESS = \"gjacobthomas@gmail.com\""
      ],
      "metadata": {
        "id": "sobm0urbDUff"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I'm assuming this API has some limiter on it so we don't want to lose access. -tyler"
      ],
      "metadata": {
        "id": "89Ei-XMSDhIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# more parameters are found here: https://developer.usajobs.gov/API-Reference/GET-api-Search\n",
        "def queryJobsAPI(keyword, location):\n",
        "  host = 'data.usajobs.gov' \n",
        "  # add these values in the config.py file\n",
        "  userAgent = config.EMAIL_ADDRESS\n",
        "  authKey = config.US_JOBS_API_KEY\n",
        "\n",
        "  base_url = \"https://data.usajobs.gov/api/search\"\n",
        "\n",
        "  parameters = {\n",
        "      \"Keyword\": keyword,\n",
        "      \"LocationName\": location\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "      \"Host\": host,          \n",
        "      \"User-Agent\": userAgent,          \n",
        "      \"Authorization-Key\": authKey  \n",
        "  }\n",
        "\n",
        "  resp = requests.request(\"GET\", base_url,headers=headers, params=parameters)\n",
        "  result = resp.json()['SearchResult']['SearchResultItems']\n",
        "\n",
        "  # get Job Title \n",
        "  print(result[1]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "  # get Job Summary\n",
        "  print(result[1]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "SVoxo2K47j_l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# KeyBERT Extraction Function"
      ],
      "metadata": {
        "id": "GVU9pbVd3-hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "-RUKzBOv5-_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HIogy6WO3hvX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from keybert import KeyBERT # pip install keybert (give it a minute...)\n",
        "from statistics import mean\n",
        "\n",
        "'''\n",
        "/*---------------------------------------------------------------------\n",
        " |  Method: extractKeywordsBERT\n",
        " |\n",
        " |  Purpose: Uses the KeyBert Keyword Extraction Tool to extract\n",
        " |           and return keywords from a given corpus. \n",
        " |      \n",
        " |  Author: Tyler Parks\n",
        " |  Created On: 10/30/22\n",
        " |\n",
        " |  Parameters:\n",
        " |      normalized_corpus -- A single string containing all text of the\n",
        " |                           normalized corpus.\n",
        " |\n",
        " |  Returns: \n",
        " |      keywords -- List of collected keywords\n",
        " |      scores -- List of those keyword's scores\n",
        " |\n",
        " |  References: https://maartengr.github.io/KeyBERT/#usage\n",
        " |\n",
        " *-------------------------------------------------------------------*/\n",
        "''' \n",
        "def extractKeywordsBERT(normalized_corpus):   \n",
        "    print('---KeyBert Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    # init. language model \n",
        "    language_model = KeyBERT(model = 'all-mpnet-base-v2')\n",
        "\n",
        "    # extract those keywords!\n",
        "    data = language_model.extract_keywords( normalized_corpus, \n",
        "                                            keyphrase_ngram_range=(1, 3), \n",
        "                                            stop_words='english',\n",
        "                                            use_maxsum=False, \n",
        "                                            use_mmr=True,\n",
        "                                            diversity=0.7,\n",
        "                                            nr_candidates=20, \n",
        "                                            top_n=15\n",
        "                                        )\n",
        "\n",
        "    # zip the lists\n",
        "    zipped = list(map(list, zip(*data)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "    print('-Skill-'.ljust(40), '-Score-')\n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "    print()\n",
        "\n",
        "    # print(\"type: \"+str(type(scores))+\"length: \"+ str(len(scores)))\n",
        "\n",
        "    avg = mean(scores[:14])\n",
        "    print(\"Score Average: \" + str(avg))\n",
        "\n",
        "    return keywords, scores, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# CSO-Classifier Extraction Function"
      ],
      "metadata": {
        "id": "vmHHZLv57ex9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install cso-classifier"
      ],
      "metadata": {
        "id": "CXlOaYQj7jDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51671007-a1b0-4851-8995-9159c80591b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-03 18:33:41.596750: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.7.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cso-classifier in /usr/local/lib/python3.8/dist-packages (3.0)\n",
            "Requirement already satisfied: nltk==3.6.2 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (3.6.2)\n",
            "Requirement already satisfied: strsimpy==0.2.0 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.2.0)\n",
            "Collecting spacy==3.0.5\n",
            "  Using cached spacy-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (12.9 MB)\n",
            "Requirement already satisfied: python-Levenshtein==0.12.2 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.12.2)\n",
            "Requirement already satisfied: kneed==0.3.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.3.1)\n",
            "Requirement already satisfied: update-checker==0.18.0 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.18.0)\n",
            "Requirement already satisfied: python-igraph==0.9.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (1.21.6)\n",
            "Requirement already satisfied: hurry.filesize==0.9 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (0.9)\n",
            "Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (2.25.1)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (7.1.2)\n",
            "Requirement already satisfied: gensim==3.8.1 in /usr/local/lib/python3.8/dist-packages (from cso-classifier) (3.8.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.1->cso-classifier) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from hurry.filesize==0.9->cso-classifier) (57.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from kneed==0.3.1->cso-classifier) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from kneed==0.3.1->cso-classifier) (1.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2->cso-classifier) (2022.6.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from python-igraph==0.9.1->cso-classifier) (1.6.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests==2.25.1->cso-classifier) (2022.9.24)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (21.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (1.7.4)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "  Using cached thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.7.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (3.0.8)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (0.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy==3.0.5->cso-classifier) (2.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy==3.0.5->cso-classifier) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy==3.0.5->cso-classifier) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->kneed==0.3.1->cso-classifier) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->kneed==0.3.1->cso-classifier) (3.1.0)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.3\n",
            "    Uninstalling spacy-3.4.3:\n",
            "      Successfully uninstalled spacy-3.4.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.0.5 which is incompatible.\u001b[0m\n",
            "Successfully installed spacy-3.0.5 thinc-8.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't restart runtime if the terminal says so! Keep going.**"
      ],
      "metadata": {
        "id": "A0Ga47IZ8cH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# update spacy\n",
        "!pip install -U spacy\n",
        "import spacy\n",
        "from cso_classifier import CSOClassifier      # import classifier tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip21zn4U8Rxj",
        "outputId": "06f05661-0cdf-44ae-b8a4-081372109529"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.0.5)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Using cached thinc-8.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (819 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.7.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.0.5\n",
            "    Uninstalling spacy-3.0.5:\n",
            "      Successfully uninstalled spacy-3.0.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cso-classifier 3.0 requires spacy==3.0.5, but you have spacy 3.4.3 which is incompatible.\u001b[0m\n",
            "Successfully installed spacy-3.4.3 thinc-8.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the most recent model\n",
        "CSOClassifier.update() \n",
        "\n",
        "# define the model object\n",
        "CSO_Extractor = CSOClassifier(modules = \"both\", enhancement = \"first\", explanation = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLaIVJli8qOe",
        "outputId": "3dd6987b-7a93-43aa-8ffc-59c1441eeb07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# ======================================================\n",
            "#     ONTOLOGY\n",
            "# ======================================================\n",
            "The ontology is already up to date.\n",
            "\n",
            "# ======================================================\n",
            "#     MODELS: CACHED & WORD2VEC\n",
            "# ======================================================\n",
            "Updating the models: cached and word2vec\n",
            "[██████████████████████████████████████████████████] 63M/63M\n",
            "[*] Done!\n",
            "[██████████████████████████████████████████████████] 349M/349M\n",
            "[*] Done!\n",
            "Models downloaded successfully.\n",
            "Update completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extractKeywordsCSO(normalized_corpus):\n",
        "\n",
        "  # run the extraction\n",
        "  result = CSO_Extractor.run(normalized_corpus)\n",
        "\n",
        "  print('\\n-----CSO Extraction-----')\n",
        "  print('------------------------\\n')\n",
        "  \n",
        "  for keyword in result['union']:\n",
        "    print(keyword)\n",
        "\n",
        "  return result['union']"
      ],
      "metadata": {
        "id": "Cctwpsvv86iZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YAKE Extractor**"
      ],
      "metadata": {
        "id": "-4h4ZTbXu9CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install yake\n",
        "import yake "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNiHvEtdvF0G",
        "outputId": "01e36326-d9c6-4c93-835b-f47faf463242"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from yake) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from yake) (1.21.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from yake) (0.8.10)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting segtok\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.8/dist-packages (from yake) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from segtok->yake) (2022.6.2)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp38-cp38-linux_x86_64.whl size=70607 sha256=f7060ab76a2d6a5febb3f6fc00478b0ddad5ea0b41afe253d5fda14da1aa0df9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/c7/3c/4c83132de76359e3a429fd09c08995945ca96c5290a41651d3\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.9.0 segtok-1.5.11 yake-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def extractKeywordsYAKE(normalized_corpus):\n",
        "    print('\\n---YAKE Extraction---')\n",
        "    print('------------------------\\n')\n",
        "\n",
        "    language = \"en\"\n",
        "    max_ngram_size = 3\n",
        "    deduplication_threshold = 0.9\n",
        "    deduplication_algo = 'seqm'\n",
        "    windowSize = 1\n",
        "    numOfKeywords = 20\n",
        "\n",
        "    kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
        "    keywords = kw_extractor.extract_keywords(normalized_corpus)\n",
        "\n",
        "    zipped = list(map(list, zip(*keywords)))\n",
        "    keywords = zipped[0]\n",
        "    scores = zipped[1]\n",
        "\n",
        "   \n",
        "    for i, value in enumerate(keywords):\n",
        "        print(value.ljust(40), scores[i])\n",
        "\n",
        "    avg = mean(scores[:19])\n",
        "    print(\"Score Average: \" + str(avg)+ \"\\n\")\n",
        "\n",
        "\n",
        "    return keywords, scores, avg"
      ],
      "metadata": {
        "id": "JGGvwVeYvaRv"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TextRank Extractor**"
      ],
      "metadata": {
        "id": "SbPfZZiZQGTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install summa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsnAZdNSQA8y",
        "outputId": "0dc7de85-f3cd-4d60-8be0-80f639ca43db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.8/dist-packages (from summa) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy>=0.19->summa) (1.21.6)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=a5519050f640a090b82ae91d495b9d670914a9d4a60610fa84d60be429999f74\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/6a/dd/209eb19d5f2266b9cfd06827539bf70435b0ad5fe8244e52d3\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summa import keywords\n",
        "from statistics import mean\n",
        "\n",
        "def extractKeywordsTextRank(normalized_corpus):\n",
        "  print('\\n')\n",
        "  print('---TextRank Extraction---')\n",
        "  print('------------------------\\n')\n",
        "\n",
        "  #extract\n",
        "  TR_keywords = keywords.keywords(normalized_corpus, scores=True)\n",
        "\n",
        "  #zip into list\n",
        "  zipped = list(map(list, zip(*TR_keywords)))\n",
        "  TR_keywords = zipped[0]\n",
        "  scores = zipped[1]\n",
        "  \n",
        "\n",
        "  #print(TR_keywords[0:20])\n",
        "  print('-Skill-'.ljust(40), '-Score-')\n",
        "  for i, value in enumerate(TR_keywords[0:19]):\n",
        "    print(value.ljust(40), scores[i])\n",
        "    \n",
        "  avg = mean(scores[:19])\n",
        "  print(\"\\nScore Average: \" + str(avg))\n",
        "  \n",
        "  \n",
        "\n",
        "  return TR_keywords, scores, avg\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "ybCL_9CJQCrl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Skill Matching**"
      ],
      "metadata": {
        "id": "Oao0RKq6aDyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install jieba\n",
        "\n",
        "from functools import reduce\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import jieba\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "class SkillsMatching:\n",
        "  def __init__(self, all_job_postings_keywords, all_jp_titles, all_resumes_keywords):\n",
        "      self.all_job_postings_keywords = all_job_postings_keywords\n",
        "      self.all_jp_titles = all_jp_titles\n",
        "      self.all_resumes_keywords = all_resumes_keywords\n",
        "      \n",
        "\n",
        "  # output JSON with matching job postings for each resume\n",
        "  def skill_match(self):\n",
        "      results = []\n",
        "      \n",
        "      for resume_json in self.all_resumes_keywords:\n",
        "          resume_arr = []\n",
        "          for key in resume_json:\n",
        "            # loop through the keys = every classifier \n",
        "            classifier_json = {key: {\"matching_job\": \"\",\n",
        "                      \"job_title\": \"\"}}\n",
        "            resume_keywords = resume_json[key]\n",
        "            scores = self.sim_score(self.all_job_postings_keywords, resume_keywords)\n",
        "            # grab the index of the largest score in the arr\n",
        "            max_idx = np.argmax(scores)\n",
        "            classifier_json[key]['matching_job'] = self.all_job_postings_keywords[max_idx]\n",
        "            classifier_json[key]['job_title'] = self.all_jp_titles[max_idx]\n",
        "            resume_arr.append(classifier_json)\n",
        "          results.append(resume_arr)\n",
        "      return results\n",
        "\n",
        "\n",
        "  def split_and_join_arr(self, arr):\n",
        "    new_arr = []\n",
        "    for w in arr:\n",
        "      word_arr = re.split('\\W+', w.lower())\n",
        "      new_arr = new_arr + word_arr\n",
        "    # print(new_arr)\n",
        "    return new_arr\n",
        "  \n",
        "  def sim_score(self, docs, keywords):\n",
        "    keywords = self.split_and_join_arr(keywords)\n",
        "    gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in docs]\n",
        "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "    tf_idf = gensim.models.TfidfModel(corpus)\n",
        "    sims = gensim.similarities.Similarity('/usr/workdir',tf_idf[corpus],\n",
        "                                      num_features=len(dictionary))\n",
        "\n",
        "    query_doc_bow = dictionary.doc2bow(keywords)\n",
        "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "    # print(sims[query_doc_tf_idf])\n",
        "    return sims[query_doc_tf_idf]\n",
        "\n",
        "  def print_results(self, match_job_postings):\n",
        "    for i, resume in enumerate(match_job_postings):\n",
        "      print('------------------------\\n')\n",
        "      print(\"Matching Job Posting for Resume %s\" % (i + 1))\n",
        "      for classifier_obj in resume:\n",
        "        classifier_type = list(classifier_obj.keys())[0]\n",
        "        print(classifier_type)\n",
        "        print(classifier_obj[classifier_type]['job_title'])\n",
        "        print(classifier_obj[classifier_type]['matching_job'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9IBrSshaKbM",
        "outputId": "fd20fccc-5a98-4755-b952-2b22e12e8e02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (0.42.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Driver Code"
      ],
      "metadata": {
        "id": "FIi95ge84CGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "9rzIUXh-7cLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd     # pip install pandas. usage: loading data from csv files into dataframes\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import contractions\n",
        "import spacy\n",
        "from numpy.lib.npyio import savez_compressed\n",
        "from array import *\n",
        "nltk.download(\"all\")\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "YpAjFMcT6H3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Helper Functions\n",
        "\n",
        "# Function to retrieve text data.\n",
        "# (either 1 or more job postings or resumes)?\n",
        "def getFileData(filename, dir):\n",
        "    return pd.read_csv('data/' + dir + '/' + filename)\n",
        "\n",
        "# Function to normalize text data. \n",
        "# (some skill extraction tools will normalize text for us; however, if not, this function is here)\n",
        "# includes removing stopwords, punctuation, dates, links, etc...\n",
        "def normalizeCorpus(corpus):\n",
        "    \n",
        "    # import nltk for stopwords\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # convert to lower case\n",
        "    lower_corpus = corpus.lower()\n",
        "\n",
        "    # remove numbers\n",
        "    no_number_corpus = re.sub(r'\\d+','',lower_corpus)\n",
        "\n",
        "    # remove all punctuation except words and space\n",
        "    no_punc_corpus = re.sub(r'[^\\w\\s]','', no_number_corpus)\n",
        "\n",
        "    # remove white spaces\n",
        "    no_wspace_corpus = no_punc_corpus.strip()\n",
        "    no_wspace_corpus\n",
        "\n",
        "    # convert string to list of words\n",
        "    lst_corpus = [no_wspace_corpus][0].split()\n",
        "    print(lst_corpus)\n",
        "\n",
        "  # remove stopwords\n",
        "    no_stpwords_corpus=\"\"\n",
        "    for i in lst_corpus:\n",
        "\t    if not i in stop_words:\n",
        "\t      no_stpwords_corpus += i+' '\n",
        "\t\t\n",
        "    # removing last space\n",
        "    no_stpwords_corpus = no_stpwords_corpus[:-1]\n",
        "\n",
        "    # output\n",
        "    print(no_stpwords_corpus)\n",
        "\n",
        "    return no_stpwords_corpus\n",
        "    # return corpus\n",
        "    # need to fix this \n",
        "    # nltk_tokenList = word_tokenize(corpus)\n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # nltk_lemmaList = []\n",
        "    # for word in nltk_tokenList:\n",
        "    #     nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    # normalized_corpus = []  \n",
        "    # nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "    # for w in nltk_lemmaList:  \n",
        "    #     if w not in nltk_stop_words:  \n",
        "    #         normalized_corpus.append(w)\n",
        "\n",
        "    # punctuation = \";:.,?!\"\n",
        "    # for word in normalized_corpus:\n",
        "    #     if word in punctuation:\n",
        "    #         normalized_corpus.remove(word)\n",
        "\n",
        "    #still need to add dates and links\n",
        "    # return normalized_corpus\n",
        "\n",
        "\n",
        "    \n",
        "def similiartychecker(tokenA, tokenB):\n",
        "\n",
        "    A = nlp(tokenA)\n",
        "    B = nlp(tokenB)\n",
        "  \n",
        "    score = A.similarity(B)\n",
        "  \n",
        "    return score\n",
        "\n",
        "# Function to extract skill words from a given corpus.\n",
        "# ideally, this function will output a set of skills extracted from the corpus\n",
        "def extractSkills(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT, avgBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    keywordsYAKE, scoresYAKE, avgYAKE = extractKeywordsYAKE(corpus)\n",
        "\n",
        "    # extraction method 3\n",
        "    keywordsTextRank, scoresTextRank, avgTextRank = extractKeywordsTextRank(corpus)\n",
        "\n",
        "    # return keywordsBERT, keywordsYAKE[0], keywordsTextRank[0]\n",
        "    # return keywordsBERT, keywordsCSO, keywordsYAKE, keywordsTextRank,\n",
        "    return {\"keywordsBERT\": keywordsBERT,\n",
        "            \"keywordsCSO\": keywordsCSO,\n",
        "            \"keywordsYAKE\": keywordsYAKE}\n",
        "            # \"keywordsTextRAnk\": keywordsTextRank}\n",
        "\n",
        "def extractSkillsforcomp(corpus):\n",
        "\n",
        "    # run KeyBert Extraction\n",
        "    keywordsBERT, scoresBERT, avgBERT = extractKeywordsBERT(corpus)\n",
        "\n",
        "    # run CSO Classifier Extraction\n",
        "    keywordsCSO = extractKeywordsCSO(corpus)\n",
        "    \n",
        "    # extraction method 2\n",
        "    keywordsYAKE, scoresYAKE, avgYAKE = extractKeywordsYAKE(corpus)\n",
        "\n",
        "    # extraction method 3\n",
        "    keywordsTextRank, scoresTextRank, avgTextRank = extractKeywordsTextRank(corpus)\n",
        "\n",
        "    # return keywordsBERT, keywordsYAKE[0], keywordsTextRank[0]\n",
        "    return keywordsBERT, keywordsCSO, keywordsYAKE, keywordsTextRank\n",
        "\n",
        "   \n",
        "### Driver Code\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # fetch the data\n",
        "    job_posting_obj = queryJobsAPI(\"Data Scientist\", \"Washington, DC\")\n",
        "    resume_dataframe = getFileData('kaggleResumes.csv', 'resumes')\n",
        "    #---------------\n",
        "\n",
        "    '''\n",
        "    # print the dataframes\n",
        "    print('DataFrame of Job Postings:')\n",
        "    print(job_posting_dataframe)    \n",
        "    print()\n",
        "\n",
        "    print('DataFrame of Resumes:')\n",
        "    print(resume_dataframe)\n",
        "    print()\n",
        "    #----------------------\n",
        "    '''\n",
        "\n",
        "    # fetch the job descriptions and resumes by themselves\n",
        "    jpCorpus = []\n",
        "    jpTitles = []\n",
        "\n",
        "    for i in range(len(job_posting_obj)):\n",
        "      jpCorpus.append(job_posting_obj[i]['MatchedObjectDescriptor']['UserArea']['Details']['JobSummary'])\n",
        "      jpTitles.append(job_posting_obj[i]['MatchedObjectDescriptor']['PositionTitle'])\n",
        "\n",
        "    rCorpus  = list(resume_dataframe['Resume'])\n",
        "    #----------------------\n",
        "\n",
        "    # # Number of resume samples to view\n",
        "    # NUM_SAMPLES = 10\n",
        "    NUM_SAMPLES = 1\n",
        "    # # number of job postings to view\n",
        "    # NUM_JPS = 10\n",
        "    NUM_JPS = 1\n",
        "\n",
        "    # for each JOB POSTING from the corpus\n",
        "    i = 0\n",
        "    all_job_postings = []\n",
        "    for posting in jpCorpus:\n",
        "        print('Job Posting #', i+1)\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(posting)\n",
        "        job_posting_json = extractSkills(text)\n",
        "        all_job_postings.append(text)\n",
        "\n",
        "        #for similiarity referencer:\n",
        "        jobKeyBERTsimComp, jobCSOsimComp, jobYAKEsimComp, jobTextRanksimComp = extractSkillsforcomp(text)\n",
        "       \n",
        "\n",
        "        # print lines, we are done with this posting\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X postings\n",
        "        i += 1\n",
        "        if i > NUM_JPS:\n",
        "            break\n",
        "    #---------------------- \n",
        "\n",
        "    # for each RESUME from the corpus\n",
        "    i = 0\n",
        "    all_resumes_keywords = []\n",
        "    for resume in rCorpus:\n",
        "        print('Resume #', i+1)\n",
        "        print()\n",
        "\n",
        "        text = normalizeCorpus(resume)\n",
        "        resume_json = extractSkills(text)\n",
        "        all_resumes_keywords.append(resume_json)\n",
        "\n",
        "        #for similiarity referencer:\n",
        "        resKeyBERTsimComp, resCSOsimComp, resYAKEsimComp, resTextRanksimComp = extractSkillsforcomp(text)\n",
        "\n",
        "        # print lines, we are done with this resume\n",
        "        print('------------------------\\n')\n",
        "\n",
        "        # break, after X resumes\n",
        "        i += 1\n",
        "        if i > NUM_SAMPLES:\n",
        "            break\n",
        "    #---------------------- \n",
        "    '''\n",
        "    ----------------------------------------------------------------------|\n",
        "    |coss reference similiary of resume and job posting extracted keywords|\n",
        "    |-addtion by Tyrell Richardson                                        |\n",
        "    |                                                                     |\n",
        "    |---------------------------------------------------------------------|\n",
        "    '''\n",
        "\n",
        "    keyBERTSimiliarity = np.zeros(14)\n",
        "\n",
        "    for k in range (0,14):\n",
        "        keyBERTSimiliarity[k] = similiartychecker(jobKeyBERTsimComp[k], resKeyBERTsimComp[k])\n",
        "\n",
        "    print('KeyBERT match Similarity Mean: '+str(np.mean(keyBERTSimiliarity)))\n",
        "    \n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    CSOSimilarity = np.zeros(len(min(jobCSOsimComp, resCSOsimComp)))\n",
        "\n",
        "    for h in range (0,3):\n",
        "      CSOSimilarity[h] = similiartychecker(jobCSOsimComp[h],resCSOsimComp[h])\n",
        "\n",
        "    print('CSO match Similiarity Mean: '+str(np.mean(CSOSimilarity)))\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    YakeSimilarity = np.zeros(len(min(jobYAKEsimComp, resYAKEsimComp)))\n",
        "    for i in range (0,19):\n",
        "      YakeSimilarity[i] = similiartychecker(jobYAKEsimComp[i],resYAKEsimComp[i])\n",
        "\n",
        "    print('Yake match Similiarity Mean: '+str(np.mean(YakeSimilarity)))\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "\n",
        "    TextRankSimiliarity = np.zeros(5)\n",
        "    for j in range (0, 5):\n",
        "      TextRankSimiliarity[j] = similiartychecker(jobTextRanksimComp[j], resTextRanksimComp[j])\n",
        "\n",
        "    print('TextRank match Similarity Mean: '+str(np.mean(TextRankSimiliarity)))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# keep going!\n",
        "\n",
        "# end of driver code\n",
        "#---------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6HTo9qA4Fxs",
        "outputId": "5cb10b81-b089-413e-f1c0-26c4083e41f8"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Scientist\n",
            "This position is located in the Department of Health & Human Services (HHS), Centers for Medicare & Medicaid Services (CMS), Office of Burden Reduction and Health Informatics (OBRHI), Emerging Innovations Group . As a Data Scientist, GS-1560-13, you will design, develop, and implement the analytical, statistical, and programming mechanisms necessary to collect, organize, analyze, and interpret unique and highly specialized data sets.\n",
            "Job Posting # 1\n",
            "\n",
            "['as', 'a', 'data', 'scientist', 'you', 'will', 'provide', 'data', 'management', 'and', 'analytical', 'support', 'to', 'the', 'technology', 'branch', 'office', 'office', 'of', 'public', 'trust', 'specific', 'to', 'the', 'body', 'worn', 'camera', 'and', 'electronic', 'records', 'management', 'systems', 'this', 'office', 'was', 'created', 'to', 'strengthen', 'public', 'trust', 'in', 'the', 'national', 'park', 'services', 'law', 'enforcement', 'programs', 'through', 'the', 'transparency', 'availability', 'and', 'accessibility', 'of', 'information', 'this', 'is', 'a', 'fully', 'remote', 'position']\n",
            "data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote position\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "law enforcement                          0.5326\n",
            "provide data management                  0.4489\n",
            "office public trust                      0.4378\n",
            "technology branch office                 0.3831\n",
            "scientist provide                        0.334\n",
            "trust national park                      0.3194\n",
            "transparency availability accessibility  0.2718\n",
            "records                                  0.2425\n",
            "electronic                               0.2121\n",
            "fully remote position                    0.1861\n",
            "created                                  0.1774\n",
            "specific body                            0.1719\n",
            "worn camera                              0.1449\n",
            "strengthen                               0.0879\n",
            "fully                                    0.0779\n",
            "\n",
            "Score Average: 0.2821714285714286\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "information management\n",
            "data management\n",
            "electronic records\n",
            "records management\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "fully remote position                    0.0019142982031786582\n",
            "analytical support technology            0.002873572627569998\n",
            "support technology branch                0.002873572627569998\n",
            "specific body worn                       0.002873572627569998\n",
            "body worn camera                         0.002873572627569998\n",
            "worn camera electronic                   0.002873572627569998\n",
            "camera electronic records                0.002873572627569998\n",
            "national park services                   0.002873572627569998\n",
            "park services law                        0.002873572627569998\n",
            "services law enforcement                 0.002873572627569998\n",
            "law enforcement programs                 0.002873572627569998\n",
            "enforcement programs transparency        0.002873572627569998\n",
            "programs transparency availability       0.002873572627569998\n",
            "transparency availability accessibility  0.002873572627569998\n",
            "availability accessibility information   0.002873572627569998\n",
            "accessibility information fully          0.002873572627569998\n",
            "information fully remote                 0.002873572627569998\n",
            "trust specific body                      0.0029865057316926695\n",
            "created strengthen public                0.0029865057316926695\n",
            "trust national park                      0.0029865057316926695\n",
            "Score Average: 0.0028349721951412615\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "management                               0.26937774962671174\n",
            "office                                   0.23981126332883315\n",
            "trust                                    0.213847813620145\n",
            "data                                     0.21332363295542517\n",
            "fully remote                             0.20168549278175896\n",
            "Score Average: 0.2276091904625748\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "law enforcement                          0.5326\n",
            "provide data management                  0.4489\n",
            "office public trust                      0.4378\n",
            "technology branch office                 0.3831\n",
            "scientist provide                        0.334\n",
            "trust national park                      0.3194\n",
            "transparency availability accessibility  0.2718\n",
            "records                                  0.2425\n",
            "electronic                               0.2121\n",
            "fully remote position                    0.1861\n",
            "created                                  0.1774\n",
            "specific body                            0.1719\n",
            "worn camera                              0.1449\n",
            "strengthen                               0.0879\n",
            "fully                                    0.0779\n",
            "\n",
            "Score Average: 0.2821714285714286\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "information management\n",
            "data management\n",
            "electronic records\n",
            "records management\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "fully remote position                    0.0019142982031786582\n",
            "analytical support technology            0.002873572627569998\n",
            "support technology branch                0.002873572627569998\n",
            "specific body worn                       0.002873572627569998\n",
            "body worn camera                         0.002873572627569998\n",
            "worn camera electronic                   0.002873572627569998\n",
            "camera electronic records                0.002873572627569998\n",
            "national park services                   0.002873572627569998\n",
            "park services law                        0.002873572627569998\n",
            "services law enforcement                 0.002873572627569998\n",
            "law enforcement programs                 0.002873572627569998\n",
            "enforcement programs transparency        0.002873572627569998\n",
            "programs transparency availability       0.002873572627569998\n",
            "transparency availability accessibility  0.002873572627569998\n",
            "availability accessibility information   0.002873572627569998\n",
            "accessibility information fully          0.002873572627569998\n",
            "information fully remote                 0.002873572627569998\n",
            "trust specific body                      0.0029865057316926695\n",
            "created strengthen public                0.0029865057316926695\n",
            "trust national park                      0.0029865057316926695\n",
            "Score Average: 0.0028349721951412615\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "management                               0.26937774962671174\n",
            "office                                   0.23981126332883315\n",
            "trust                                    0.213847813620145\n",
            "data                                     0.21332363295542517\n",
            "fully remote                             0.20168549278175896\n",
            "Score Average: 0.2276091904625748\n",
            "------------------------\n",
            "\n",
            "Job Posting # 2\n",
            "\n",
            "['this', 'position', 'is', 'located', 'in', 'the', 'department', 'of', 'health', 'human', 'services', 'hhs', 'centers', 'for', 'medicare', 'medicaid', 'services', 'cms', 'office', 'of', 'burden', 'reduction', 'and', 'health', 'informatics', 'obrhi', 'emerging', 'innovations', 'group', 'as', 'a', 'data', 'scientist', 'gs', 'you', 'will', 'design', 'develop', 'and', 'implement', 'the', 'analytical', 'statistical', 'and', 'programming', 'mechanisms', 'necessary', 'to', 'collect', 'organize', 'analyze', 'and', 'interpret', 'unique', 'and', 'highly', 'specialized', 'data', 'sets']\n",
            "position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly specialized data sets\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "health informatics                       0.6339\n",
            "analytical statistical programming       0.4036\n",
            "group data                               0.3918\n",
            "medicaid                                 0.3757\n",
            "position located department              0.3249\n",
            "emerging innovations                     0.2476\n",
            "analyze interpret                        0.2452\n",
            "gs design develop                        0.2427\n",
            "cms office burden                        0.2234\n",
            "reduction                                0.1954\n",
            "human                                    0.146\n",
            "centers                                  0.1354\n",
            "unique highly specialized                0.1341\n",
            "obrhi emerging                           0.1083\n",
            "mechanisms necessary                     0.0224\n",
            "\n",
            "Score Average: 0.272\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "service delivery\n",
            "integrated data\n",
            "programming languages\n",
            "correlation analysis\n",
            "target position\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "position located department              0.003683438125747622\n",
            "specialized data sets                    0.005385420690931887\n",
            "hhs centers medicare                     0.006172028049662181\n",
            "centers medicare medicaid                0.006172028049662181\n",
            "cms office burden                        0.006172028049662181\n",
            "office burden reduction                  0.006172028049662181\n",
            "informatics obrhi emerging               0.006172028049662181\n",
            "obrhi emerging innovations               0.006172028049662181\n",
            "emerging innovations group               0.006172028049662181\n",
            "design develop implement                 0.006172028049662181\n",
            "develop implement analytical             0.006172028049662181\n",
            "implement analytical statistical         0.006172028049662181\n",
            "analytical statistical programming       0.006172028049662181\n",
            "statistical programming mechanisms       0.006172028049662181\n",
            "collect organize analyze                 0.006172028049662181\n",
            "organize analyze interpret               0.006172028049662181\n",
            "analyze interpret unique                 0.006172028049662181\n",
            "interpret unique highly                  0.006172028049662181\n",
            "unique highly specialized                0.006172028049662181\n",
            "located department health                0.009062863878918739\n",
            "Score Average: 0.005999649245312452\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.28584294577822333\n",
            "health                                   0.2692405194482652\n",
            "services                                 0.26191487903200117\n",
            "located                                  0.1707514088341824\n",
            "programming mechanisms necessary         0.15780425521312472\n",
            "Score Average: 0.22911080166115938\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "health informatics                       0.6339\n",
            "analytical statistical programming       0.4036\n",
            "group data                               0.3918\n",
            "medicaid                                 0.3757\n",
            "position located department              0.3249\n",
            "emerging innovations                     0.2476\n",
            "analyze interpret                        0.2452\n",
            "gs design develop                        0.2427\n",
            "cms office burden                        0.2234\n",
            "reduction                                0.1954\n",
            "human                                    0.146\n",
            "centers                                  0.1354\n",
            "unique highly specialized                0.1341\n",
            "obrhi emerging                           0.1083\n",
            "mechanisms necessary                     0.0224\n",
            "\n",
            "Score Average: 0.272\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "service delivery\n",
            "integrated data\n",
            "programming languages\n",
            "correlation analysis\n",
            "target position\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "position located department              0.003683438125747622\n",
            "specialized data sets                    0.005385420690931887\n",
            "hhs centers medicare                     0.006172028049662181\n",
            "centers medicare medicaid                0.006172028049662181\n",
            "cms office burden                        0.006172028049662181\n",
            "office burden reduction                  0.006172028049662181\n",
            "informatics obrhi emerging               0.006172028049662181\n",
            "obrhi emerging innovations               0.006172028049662181\n",
            "emerging innovations group               0.006172028049662181\n",
            "design develop implement                 0.006172028049662181\n",
            "develop implement analytical             0.006172028049662181\n",
            "implement analytical statistical         0.006172028049662181\n",
            "analytical statistical programming       0.006172028049662181\n",
            "statistical programming mechanisms       0.006172028049662181\n",
            "collect organize analyze                 0.006172028049662181\n",
            "organize analyze interpret               0.006172028049662181\n",
            "analyze interpret unique                 0.006172028049662181\n",
            "interpret unique highly                  0.006172028049662181\n",
            "unique highly specialized                0.006172028049662181\n",
            "located department health                0.009062863878918739\n",
            "Score Average: 0.005999649245312452\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data                                     0.28584294577822333\n",
            "health                                   0.2692405194482652\n",
            "services                                 0.26191487903200117\n",
            "located                                  0.1707514088341824\n",
            "programming mechanisms necessary         0.15780425521312472\n",
            "Score Average: 0.22911080166115938\n",
            "------------------------\n",
            "\n",
            "Resume # 1\n",
            "\n",
            "['skills', 'programming', 'languages', 'python', 'pandas', 'numpy', 'scipy', 'scikitlearn', 'matplotlib', 'sql', 'java', 'javascriptjquery', 'machine', 'learning', 'regression', 'svm', 'naãve', 'bayes', 'knn', 'random', 'forest', 'decision', 'trees', 'boosting', 'techniques', 'cluster', 'analysis', 'word', 'embedding', 'sentiment', 'analysis', 'natural', 'language', 'processing', 'dimensionality', 'reduction', 'topic', 'modelling', 'lda', 'nmf', 'pca', 'neural', 'nets', 'database', 'visualizations', 'mysql', 'sqlserver', 'cassandra', 'hbase', 'elasticsearch', 'djs', 'dcjs', 'plotly', 'kibana', 'matplotlib', 'ggplot', 'tableau', 'others', 'regular', 'expression', 'html', 'css', 'angular', 'logstash', 'kafka', 'python', 'flask', 'git', 'docker', 'computer', 'vision', 'open', 'cv', 'and', 'understanding', 'of', 'deep', 'learningeducation', 'details', 'data', 'science', 'assurance', 'associate', 'data', 'science', 'assurance', 'associate', 'ernst', 'young', 'llp', 'skill', 'details', 'javascript', 'exprience', 'months', 'jquery', 'exprience', 'months', 'python', 'exprience', 'monthscompany', 'details', 'company', 'ernst', 'young', 'llp', 'description', 'fraud', 'investigations', 'and', 'dispute', 'services', 'assurance', 'technology', 'assisted', 'review', 'tar', 'technology', 'assisted', 'review', 'assists', 'in', 'accelerating', 'the', 'review', 'process', 'and', 'run', 'analytics', 'and', 'generate', 'reports', 'core', 'member', 'of', 'a', 'team', 'helped', 'in', 'developing', 'automated', 'review', 'platform', 'tool', 'from', 'scratch', 'for', 'assisting', 'e', 'discovery', 'domain', 'this', 'tool', 'implements', 'predictive', 'coding', 'and', 'topic', 'modelling', 'by', 'automating', 'reviews', 'resulting', 'in', 'reduced', 'labor', 'costs', 'and', 'time', 'spent', 'during', 'the', 'lawyers', 'review', 'understand', 'the', 'end', 'to', 'end', 'flow', 'of', 'the', 'solution', 'doing', 'research', 'and', 'development', 'for', 'classification', 'models', 'predictive', 'analysis', 'and', 'mining', 'of', 'the', 'information', 'present', 'in', 'text', 'data', 'worked', 'on', 'analyzing', 'the', 'outputs', 'and', 'precision', 'monitoring', 'for', 'the', 'entire', 'tool', 'tar', 'assists', 'in', 'predictive', 'coding', 'topic', 'modelling', 'from', 'the', 'evidence', 'by', 'following', 'ey', 'standards', 'developed', 'the', 'classifier', 'models', 'in', 'order', 'to', 'identify', 'red', 'flags', 'and', 'fraudrelated', 'issues', 'tools', 'technologies', 'python', 'scikitlearn', 'tfidf', 'wordvec', 'docvec', 'cosine', 'similarity', 'naãve', 'bayes', 'lda', 'nmf', 'for', 'topic', 'modelling', 'vader', 'and', 'text', 'blob', 'for', 'sentiment', 'analysis', 'matplot', 'lib', 'tableau', 'dashboard', 'for', 'reporting', 'multiple', 'data', 'science', 'and', 'analytic', 'projects', 'usa', 'clients', 'text', 'analytics', 'motor', 'vehicle', 'customer', 'review', 'data', 'received', 'customer', 'feedback', 'survey', 'data', 'for', 'past', 'one', 'year', 'performed', 'sentiment', 'positive', 'negative', 'neutral', 'and', 'time', 'series', 'analysis', 'on', 'customer', 'comments', 'across', 'all', 'categories', 'created', 'heat', 'map', 'of', 'terms', 'by', 'survey', 'category', 'based', 'on', 'frequency', 'of', 'words', 'extracted', 'positive', 'and', 'negative', 'words', 'across', 'all', 'the', 'survey', 'categories', 'and', 'plotted', 'word', 'cloud', 'created', 'customized', 'tableau', 'dashboards', 'for', 'effective', 'reporting', 'and', 'visualizations', 'chatbot', 'developed', 'a', 'user', 'friendly', 'chatbot', 'for', 'one', 'of', 'our', 'products', 'which', 'handle', 'simple', 'questions', 'about', 'hours', 'of', 'operation', 'reservation', 'options', 'and', 'so', 'on', 'this', 'chat', 'bot', 'serves', 'entire', 'product', 'related', 'questions', 'giving', 'overview', 'of', 'tool', 'via', 'qa', 'platform', 'and', 'also', 'give', 'recommendation', 'responses', 'so', 'that', 'user', 'question', 'to', 'build', 'chain', 'of', 'relevant', 'answer', 'this', 'too', 'has', 'intelligence', 'to', 'build', 'the', 'pipeline', 'of', 'questions', 'as', 'per', 'user', 'requirement', 'and', 'asks', 'the', 'relevant', 'recommended', 'questions', 'tools', 'technologies', 'python', 'natural', 'language', 'processing', 'nltk', 'spacy', 'topic', 'modelling', 'sentiment', 'analysis', 'word', 'embedding', 'scikitlearn', 'javascriptjquery', 'sqlserver', 'information', 'governance', 'organizations', 'to', 'make', 'informed', 'decisions', 'about', 'all', 'of', 'the', 'information', 'they', 'store', 'the', 'integrated', 'information', 'governance', 'portfolio', 'synthesizes', 'intelligence', 'across', 'unstructured', 'data', 'sources', 'and', 'facilitates', 'action', 'to', 'ensure', 'organizations', 'are', 'best', 'positioned', 'to', 'counter', 'information', 'risk', 'scan', 'data', 'from', 'multiple', 'sources', 'of', 'formats', 'and', 'parse', 'different', 'file', 'formats', 'extract', 'meta', 'data', 'information', 'push', 'results', 'for', 'indexing', 'elastic', 'search', 'and', 'created', 'customized', 'interactive', 'dashboards', 'using', 'kibana', 'preforming', 'rot', 'analysis', 'on', 'the', 'data', 'which', 'give', 'information', 'of', 'data', 'which', 'helps', 'identify', 'content', 'that', 'is', 'either', 'redundant', 'outdated', 'or', 'trivial', 'preforming', 'fulltext', 'search', 'analysis', 'on', 'elastic', 'search', 'with', 'predefined', 'methods', 'which', 'can', 'tag', 'as', 'pii', 'personally', 'identifiable', 'information', 'social', 'security', 'numbers', 'addresses', 'names', 'etc', 'which', 'frequently', 'targeted', 'during', 'cyberattacks', 'tools', 'technologies', 'python', 'flask', 'elastic', 'search', 'kibana', 'fraud', 'analytic', 'platform', 'fraud', 'analytics', 'and', 'investigative', 'platform', 'to', 'review', 'all', 'red', 'flag', 'cases', 'â', 'fap', 'is', 'a', 'fraud', 'analytics', 'and', 'investigative', 'platform', 'with', 'inbuilt', 'case', 'manager', 'and', 'suite', 'of', 'analytics', 'for', 'various', 'erp', 'systems', 'it', 'can', 'be', 'used', 'by', 'clients', 'to', 'interrogate', 'their', 'accounting', 'systems', 'for', 'identifying', 'the', 'anomalies', 'which', 'can', 'be', 'indicators', 'of', 'fraud', 'by', 'running', 'advanced', 'analytics', 'tools', 'technologies', 'html', 'javascript', 'sqlserver', 'jquery', 'css', 'bootstrap', 'nodejs', 'djs', 'dcjs']\n",
            "skills programming languages python pandas numpy scipy scikitlearn matplotlib sql java javascriptjquery machine learning regression svm naãve bayes knn random forest decision trees boosting techniques cluster analysis word embedding sentiment analysis natural language processing dimensionality reduction topic modelling lda nmf pca neural nets database visualizations mysql sqlserver cassandra hbase elasticsearch djs dcjs plotly kibana matplotlib ggplot tableau others regular expression html css angular logstash kafka python flask git docker computer vision open cv understanding deep learningeducation details data science assurance associate data science assurance associate ernst young llp skill details javascript exprience months jquery exprience months python exprience monthscompany details company ernst young llp description fraud investigations dispute services assurance technology assisted review tar technology assisted review assists accelerating review process run analytics generate reports core member team helped developing automated review platform tool scratch assisting e discovery domain tool implements predictive coding topic modelling automating reviews resulting reduced labor costs time spent lawyers review understand end end flow solution research development classification models predictive analysis mining information present text data worked analyzing outputs precision monitoring entire tool tar assists predictive coding topic modelling evidence following ey standards developed classifier models order identify red flags fraudrelated issues tools technologies python scikitlearn tfidf wordvec docvec cosine similarity naãve bayes lda nmf topic modelling vader text blob sentiment analysis matplot lib tableau dashboard reporting multiple data science analytic projects usa clients text analytics motor vehicle customer review data received customer feedback survey data past one year performed sentiment positive negative neutral time series analysis customer comments across categories created heat map terms survey category based frequency words extracted positive negative words across survey categories plotted word cloud created customized tableau dashboards effective reporting visualizations chatbot developed user friendly chatbot one products handle simple questions hours operation reservation options chat bot serves entire product related questions giving overview tool via qa platform also give recommendation responses user question build chain relevant answer intelligence build pipeline questions per user requirement asks relevant recommended questions tools technologies python natural language processing nltk spacy topic modelling sentiment analysis word embedding scikitlearn javascriptjquery sqlserver information governance organizations make informed decisions information store integrated information governance portfolio synthesizes intelligence across unstructured data sources facilitates action ensure organizations best positioned counter information risk scan data multiple sources formats parse different file formats extract meta data information push results indexing elastic search created customized interactive dashboards using kibana preforming rot analysis data give information data helps identify content either redundant outdated trivial preforming fulltext search analysis elastic search predefined methods tag pii personally identifiable information social security numbers addresses names etc frequently targeted cyberattacks tools technologies python flask elastic search kibana fraud analytic platform fraud analytics investigative platform review red flag cases â fap fraud analytics investigative platform inbuilt case manager suite analytics various erp systems used clients interrogate accounting systems identifying anomalies indicators fraud running advanced analytics tools technologies html javascript sqlserver jquery css bootstrap nodejs djs dcjs\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "text analytics                           0.5406\n",
            "categories created                       0.3252\n",
            "ey standards                             0.2298\n",
            "flask elastic search                     0.2127\n",
            "deep learningeducation details           0.2003\n",
            "reviews resulting reduced                0.1973\n",
            "governance portfolio                     0.1942\n",
            "review tar technology                    0.1859\n",
            "scipy scikitlearn matplotlib             0.1623\n",
            "flag cases fap                           0.1531\n",
            "months                                   0.1368\n",
            "end                                      0.1255\n",
            "javascript sqlserver jquery              0.1012\n",
            "regular expression html                  0.0628\n",
            "git docker computer                      0.0504\n",
            "\n",
            "Score Average: 0.20197857142857142\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "random forests\n",
            "css\n",
            "decision trees\n",
            "cyber-attacks\n",
            "vehicles\n",
            "sentiment analysis\n",
            "classification models\n",
            "python\n",
            "text data\n",
            "visualization\n",
            "programming languages\n",
            "html\n",
            "natural languages\n",
            "principle component analysis\n",
            "classifiers\n",
            "user information\n",
            "k-nearest neighbors\n",
            "social sciences\n",
            "cluster analysis\n",
            "computer vision\n",
            "natural language processing\n",
            "svm\n",
            "database systems\n",
            "customer review\n",
            "javascript\n",
            "boosting\n",
            "question answering\n",
            "association rules\n",
            "dimensionality reduction\n",
            "word embedding\n",
            "erp system\n",
            "machine learning\n",
            "java\n",
            "visualization tools\n",
            "recommendation\n",
            "textual data\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "coding topic modelling                   0.00019373089847710953\n",
            "tools technologies python                0.00020533252874614465\n",
            "predictive coding topic                  0.00022660120047345922\n",
            "science assurance associate              0.00023597521189463587\n",
            "ernst young llp                          0.00025489956207999924\n",
            "natural language processing              0.00028034724728379535\n",
            "fraud analytics investigative            0.0003747231806369725\n",
            "technology assisted review               0.00038159055373030847\n",
            "data science assurance                   0.00038914530362812435\n",
            "analytics investigative platform         0.00041165922398054097\n",
            "analysis word embedding                  0.00043046293272864524\n",
            "topic modelling lda                      0.00044486609829637617\n",
            "nmf topic modelling                      0.00044486609829637617\n",
            "topic modelling sentiment                0.00046518759084287364\n",
            "technologies python natural              0.00047524101711444295\n",
            "technologies python flask                0.00047524101711444295\n",
            "topic modelling automating               0.00047817022156228104\n",
            "topic modelling evidence                 0.00047817022156228104\n",
            "topic modelling vader                    0.00047817022156228104\n",
            "reduction topic modelling                0.00047817022156228115\n",
            "Score Average: 0.0003749673857900574\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "information                              0.21707094170071853\n",
            "informed                                 0.21707094170071853\n",
            "analytics                                0.17028322773356402\n",
            "analytic                                 0.17028322773356402\n",
            "details data                             0.17016559159094552\n",
            "models                                   0.1498117414997547\n",
            "questions                                0.14548657836337686\n",
            "analysis word                            0.1453646611595867\n",
            "platform tool                            0.13991718971442807\n",
            "customer                                 0.13494592894668267\n",
            "customized                               0.13494592894668267\n",
            "identify                                 0.13293069204525032\n",
            "identifiable                             0.13293069204525032\n",
            "identifying                              0.13293069204525032\n",
            "python                                   0.13229443958981782\n",
            "tools technologies                       0.1312619231349776\n",
            "development                              0.12430786157985285\n",
            "developed                                0.12430786157985285\n",
            "fraud                                    0.11653807488695846\n",
            "Score Average: 0.1485709576840649\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "text analytics                           0.5406\n",
            "categories created                       0.3252\n",
            "ey standards                             0.2298\n",
            "flask elastic search                     0.2127\n",
            "deep learningeducation details           0.2003\n",
            "reviews resulting reduced                0.1973\n",
            "governance portfolio                     0.1942\n",
            "review tar technology                    0.1859\n",
            "scipy scikitlearn matplotlib             0.1623\n",
            "flag cases fap                           0.1531\n",
            "months                                   0.1368\n",
            "end                                      0.1255\n",
            "javascript sqlserver jquery              0.1012\n",
            "regular expression html                  0.0628\n",
            "git docker computer                      0.0504\n",
            "\n",
            "Score Average: 0.20197857142857142\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "random forests\n",
            "css\n",
            "decision trees\n",
            "cyber-attacks\n",
            "vehicles\n",
            "sentiment analysis\n",
            "classification models\n",
            "python\n",
            "text data\n",
            "visualization\n",
            "programming languages\n",
            "html\n",
            "natural languages\n",
            "principle component analysis\n",
            "classifiers\n",
            "user information\n",
            "k-nearest neighbors\n",
            "social sciences\n",
            "cluster analysis\n",
            "computer vision\n",
            "natural language processing\n",
            "svm\n",
            "database systems\n",
            "customer review\n",
            "javascript\n",
            "boosting\n",
            "question answering\n",
            "association rules\n",
            "dimensionality reduction\n",
            "word embedding\n",
            "erp system\n",
            "machine learning\n",
            "java\n",
            "visualization tools\n",
            "recommendation\n",
            "textual data\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "coding topic modelling                   0.00019373089847710953\n",
            "tools technologies python                0.00020533252874614465\n",
            "predictive coding topic                  0.00022660120047345922\n",
            "science assurance associate              0.00023597521189463587\n",
            "ernst young llp                          0.00025489956207999924\n",
            "natural language processing              0.00028034724728379535\n",
            "fraud analytics investigative            0.0003747231806369725\n",
            "technology assisted review               0.00038159055373030847\n",
            "data science assurance                   0.00038914530362812435\n",
            "analytics investigative platform         0.00041165922398054097\n",
            "analysis word embedding                  0.00043046293272864524\n",
            "topic modelling lda                      0.00044486609829637617\n",
            "nmf topic modelling                      0.00044486609829637617\n",
            "topic modelling sentiment                0.00046518759084287364\n",
            "technologies python natural              0.00047524101711444295\n",
            "technologies python flask                0.00047524101711444295\n",
            "topic modelling automating               0.00047817022156228104\n",
            "topic modelling evidence                 0.00047817022156228104\n",
            "topic modelling vader                    0.00047817022156228104\n",
            "reduction topic modelling                0.00047817022156228115\n",
            "Score Average: 0.0003749673857900574\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "information                              0.21707094170071853\n",
            "informed                                 0.21707094170071853\n",
            "analytics                                0.17028322773356402\n",
            "analytic                                 0.17028322773356402\n",
            "details data                             0.17016559159094552\n",
            "models                                   0.1498117414997547\n",
            "questions                                0.14548657836337686\n",
            "analysis word                            0.1453646611595867\n",
            "platform tool                            0.13991718971442807\n",
            "customer                                 0.13494592894668267\n",
            "customized                               0.13494592894668267\n",
            "identify                                 0.13293069204525032\n",
            "identifiable                             0.13293069204525032\n",
            "identifying                              0.13293069204525032\n",
            "python                                   0.13229443958981782\n",
            "tools technologies                       0.1312619231349776\n",
            "development                              0.12430786157985285\n",
            "developed                                0.12430786157985285\n",
            "fraud                                    0.11653807488695846\n",
            "Score Average: 0.1485709576840649\n",
            "------------------------\n",
            "\n",
            "Resume # 2\n",
            "\n",
            "['education', 'details', 'may', 'to', 'may', 'be', 'uitrgpv', 'data', 'scientist', 'data', 'scientist', 'matelabs', 'skill', 'details', 'python', 'exprience', 'less', 'than', 'year', 'months', 'statsmodels', 'exprience', 'months', 'aws', 'exprience', 'less', 'than', 'year', 'months', 'machine', 'learning', 'exprience', 'less', 'than', 'year', 'months', 'sklearn', 'exprience', 'less', 'than', 'year', 'months', 'scipy', 'exprience', 'less', 'than', 'year', 'months', 'keras', 'exprience', 'less', 'than', 'year', 'monthscompany', 'details', 'company', 'matelabs', 'description', 'ml', 'platform', 'for', 'business', 'professionals', 'dummies', 'and', 'enthusiasts', 'a', 'koramangala', 'th', 'block', 'achievementstasks', 'behind', 'sukh', 'sagar', 'bengaluru', 'india', 'developed', 'and', 'deployed', 'auto', 'preprocessing', 'steps', 'of', 'machine', 'learning', 'mainly', 'missing', 'value', 'treatment', 'outlier', 'detection', 'encoding', 'scaling', 'feature', 'selection', 'and', 'dimensionality', 'reduction', 'deployed', 'automated', 'classification', 'and', 'regression', 'model', 'linkedincominadityarathore', 'bb', 'reasearch', 'and', 'deployed', 'the', 'time', 'series', 'forecasting', 'model', 'arima', 'sarimax', 'holtwinter', 'and', 'prophet', 'worked', 'on', 'metafeature', 'extracting', 'problem', 'githubcomrathorology', 'implemented', 'a', 'state', 'of', 'the', 'art', 'research', 'paper', 'on', 'outlier', 'detection', 'for', 'mixed', 'attributes', 'company', 'matelabs', 'description']\n",
            "education details may may uitrgpv data scientist data scientist matelabs skill details python exprience less year months statsmodels exprience months aws exprience less year months machine learning exprience less year months sklearn exprience less year months scipy exprience less year months keras exprience less year monthscompany details company matelabs description ml platform business professionals dummies enthusiasts koramangala th block achievementstasks behind sukh sagar bengaluru india developed deployed auto preprocessing steps machine learning mainly missing value treatment outlier detection encoding scaling feature selection dimensionality reduction deployed automated classification regression model linkedincominadityarathore bb reasearch deployed time series forecasting model arima sarimax holtwinter prophet worked metafeature extracting problem githubcomrathorology implemented state art research paper outlier detection mixed attributes company matelabs description\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist matelabs                  0.6191\n",
            "time series forecasting                  0.3737\n",
            "months aws exprience                     0.3223\n",
            "deployed auto preprocessing              0.251\n",
            "extracting problem githubcomrathorology  0.2247\n",
            "treatment outlier                        0.2139\n",
            "scaling feature                          0.1992\n",
            "india developed                          0.1436\n",
            "value                                    0.142\n",
            "prophet worked                           0.1358\n",
            "art research paper                       0.1122\n",
            "block achievementstasks                  0.1025\n",
            "detection encoding                       0.0939\n",
            "state                                    0.0725\n",
            "mainly                                   0.0446\n",
            "\n",
            "Score Average: 0.21474285714285712\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "business process\n",
            "forecasting models\n",
            "regression model\n",
            "machine learning\n",
            "learning environments\n",
            "genetic selection\n",
            "education\n",
            "dimensionality reduction\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "company matelabs description             0.00046903504138587023\n",
            "year months machine                      0.0006165309513983214\n",
            "year months statsmodels                  0.0006377329958494993\n",
            "year months sklearn                      0.0006377329958494993\n",
            "year months scipy                        0.0006377329958494993\n",
            "year months keras                        0.0006377329958494993\n",
            "year monthscompany details               0.0007262375320516979\n",
            "data scientist matelabs                  0.0009380700827717407\n",
            "data scientist data                      0.0009447459079394153\n",
            "scientist data scientist                 0.0009447459079394153\n",
            "scientist matelabs skill                 0.0009703941122482417\n",
            "attributes company matelabs              0.0009703941122482417\n",
            "uitrgpv data scientist                   0.0009773018423854854\n",
            "steps machine learning                   0.0009773018423854854\n",
            "treatment outlier detection              0.0009773018423854854\n",
            "paper outlier detection                  0.0009773018423854854\n",
            "outlier detection encoding               0.0009773018423854857\n",
            "outlier detection mixed                  0.0009773018423854857\n",
            "detection encoding scaling               0.0010109890842991507\n",
            "detection mixed attributes               0.0010109890842991507\n",
            "Score Average: 0.0008424150405259476\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "months                                   0.2612063193938972\n",
            "exprience                                0.260656637394355\n",
            "deployed                                 0.25328244113823245\n",
            "details                                  0.19875685454684705\n",
            "model                                    0.1771486046046705\n",
            "matelabs                                 0.17110520320284117\n",
            "outlier detection                        0.13646130011191365\n",
            "company                                  0.12482946331638518\n",
            "sagar                                    0.12069108696200458\n",
            "th block                                 0.11984781557708413\n",
            "data                                     0.11326414692040532\n",
            "machine learning                         0.11301717859560359\n",
            "Score Average: 0.17085558764702\n",
            "---KeyBert Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "data scientist matelabs                  0.6191\n",
            "time series forecasting                  0.3737\n",
            "months aws exprience                     0.3223\n",
            "deployed auto preprocessing              0.251\n",
            "extracting problem githubcomrathorology  0.2247\n",
            "treatment outlier                        0.2139\n",
            "scaling feature                          0.1992\n",
            "india developed                          0.1436\n",
            "value                                    0.142\n",
            "prophet worked                           0.1358\n",
            "art research paper                       0.1122\n",
            "block achievementstasks                  0.1025\n",
            "detection encoding                       0.0939\n",
            "state                                    0.0725\n",
            "mainly                                   0.0446\n",
            "\n",
            "Score Average: 0.21474285714285712\n",
            "\n",
            "-----CSO Extraction-----\n",
            "------------------------\n",
            "\n",
            "business process\n",
            "forecasting models\n",
            "regression model\n",
            "machine learning\n",
            "learning environments\n",
            "genetic selection\n",
            "education\n",
            "dimensionality reduction\n",
            "\n",
            "---YAKE Extraction---\n",
            "------------------------\n",
            "\n",
            "company matelabs description             0.00046903504138587023\n",
            "year months machine                      0.0006165309513983214\n",
            "year months statsmodels                  0.0006377329958494993\n",
            "year months sklearn                      0.0006377329958494993\n",
            "year months scipy                        0.0006377329958494993\n",
            "year months keras                        0.0006377329958494993\n",
            "year monthscompany details               0.0007262375320516979\n",
            "data scientist matelabs                  0.0009380700827717407\n",
            "data scientist data                      0.0009447459079394153\n",
            "scientist data scientist                 0.0009447459079394153\n",
            "scientist matelabs skill                 0.0009703941122482417\n",
            "attributes company matelabs              0.0009703941122482417\n",
            "uitrgpv data scientist                   0.0009773018423854854\n",
            "steps machine learning                   0.0009773018423854854\n",
            "treatment outlier detection              0.0009773018423854854\n",
            "paper outlier detection                  0.0009773018423854854\n",
            "outlier detection encoding               0.0009773018423854857\n",
            "outlier detection mixed                  0.0009773018423854857\n",
            "detection encoding scaling               0.0010109890842991507\n",
            "detection mixed attributes               0.0010109890842991507\n",
            "Score Average: 0.0008424150405259476\n",
            "\n",
            "\n",
            "\n",
            "---TextRank Extraction---\n",
            "------------------------\n",
            "\n",
            "-Skill-                                  -Score-\n",
            "months                                   0.2612063193938972\n",
            "exprience                                0.260656637394355\n",
            "deployed                                 0.25328244113823245\n",
            "details                                  0.19875685454684705\n",
            "model                                    0.1771486046046705\n",
            "matelabs                                 0.17110520320284117\n",
            "outlier detection                        0.13646130011191365\n",
            "company                                  0.12482946331638518\n",
            "sagar                                    0.12069108696200458\n",
            "th block                                 0.11984781557708413\n",
            "data                                     0.11326414692040532\n",
            "machine learning                         0.11301717859560359\n",
            "Score Average: 0.17085558764702\n",
            "------------------------\n",
            "\n",
            "KeyBERT match Similarity Mean: 0.36621243999212566\n",
            "CSO match Similiarity Mean: 0.23854360018990262\n",
            "Yake match Similiarity Mean: 0.29001494354092194\n",
            "TextRank match Similarity Mean: 0.24275117212956615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume and Job Matching"
      ],
      "metadata": {
        "id": "MsExUakXPC_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume and Job Posting Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, all_resumes_keywords)\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)"
      ],
      "metadata": {
        "id": "LQHvfMS9l7Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd57a27-f08e-458b-a4df-ba7c79e26620"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------\n",
            "\n",
            "Matching Job Posting for Resume 1\n",
            "keywordsBERT\n",
            "Data Scientist\n",
            "data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote position\n",
            "keywordsCSO\n",
            "Data Scientist\n",
            "position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly specialized data sets\n",
            "keywordsYAKE\n",
            "Data Scientist\n",
            "position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly specialized data sets\n",
            "------------------------\n",
            "\n",
            "Matching Job Posting for Resume 2\n",
            "keywordsBERT\n",
            "Data Scientist\n",
            "data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote position\n",
            "keywordsCSO\n",
            "Data Scientist\n",
            "position located department health human services hhs centers medicare medicaid services cms office burden reduction health informatics obrhi emerging innovations group data scientist gs design develop implement analytical statistical programming mechanisms necessary collect organize analyze interpret unique highly specialized data sets\n",
            "keywordsYAKE\n",
            "Data Scientist\n",
            "data scientist provide data management analytical support technology branch office office public trust specific body worn camera electronic records management systems office created strengthen public trust national park services law enforcement programs transparency availability accessibility information fully remote position\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Upload"
      ],
      "metadata": {
        "id": "xxp4OHVTXpSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drag and Drop a Data Scientist resum into Colab \n",
        "\n",
        "Make sure the file is named **resume.pdf**\n",
        "\n",
        "There is a dummy resume located in the `/data` directory.\n",
        "\n",
        "\n",
        "\n",
        "< - - - - - - -"
      ],
      "metadata": {
        "id": "1RCg41ohJkHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer3"
      ],
      "metadata": {
        "id": "PPWWB4B9YP5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9587cfd4-59c1-4999-b6a5-09586b149735"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer3\n",
            "  Downloading pdfminer3-2018.12.3.0.tar.gz (5.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (2.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from pdfminer3) (3.0.4)\n",
            "Building wheels for collected packages: pdfminer3\n",
            "  Building wheel for pdfminer3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer3: filename=pdfminer3-2018.12.3.0-py3-none-any.whl size=117823 sha256=f066bbfc81d96d284c624077bdbc6e3b56951403c5ccd077575ce3daaecda34b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/bc/f6/b518c318a55ab9e6d72092195cb9b49cac9ca60d6e000d0a1c\n",
            "Successfully built pdfminer3\n",
            "Installing collected packages: pycryptodome, pdfminer3\n",
            "Successfully installed pdfminer3-2018.12.3.0 pycryptodome-3.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer3.layout import LAParams, LTTextBox\n",
        "from pdfminer3.pdfpage import PDFPage\n",
        "from pdfminer3.pdfinterp import PDFResourceManager\n",
        "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer3.converter import PDFPageAggregator\n",
        "from pdfminer3.converter import TextConverter\n",
        "import io\n",
        "import sys\n",
        "from contextlib import redirect_stdout\n",
        "from io import StringIO \n",
        "\n",
        "resource_manager = PDFResourceManager()\n",
        "fake_file_handle = io.StringIO()\n",
        "converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
        "page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "\n",
        "with open('resume.pdf', 'rb') as fh:\n",
        "\n",
        "    for page in PDFPage.get_pages(fh,\n",
        "                                  caching=True,\n",
        "                                  check_extractable=True):\n",
        "        page_interpreter.process_page(page)\n",
        "\n",
        "    text = fake_file_handle.getvalue()\n",
        "\n",
        "# close open handles\n",
        "converter.close()\n",
        "fake_file_handle.close()\n",
        "\n",
        "# normalize and extract skills \n",
        "text = normalizeCorpus(text)\n",
        "resume_kw = extractSkills(text)\n",
        "\n",
        "# Matching\n",
        "matching = SkillsMatching(all_job_postings, jpTitles, [resume_kw])\n",
        "match_job_postings = matching.skill_match()\n",
        "matching.print_results(match_job_postings)"
      ],
      "metadata": {
        "id": "3vsrbN-UXsG8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "8788942f-eb85-4047-8d5f-3c239ab6cfad"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-3f2f897f1ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpage_interpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resume.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     for page in PDFPage.get_pages(fh,\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resume.pdf'"
          ]
        }
      ]
    }
  ]
}